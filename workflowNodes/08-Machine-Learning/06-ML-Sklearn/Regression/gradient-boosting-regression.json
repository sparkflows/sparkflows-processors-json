{
  "id": "5",
  "name": "Sklearn Gradient Boosting Regression",
  "description": "Gradient Boosting Regression, builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.",
  "input" : "",
  "output": "",
  "engine": "pyspark",
  "type": "ml-estimator",

  "nodeClass": "fire.nodes.sklearn.NodeSklearnGradientBoostingRegressor",
  "fields" : [

    {"name": "targetCol", "value": "", "widget": "variable", "title": "Target Column", "description": "The label column for model fitting", "datatypes": ["integer", "long", "double", "float", "int"], "required": true},
    {"name": "featureCols", "value": "[]", "widget": "variables", "title": "Feature Columns", "description": "Feature columns of type - all numeric, boolean and vector", "datatypes": ["integer", "long", "double", "float", "int"], "required": true},
    {"name": "loss", "value": "absolute_error", "widget": "array", "title": "Loss", "optionsArray": ["absolute_error", "squared_error", "huber", "quantile"], "description": "The loss function to be optimized. 'ls' refers to least squares regression."},
    {"name": "learning_rate", "value": 1.0, "widget": "textfield", "title": "Learning Rate", "description": "Learning rate shrinks the contribution of each tree by learning_rate."},
    {"name": "n_estimators", "value": 100, "widget": "textfield", "title": "Number of Estimators", "description": "The number of boosting stages to be run."},
    {"name": "subsample", "value": 1.0, "widget": "textfield", "title": "Subsample", "description": "The fraction of samples to be used for fitting the individual base learners."},
    {"name": "criterion", "value": "friedman_mse", "widget": "array", "title": "Criterion", "optionsArray": ["friedman_mse", "mse", "mae"], "description": "The function to measure the quality of a split."},
    {"name": "min_samples_split", "value": 2, "widget": "textfield", "title": "Min Samples Split", "description": "The minimum number of samples required to split an internal node."},
    {"name": "min_samples_leaf", "value": 1, "widget": "textfield", "title": "Min Samples Leaf", "description": "The minimum number of samples required to be at a leaf node."},
    {"name": "min_weight_fraction_leaf", "value": 0.0, "widget": "textfield", "title": "Min Weight Fraction Leaf", "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node."},
    {"name": "max_depth", "value": 3, "widget": "textfield", "title": "Max Depth", "description": "Maximum depth of the individual regression estimators."},
    {"name": "min_impurity_decrease", "value": 0.0, "widget": "textfield", "title": "Min Impurity Decrease", "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."},
    {"name": "random_state", "value": "", "widget": "textfield", "title": "Random State", "description": "Controls the randomness of the bootstrapping of the samples used when building trees."},
    {"name": "alpha", "value": 0.9, "widget": "textfield", "title": "Alpha", "description": "The alpha-quantile of the huber loss function and the quantile loss function."},
    {"name": "verbose", "value": 0, "widget": "textfield", "title": "Verbose", "description": "Enable verbose output. If 1 then it prints progress and performance once in a while."},
    {"name": "max_leaf_nodes", "value": "", "widget": "textfield", "title": "Max Leaf Nodes", "description": "Grow trees with `max_leaf_nodes` in best-first fashion. If not set, then unlimited number of leaf nodes."},
    {"name": "warm_start", "value": "false", "widget": "array", "title": "Warm Start", "optionsArray": ["True", "False"], "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution."},
    {"name": "presort", "value": "auto", "widget": "textfield", "title": "Presort", "description": "Whether to presort the data to speed up the finding of best splits in fitting."},
    {"name": "validation_fraction", "value": 0.1, "widget": "textfield", "title": "Validation Fraction", "description": "The proportion of training data to set aside as validation set for early stopping."},
    {"name": "n_iter_no_change", "value": "", "widget": "textfield", "title": "N Iter No Change", "description": "Used to decide if early stopping will be used to terminate training when validation score is not improving."},
    {"name": "tol", "value": 0.0001, "widget": "textfield", "title": "Tolerance", "description": "Tolerance for the early stopping. When the loss or score is not improving by at least `tol` for `n_iter_no_change` iterations, training stops."}


  ]
}

start-details:

More details are available at : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html

end-details:

