{
  "id": "11",
  "name": "PySpark",
  "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
  "input": "The input dataframe is passed in the variable to the function myfn.",
  "output": "The output dataframe is returned back from the function",
  "type": "pyspark",
  "engine": "pyspark",
  "nodeClass": "fire.nodes.etl.NodePySpark",
  "fields" : [
    {"name":"code", "value":"'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n    outDF = inDF  #add custom logic\n    return outDF", "widget": "textarea_large", "type": "python", "title": "PySpark", "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")"},

    {"name": "schema", "value":"", "widget": "tab", "title": "InferSchema"},

    {"name":"outputColNames", "value":"[]", "widget": "schema_col_names", "title": "Column Names", "description": "New Output Columns of the SQL"},
    {"name":"outputColTypes", "value":"[]", "widget": "schema_col_types", "title": "Column Types", "description": "Data Type of the Output Columns"},
    {"name":"outputColFormats", "value":"[]", "widget": "schema_col_formats", "title": "Column Formats", "description": "Format of the Output Columns"}

  ]
}

start-details:

h2:Pyspark Details

This node receives receives an input pyspark dataframe in function called myfn.

The pyspark/python code processes it and returns one computed pyspark dataframe.

end-details:

start-examples:

h2:Pyspark Examples

Input Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea

h4: Add the house_type column

from pyspark.sql.types import StringType
from pyspark.sql.functions import *
from pyspark.sql import *
from fire.workflowcontext import *

def myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):
house_type_udf = udf(lambda bedrooms: "big house" if int(bedrooms) >2 else "small house", StringType())
filetr_df = inDF.select("id", "price", "lotsize", "bedrooms")
outDF = filetr_df.withColumn("house_type", house_type_udf(filetr_df.bedrooms))
return outDF

h4: Using pandas dataframe

from pyspark.sql.types import StringType
from pyspark.sql.functions import *
from pyspark.sql import *

from fire.workflowcontext import *

def myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):
# Convert the Spark DataFrame to a Pandas DataFrame
pdf = inDF.select("*").toPandas()

# Display the result on the Executions page
workflowContext.outStr(id, "Outputting Pandas Dataframe")

# Display the dataframe on the Executions page
workflowContext.outPandasDataframe(id, "Pandas DataFrame", pdf, 10)

# Create a Spark DataFrame from a Pandas DataFrame
df = spark.createDataFrame(pdf)

return df

h4: Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe

from pyspark.sql.types import StringType
from pyspark.sql.functions import *
from pyspark.sql import *
import numpy as np
import pandas as pd

from fire.workflowcontext import *
def myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):

# Create the numpy 2d array
example_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])

# Convert to Pandas Dataframe
pandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])

# Convert Pandas Dataframe to Spark Dataframe
spark_dataframe = spark.createDataFrame(pandas_dataframe)
return spark_dataframe

end-examples:
