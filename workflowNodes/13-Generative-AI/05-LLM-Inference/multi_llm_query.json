{
  "id": "6",
  "name": " Multi LLM Query",
  "iconImage": "/images/icons/node-icon/Graph_group_by_column.svg",
  "description": "The Multi LLM Query node is designed to query multiple large language models (LLMs) from providers such as OpenAI, Bedrock, and Gemini, using a DataFrame as input. It processes user queries, text content, and/or base64-encoded images to generate responses based on the selected model and task, producing a structured DataFrame output.",
  "input": "It may take in Dataframe as an input",
  "output": "Returns response as Dataframe",
  "type": "pyspark",
  "engine": "pyspark",
  "nodeClass": "fire.nodes.gai.NodeMultiLLMQuery",

  "fields": [
    {"name": "llmConnection", "value": "", "widget": "object_array", "title": "Select Connection", "description": "Select Connection"},
    {"name": "temperature", "value": "0", "widget": "textfield", "title": "Temperature", "description": "Temperature setting for the model (default: 0)."},
    {"name": "contentCol", "value": "", "widget": "variable", "title": "Content Column", "description": "Column name for the text content."},
    {"name": "imageCol", "value": "", "widget": "variable", "title": "Image Column", "description": "Column name for the base 64 image."},
    {"name": "inputMode", "value": "text", "widget": "array", "optionsArray": ["text", "image", "text+image"], "title": "Mode Selection", "description": "Select the model to use (text, image, text+image)."},

    {"name": "Prompt", "value": "", "widget": "tab", "title": "Prompt"},
    {"name": "task", "value":"other", "widget": "key_value_array", "title": "Select Prompt", "optionsMap": {"summary":"Generate a concise, multi-level bullet-point summary capturing key facts, insights, and implications from the content. Preserve structure and section hierarchy.","translation":"Translate the content into fluent, formal English while preserving tone, context, cultural nuances, and domain-specific terminology (e.g., legal, medical, technical).","topic_extraction": "Identify and extract key topics, subtopics, and entities. Categorize them with tags (e.g., Person, Location, Concept) and provide brief descriptions or summaries of each.", "other": ""}, "description": "Specify the task to perform: summary, translation, topic extraction, or other.", "keyFields":"conditionExpr"},
    {"name": "customPrompt", "value": "", "widget": "textareafield", "title": "Prompt", "description": "Custom prompt to override the default instructions."},
    {"name": "userQueryCol", "value": "", "widget": "variable", "title": "User Query Column", "description": "Column name for user query, (if the query is in a column)"},

    {"name": "Advanced", "value": "", "widget": "tab", "title": "Advanced"},
    {"name":"aggregateMode", "value":"ALL", "widget": "enum", "title": "Aggregate Response", "optionsMap":{"NONE":"Do not aggregate rows","ALL":"Aggregate all rows", "PERFILE":"Aggregate per file"}},
    {"name": "numPartitions", "value": "0", "widget": "textfield", "title": "Number of Partitions", "description": "Number of Partitions"},
    {"name": "fileNameCol", "value": "", "widget": "variable", "title": "File Name Column", "description": "Select File Name Column"},
    {"name": "pageNumberCol", "value": "", "widget": "variable", "title": "Page Number Column", "description": " Select Page Number column."},
    {"name": "timeout", "value": "180", "widget": "textfield", "title": "Timeout (seconds)", "description": "Maximum time to wait for Openai and Gemini API response"},
    {"name": "thinkingBudget", "value": "-1", "widget": "textfield", "title": "Thinking Budget", "description": "Configure the Gemini thinking budget by specifying the number of tokens to allocate for thinking. For Flash and Flash Lite models, values can range from 0 to 24,576 or -1 for dynamic thinking. For 2.5 Pro model, values must be between 1 and 24,576; setting 0 is not allowed."}
  ]
}

start-details:

h2:Multi LLM Query Node Details

The Multi LLM Query node is designed to query multiple large language models (LLMs) from providers such as OpenAI, Bedrock, and Gemini, using a DataFrame as input. It processes user queries, text content, and/or base64-encoded images to generate responses based on the selected connection and task, producing a structured DataFrame output.

h3:General:

h4:Select Task:
Specifies the task to perform. Options include:
- summary: Generates a summary of the content in bullet points.
- translation: Translates the content to English.
- topic_extraction: Extracts key topics from the content.
- other: Allows for a custom task defined by the user.

h4:Prompt:
Allows users to provide a custom prompt / instructions for the selected task.

h4:Content Column:
Specifies the DataFrame column containing the text content to be processed. Required for text or text+image modes.

h4:Select Connection:
Specifies the connection details for the selected LLM provider (e.g., API keys for OpenAI/Gemini, AWS credentials for Bedrock). Required to authenticate and access the respective model.

h4:Temperature:
Controls the randomness of the LLM's output. Default is 0.7. Higher values increase creativity, while lower values ensure more deterministic responses.

h4:Image Column:
Specifies the DataFrame column containing base64-encoded images. Required for image or text+image modes.

h4:Mode Selection:
Determines the input mode for the LLM. Options are:
- text: Processes text-only input from the content column or custom prompt.
- image: Processes base64-encoded images from the image column.
- text+image: Processes both text and base64-encoded images.

h4:Timeout (seconds):
Specifies the maximum time (in seconds) to wait for the model response. Visible when OpenAI or Gemini is selected.

h4:Thinking Budget:
Controls the computational budget (e.g., steps or tokens) for Gemini models. Only visible when Gemini is selected.

h3:Advanced:

h4:Aggregate Response:
Specifies how to aggregate input data before processing. Options are:
- none: Processes each row individually, retaining fileName and pageNumber (if provided).
- all: Aggregates all rows into a single response.
- perfile: Aggregates rows by fileName, producing one response per file.

h4:Number of Partitions:
Specifies the number of Spark partitions for distributed processing. Default is 3.

h4:File Name Column:
Specifies the DataFrame column containing file names. Required for perfile aggregation mode.

h4:Page Number Column:
Specifies the DataFrame column containing page numbers (e.g., for PDFs). Optional, used for row-wise processing with none aggregation mode.

h3:Output:
The node outputs a DataFrame with columns based on the aggregation mode:
- none: Includes fileName (if provided), pageNumber (if provided), and response.
- perfile: Includes fileName and response.
- all: Includes only the response column.
The response column contains the LLM-generated text or error messages if the API call fails.

end-details:

start-examples:

h2:Multi LLM Query Node Examples

h3:Input:
A DataFrame contains the following data:
- fileName: ["doc1.pdf", "doc1.pdf", "doc2.pdf"]
- pageNumber: ["1", "2", null]
- content: ["Article about climate change...", "Climate change impacts...", "Renewable energy report..."]
- imageBase64: [null, "iVBORw0KGgoAAAANSUhEUg...", null]

The Multi LLM Query node is configured as follows:
- Select Task: summary
- Prompt: "Summarize the content in bullet points."
- Content Column: content
- Select Connection: Configured with valid OpenAI API key
- Temperature: 0.7
- Timeout (seconds): 90
- Image Column: imageBase64
- Mode Selection: text+image
- Aggregate Response: perfile
- Number of Partitions: 3
- File Name Column: fileName
- Page Number Column: pageNumber

h3:Output:
The node processes the DataFrame and produces a DataFrame with the following structure:
- fileName: doc1.pdf
response:
- Climate change effects on ecosystems
- Rising temperatures

- fileName: doc2.pdf
response:
- Renewable energy advancements
- Solar and wind adoption

end-examples: