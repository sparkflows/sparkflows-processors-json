{
  "id": "7",
  "name": "Interactive LLM Agent",
  "iconImage": "/images/icons/node-icon/Graph_group_by_column.svg",
  "description": "This node enables standalone or dataframe-optional LLM queries across multiple providers (OpenAI, Bedrock, Gemini). It is designed for sequential agent flows like 'Similar Company Finder' and supports saving structured responses.",
  "input": "Optional DataFrame (if metadata columns are selected)",
  "output": "Returns response as DataFrame",
  "type": "pyspark",
  "engine": "pyspark",
  "nodeClass": "fire.nodes.gai.NodeInteractiveLLMAgent",

  "fields": [
    {"name": "llmConnection", "value": "", "widget": "object_array", "title": "Select Connection", "description": "Select Connection"},
    {"name": "customPrompt", "value": "", "widget": "textareafield", "title": "Prompt", "description": "Custom prompt to instruct the model."},
    {"name": "metadataCols", "value": "[]", "widget": "variables", "title": "Metadata Columns", "description": "Select one or more content columns to pass as input to the model."}

  ]
}
start-details:

<h2>Interactive LLM Agent Node Details</h2>
The Interactive LLM Agent node enables querying large language models (LLMs) such as OpenAI, Bedrock (Anthropic), and Gemini from Google, using either a standalone prompt or content from a DataFrame. It is designed for flexible agent workflows and provides safe input and output validation.

<h4>General:</h4>

<h5>Custom Prompt:</h5>
This field lets you specify the user query or task description. If the node is used without a DataFrame, this prompt is the only content passed to the LLM.

<h5>Metadata Columns:</h5>
When a DataFrame is passed into the node, you can select one or more content columns whose text will be sent to the LLM. The prompt and content will be combined and sent to the model.


<h5>Select Connection:</h5>
Each model provider requires specific connection credentials (e.g., API keys). These must be configured in the backend and are referenced here.

<h5>Temperature, Max Tokens, and Retries:</h5>
<ul>
<li><b>Temperature</b> controls creativity: lower values yield more deterministic outputs.</li>
<li><b>Max Tokens</b> sets the maximum length of the generated response.</li>
<li><b>Retries</b> determine how many times to retry in case of failure.</li>
</ul>

<h4>Output:</h4>
The output is a Spark DataFrame with a single column:
<ul>
<li><b>response</b>: Contains the LLM-generated response, cleaned and validated.</li>
</ul>

If a DataFrame is passed in with metadata columns, their values are merged into the prompt. Otherwise, the prompt is used as-is.



end-details:

start-examples:

<h2>Example: Interactive LLM Agent Node</h2>

<h3>Input:</h3>
A DataFrame with the following data:
<table border="1">
<tr><th>company_name</th><th>industry</th></tr>
<tr><td>TechCorp</td><td>Software</td></tr>
<tr><td>InnovateAI</td><td>Artificial Intelligence</td></tr>
</table>

<h3>Configuration:</h3>
<ul>
<li><b>Custom Prompt:</b> "Find companies similar to {company_name} in the {industry} industry."</li>
<li><b>Metadata Columns:</b> [company_name, industry]</li>
<li><b>Temperature:</b> 0.7</li>
<li><b>Max Tokens:</b> 512</li>
<li><b>Retries:</b> 2</li>
</ul>

<h3>Output:</h3>
A DataFrame with a single response column:
<table border="1">
<tr><th>response</th></tr>
<tr><td>TechCorp (Software): - CodeZap - SoftPeak - Nexlify<br/>InnovateAI (Artificial Intelligence): - AIWorks - NeuralNest - DeepMind</td></tr>
</table>

<h3>Explanation:</h3>
<ul>
<li>The node reads values from metadata columns and constructs a prompt for each row.</li>
<li>The content and task are passed to the OpenAI LLM.</li>
<li>The response is generated, cleaned of Markdown wrappers (e.g., ```json)</li>
<li>The output is returned in a DataFrame as a single response string (across all rows).</li>
</ul>


end-examples:
