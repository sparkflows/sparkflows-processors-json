{
  "name": "Product-Review-Embeddings",
  "uuid": "87969b93-06b1-4abe-aa82-14a9e4003eef",
  "category": "-",
  "description": "-",
  "parameters": "--var querystr='what are defects about products reported by customers' --var execute=true --var genAiResponse= --var destinationPath=/home/sparkflows/fire-data/data/Product_Review_Data/Review_Input/Skechers_Review_text.txt",
  "nodes": [
    {
      "id": "1",
      "name": "PySpark",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "<h2>Pyspark Details</h2>\n<br>\nThis node receives receives an input pyspark dataframe in function called myfn.<br>\n<br>\nThe pyspark/python code processes it and returns one computed pyspark dataframe.<br>",
      "examples": "<h2>Pyspark Examples</h2>\n<br>\nInput Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> Add the house_type column</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\nhouse_type_udf = udf(lambda bedrooms: \"big house\" if int(bedrooms) >2 else \"small house\", StringType())<br>\nfiletr_df = inDF.select(\"id\", \"price\", \"lotsize\", \"bedrooms\")<br>\noutDF = filetr_df.withColumn(\"house_type\", house_type_udf(filetr_df.bedrooms))<br>\nreturn outDF<br>\n<br>\n<h4> Using pandas dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\n<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n# Convert the Spark DataFrame to a Pandas DataFrame<br>\npdf = inDF.select(\"*\").toPandas()<br>\n<br>\n# Display the result on the Executions page<br>\nworkflowContext.outStr(id, \"Outputting Pandas Dataframe\")<br>\n<br>\n# Display the dataframe on the Executions page<br>\nworkflowContext.outPandasDataframe(id, \"Pandas DataFrame\", pdf, 10)<br>\n<br>\n# Create a Spark DataFrame from a Pandas DataFrame<br>\ndf = spark.createDataFrame(pdf)<br>\n<br>\nreturn df<br>\n<br>\n<h4> Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nimport numpy as np<br>\nimport pandas as pd<br>\n<br>\nfrom fire.workflowcontext import *<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n<br>\n# Create the numpy 2d array<br>\nexample_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])<br>\n<br>\n# Convert to Pandas Dataframe<br>\npandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])<br>\n<br>\n# Convert Pandas Dataframe to Spark Dataframe<br>\nspark_dataframe = spark.createDataFrame(pandas_dataframe)<br>\nreturn spark_dataframe<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "68.8122px",
      "y": "47.8466px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext\nfrom langchain.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import PyPDFLoader, DirectoryLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom pathlib import Path\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.document_loaders import TextLoader\n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n  OPENAI_API_KEY = '$Openai_Api' #\"sk-proj-BoMiS6jSiXTt1udNolZhT3BlbkFJrRrR7BxptsiII8IwPAGO\"\n  file_path = '${destinationPath}'\n  path = Path(file_path)\n  file_extension = path.suffix.lower()\n  if file_extension == '.csv':\n    loader = CSVLoader(file_path)\n      # raw_documents = loader.load()\n  elif file_extension == '.pdf':\n    loader = PyPDFLoader(file_path)\n  else:\n      loader = TextLoader(file_path)\n      # raw_documents = loader.load()\n  loader = PyPDFLoader(\"data/GENAI/GenAI-Applications/Product-Review-Analysis/Review-Input/Skechers-Review-text.pdf\")\n  raw_documents = loader.load()\n  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 10)\n  all_splits = text_splitter.split_documents(raw_documents)\n  embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY\n      )\n  # Build and persist FAISS vector store\n  vectorstore = FAISS.from_documents(all_splits, embeddings)\n  vectorstore.save_local('data/GENAI/GenAI-Applications/Product-Review-Analysis/Vectorstore/DB-Sketchers-faiss')\n  from langchain.chains import RetrievalQA  \n  from langchain.chat_models import ChatOpenAI\n  llm = ChatOpenAI(\n          openai_api_key = OPENAI_API_KEY,\n          model_name='gpt-3.5-turbo',\n          temperature=0.0\n      )\n  retriver = vectorstore.as_retriever()\n  chain = RetrievalQA.from_chain_type(llm = llm, retriever = retriver, verbose=False, chain_type = \"stuff\")\n  # workflowContext.outStr(3,chain,\"Chain\")\n  text = chain.run('What are negative and positive review regarding products')#\"${querystr}\")\n  text1 = text[:100]\n  workflowContext.outStr(3,text1,\"text1\")\n  text2 = text[-100:]\n  workflowContext.outStr(3,text2,\"text2\")\n  output_path = 'data/GENAI/GenAI-Applications/Product-Review-Analysis/Query-Output/output.txt'\n  with open(output_path, 'w') as file:\n    file.write(text)\n  workflowContext.outStr(3,text,\"Query_Output\")\n  workflowContext.outStr(3,\"Saved file at : \"+output_path,\"Saved Summaries\")\n  # return outDF",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [],
  "dataSetDetails": [],
  "engine": "pyspark"
}