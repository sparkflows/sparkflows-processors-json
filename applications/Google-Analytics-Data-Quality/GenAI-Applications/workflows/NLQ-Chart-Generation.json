{
  "name": "NLQ-Chart-Generation",
  "uuid": "d7ac9e25-cb6e-43b0-a05c-4797fd719d55",
  "category": "Text-2-SQL",
  "description": "-",
  "parameters": " --var getStarted=true --var viewDatabaseSchema=true --var selectQueryOptions=1 --var option=openAi --var sampleQueries=\"Show the total sales amount for each customer group.\" --var selectQueryOptions=2 --var query=List all products",
  "nodes": [
    {
      "id": "1",
      "name": "PySpark",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "<h2>Pyspark Details</h2>\n<br>\nThis node receives receives an input pyspark dataframe in function called myfn.<br>\n<br>\nThe pyspark/python code processes it and returns one computed pyspark dataframe.<br>",
      "examples": "<h2>Pyspark Examples</h2>\n<br>\nInput Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> Add the house_type column</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\nhouse_type_udf = udf(lambda bedrooms: \"big house\" if int(bedrooms) >2 else \"small house\", StringType())<br>\nfiletr_df = inDF.select(\"id\", \"price\", \"lotsize\", \"bedrooms\")<br>\noutDF = filetr_df.withColumn(\"house_type\", house_type_udf(filetr_df.bedrooms))<br>\nreturn outDF<br>\n<br>\n<h4> Using pandas dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\n<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n# Convert the Spark DataFrame to a Pandas DataFrame<br>\npdf = inDF.select(\"*\").toPandas()<br>\n<br>\n# Display the result on the Executions page<br>\nworkflowContext.outStr(id, \"Outputting Pandas Dataframe\")<br>\n<br>\n# Display the dataframe on the Executions page<br>\nworkflowContext.outPandasDataframe(id, \"Pandas DataFrame\", pdf, 10)<br>\n<br>\n# Create a Spark DataFrame from a Pandas DataFrame<br>\ndf = spark.createDataFrame(pdf)<br>\n<br>\nreturn df<br>\n<br>\n<h4> Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nimport numpy as np<br>\nimport pandas as pd<br>\n<br>\nfrom fire.workflowcontext import *<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n<br>\n# Create the numpy 2d array<br>\nexample_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])<br>\n<br>\n# Convert to Pandas Dataframe<br>\npandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])<br>\n<br>\n# Convert Pandas Dataframe to Spark Dataframe<br>\nspark_dataframe = spark.createDataFrame(pandas_dataframe)<br>\nreturn spark_dataframe<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "119.889px",
      "y": "168.889px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \nimport re\nimport requests\nimport json\nimport os\nimport pg8000\nimport pg8000.native\nimport pandas as pd\n\n\n'''customer_data_schema = \"\"\"CREATE TABLE Customer (\n    customer_id SERIAL PRIMARY KEY,\n    customer_name VARCHAR(255) NOT NULL,\n    customer_group VARCHAR(255) NOT NULL,\n    customer_address TEXT NOT NULL,\n    customer_email VARCHAR(255) NOT NULL,\n    customer_phone VARCHAR(20) NOT NULL\n);\"\"\"\n\nproduct_data_schema = \"\"\"CREATE TABLE Product (\n    product_id SERIAL PRIMARY KEY,\n    product_name VARCHAR(255) NOT NULL,\n    product_group VARCHAR(255) NOT NULL,\n    product_price NUMERIC(10, 2) NOT NULL,\n    product_description TEXT NOT NULL,\n    product_stock INTEGER NOT NULL\n);\"\"\"\n\nsales_schema = \"\"\"CREATE TABLE Sales (\n    sale_id SERIAL PRIMARY KEY,\n    invoice_number UUID NOT NULL,\n    order_number UUID NOT NULL,\n    customer_id INTEGER NOT NULL,\n    product_id INTEGER NOT NULL,\n    sale_date DATE NOT NULL,\n    quantity INTEGER NOT NULL,\n    transaction_amount NUMERIC(10, 2) NOT NULL,\n    discount NUMERIC(5, 2) NOT NULL,\n    payment_term VARCHAR(50) NOT NULL,\n    contract_period VARCHAR(50) NOT NULL\n);\"\"\"\n\nsystem_prompt = f\"\"\"----------- Customer Schema ------------\n{customer_data_schema}\n\n------------ Product Schema -----------------\n{product_data_schema}\n\n------------- Sales Schema ----------------\n{sales_schema}\"\"\"'''\n\ndef get_data_from_sql(sql):\n  print(\"Connection Begin\")\n  con = pg8000.native.Connection(\n    host=\"postgres-developmentdb.postgres.database.azure.com\",\n    port=5432,\n    database=\"erp_finance\",\n    user=\"sparkflows\",\n    password=\"abcninded\",\n\tssl_context=True)\n\n  data = con.run(sql)\n  columns = [c['name'] for c in con.columns]\n  \n  df_data = pd.DataFrame(data,columns = columns)\n  #df_data = spark.createDataFrame(data, schema=columns)\n  return df_data\n\nschema_query = \"\"\"SELECT table_schema, table_name, column_name, data_type\nFROM information_schema.columns\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema')\nORDER BY table_schema, table_name, ordinal_position;\n\"\"\"\nschema = get_data_from_sql(schema_query)\n\nsystem_prompt = f\"\"\"----------- Database Schema ------------\n{schema.to_string(index=False)}\n\"\"\"\n\ngemini_history=[\n  {\n    \"role\": \"user\",\n    \"parts\":[{\"text\": system_prompt}]\n  },\n  {\n      \"role\": \"user\",\n      \"parts\": [{\"text\":\"\"\"You are very much expert to convert a natural english language into Postgre-SQL. I have provided all the table schema with their DDL statement. Any question comes, you have to create a postgre sql query correspoding to that.\n\nfor summary of the schema relation: \n\nSales.customer_id = Customer.customer_id\nSales.product_id = Product.product_id\"\"\"}]\n  },\n  {\n      \"role\": \"model\",\n      \"parts\": [{\"text\":\"Got it. Do you have any example?\"}]\n  },\n  {\n      \"role\": \"user\",\n      \"parts\": [{\"text\":\"\"\"For example: \nNLQ 1: Show the total sales amount for each customer group.\n      \nAnswer 1 :  \nSELECT customer_group, SUM(transaction_amount) AS total_sales\nFROM \"Sales\"\nJOIN \"Customer\" ON \"Sales\".customer_id = \"Customer\".customer_id\nGROUP BY customer_group;\n\nNLQ 2: What is the total sales amount for each product group? \n\nAnswer 2:\nSELECT product_group, SUM(transaction_amount) AS total_sales\nFROM \"Sales\"\nJOIN \"Product\" ON \"Sales\".product_id = \"Product\".product_id\nGROUP BY product_group; \n \"\"\"}]\n  }]\n\nopenai_history = [\n  {\n    \"role\": \"system\",\n    \"content\": system_prompt\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"\"\"You are very much expert to convert a natural English language into PostgreSQL. I have provided the database schema with a table format. Any question comes, you have to create a PostgreSQL query corresponding to that.\n\nFor the schema relations: \n\n\"Sales\".customer_id = \"Customer\".customer_id\n\"Sales\".product_id = \"Product\".product_id\"\"\"\n  },\n  {\n    \"role\": \"assistant\",\n    \"content\": \"Got it. Do you have any example?\"\n  },\n  {\n    \"role\": \"user\",\n    \"content\": \"\"\"For example: \nNLQ 1: Show the total sales amount for each customer group.\n      \nAnswer 1 :  \nSELECT customer_group, SUM(transaction_amount) AS total_sales\nFROM \"Sales\"\nJOIN \"Customer\" ON \"Sales\".customer_id = \"Customer\".customer_id\nGROUP BY customer_group;\n\nNLQ 2: What is the total sales amount for each product group? \n\nAnswer 2:\nSELECT product_group, SUM(transaction_amount) AS total_sales\nFROM \"Sales\"\nJOIN \"Product\" ON \"Sales\".product_id = \"Product\".product_id\nGROUP BY product_group; \n\"\"\"\n  }\n]\n\ndef extract_sql(text):\n  regex = r\"(?s)^.*?```(sql|[^`]+?)`(.*?)`.*?$\"\n  match = re.search(regex, text, re.DOTALL)\n  if match:\n    return match.group(1)\n  return None\n\ndef get_sql(history, option):\n  if option == \"Gemini\":\n    # Replace with your actual Google API key\n    GOOGLE_API_KEY = \"$Gemini_Api\"\n\n    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GOOGLE_API_KEY}\"\n    headers = {\n        'Content-Type': 'application/json',\n    }\n    data = {\n        \"contents\": history\n    }\n\n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    response_json = response.json()\n\n    if not response_json.get(\"candidates\"):\n      print(\"Something went wrong\")\n    else:\n      for content in response_json.get(\"candidates\"):\n        text = content.get(\"content\").get(\"parts\")[0].get(\"text\")\n  else:\n    OPENAI_API_KEY = \"$Openai_Api\"\n    url = \"https://api.openai.com/v1/chat/completions\"\n    headers = {\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {OPENAI_API_KEY}',\n    }\n\n    # Prepare the data in the format expected by OpenAI's API\n    data = {\n        \"model\": \"gpt-4o\",\n        \"messages\": history,\n        \"max_tokens\": 500,\n        \"temperature\": 0.5\n    }\n\n    # Make the request to the OpenAI API\n    response = requests.post(url, headers=headers, data=json.dumps(data))\n\n    if response.status_code == 200:\n        response_json = response.json()\n\n        # Extract the content of the assistant's response\n        text = response_json['choices'][0]['message']['content']\n  \n  sql = extract_sql(text)\n  if sql:\n    if \"sql\" in sql:\n      return sql[4:]\n    else:\n      return sql\n  else:\n    return \"No SQL code found\"\n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n  option = '${selectQueryOptions}'\n  if option == \"2\":\n     raw_query = '''${query}'''\n  else:\n     raw_query = '${sampleQueries}'   \n  option = '''${option}'''\n  query_text = f\"\"\"{raw_query}. Whenever you use a table name always keep it inside double quataion. For example: \"Product\", \"Sales\", \"Customer\".\"\"\"\n  if option == \"Gemini\":\n    query = {\n          \"role\": \"user\",\n          \"parts\": [{\n            \"text\": query_text\n          }]\n      }\n    gemini_history.append(query)\n    history = gemini_history\n  else:\n    query={\n        \"role\": \"user\",\n        \"content\": query_text\n    }  \n    openai_history.append(query)\n    history = openai_history\n  try:\n    sql = get_sql(history, option)\n    processed_sql = sql.replace(\"\\n\", \" \").replace(\"  \",\" \")\n    df = get_data_from_sql(processed_sql)\n    nlq_folder = \"data/GENAI/GenAI-Applications/NLQ-Reports/nlq\"\n\n    if not os.path.exists(nlq_folder):\n        os.makedirs(nlq_folder)\n        print(f\"Response folder '{nlq_folder}' created successfully.\")\n    else:\n        print(f\"Response folder '{nlq_folder}' already exists.\")\n        \n    df.to_csv(\"data/GENAI/GenAI-Applications/NLQ-Reports/nlq/result.csv\", index=False)\n    print(raw_query)\n    print(processed_sql)\n    workflowContext.outStr(3,processed_sql, raw_query.replace(\"'\",\"\") )\n    print(\"==================\")\n    print(df)\n    try:\n    \toutDF = spark.createDataFrame(df)  #add custom logic\n    except:\n      schema = StructType([StructField(col, StringType(), True) for col in df.columns])\n      empty_rdd = spark.sparkContext.emptyRDD()\n      outDF = spark.createDataFrame(empty_rdd, schema)\n    return outDF\n  except:\n    error_data = pd.DataFrame({\"Status\": \"Error\",\"Message\":\"Something Went Wrong. Please try again\"},index=[0])\n    error_data.to_csv(\"data/genai/Chart-generation-Gen-AI/nlq/result.csv\", index=False)\n    outDF = spark.createDataFrame(error_data)  #add custom logic\n    return outDF",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    },
    {
      "id": "2",
      "name": "Print N Rows",
      "iconImage": "fa fa-tumblr-square",
      "description": "Prints the specified number of records in the DataFrame. It is useful for seeing intermediate output",
      "details": "<h2>Print N Rows Node Details</h2>\n<br>\nThis node is used to print the first N rows from the incoming dataframe.<br>\n<br>\nThe Number of rows that needs to be printed can be configured in the node.<br>\n<br>\n<h4>Input Parameters</h4>\n<ul>\n<li> OUTPUT STORAGE LEVEL : Keep this as DEFAULT.</li>\n<li> TITLE : Enter a short description for the type of information being displayed.</li>\n<li> NUM ROWS TO PRINT : Set an integer value(N) which controls the number of rows to be displayed(Default N=10).</li>\n<li> DISPLAY DATA TYPE : Shows the output dataframe column datatypes by default.</li>\n</ul>\n<h4>Output</h4>\n<ul>\n<li> This node can be used to view, analyze and validate the output of the Dataframe.</li>\n</ul>",
      "examples": "when input 5 in no of rows,it will show first 5 rows of the table as follows<br>\n<br>\nPartID\tSupplierID\tPartName\t<br>\n<br>\nP9271\t  S798\t    Part_D\t<br>\nP523\t  S955\t    Part_K\t<br>\nP3201\t  S332\t    Part_M\t<br>\nP9634\t  S527\t    Part_G\t<br>\nP9345\t  S850\t    Part_M<br>",
      "type": "transform",
      "nodeClass": "fire.nodes.util.NodePrintFirstNRows",
      "x": "272.889px",
      "y": "167.889px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "title",
          "value": "Result",
          "widget": "textfield",
          "title": "Title",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "n",
          "value": "100",
          "widget": "textfield",
          "title": "Num Rows to Print",
          "description": "number of rows to be printed",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "displayDataType",
          "value": "",
          "widget": "array",
          "title": "Display Data Type",
          "description": "If true display rows DataType",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "3",
      "name": "PySpark",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "<h2>Pyspark Details</h2>\n<br>\nThis node receives receives an input pyspark dataframe in function called myfn.<br>\n<br>\nThe pyspark/python code processes it and returns one computed pyspark dataframe.<br>",
      "examples": "<h2>Pyspark Examples</h2>\n<br>\nInput Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> Add the house_type column</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\nhouse_type_udf = udf(lambda bedrooms: \"big house\" if int(bedrooms) >2 else \"small house\", StringType())<br>\nfiletr_df = inDF.select(\"id\", \"price\", \"lotsize\", \"bedrooms\")<br>\noutDF = filetr_df.withColumn(\"house_type\", house_type_udf(filetr_df.bedrooms))<br>\nreturn outDF<br>\n<br>\n<h4> Using pandas dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\n<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n# Convert the Spark DataFrame to a Pandas DataFrame<br>\npdf = inDF.select(\"*\").toPandas()<br>\n<br>\n# Display the result on the Executions page<br>\nworkflowContext.outStr(id, \"Outputting Pandas Dataframe\")<br>\n<br>\n# Display the dataframe on the Executions page<br>\nworkflowContext.outPandasDataframe(id, \"Pandas DataFrame\", pdf, 10)<br>\n<br>\n# Create a Spark DataFrame from a Pandas DataFrame<br>\ndf = spark.createDataFrame(pdf)<br>\n<br>\nreturn df<br>\n<br>\n<h4> Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nimport numpy as np<br>\nimport pandas as pd<br>\n<br>\nfrom fire.workflowcontext import *<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n<br>\n# Create the numpy 2d array<br>\nexample_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])<br>\n<br>\n# Convert to Pandas Dataframe<br>\npandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])<br>\n<br>\n# Convert Pandas Dataframe to Spark Dataframe<br>\nspark_dataframe = spark.createDataFrame(pandas_dataframe)<br>\nreturn spark_dataframe<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "254.133px",
      "y": "394.133px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \nimport time\nimport requests\nimport base64\nimport uuid\nimport pandas as pd\nopen_ai_api_key = \"$Openai_Api\"\n\ndef create_thread(ci_prompt):\n  url = \"https://api.openai.com/v1/threads\"\n\n  # Headers\n  headers = {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": f\"Bearer {open_ai_api_key}\",\n      \"OpenAI-Beta\": \"assistants=v2\"\n  }\n  # Empty payload as specified in the curl command\n  payload = {\"messages\":[\n                {\n                    \"role\": \"user\",\n                    \"content\": ci_prompt,\n                }\n            ]}\n\n  # Make the POST request\n  response = requests.post(url, headers=headers, json=payload)\n  print(\"thread\",response)\n  # Handle the response\n  if response.status_code == 200:\n      thread_data = response.json()\n      thread_id = thread_data['id'] \n      return thread_id\n  else:\n    return False\n\ndef run_thread(thread_id, assistant_id):\n  url = f\"https://api.openai.com/v1/threads/{thread_id}/runs\"\n\n  # Headers\n  headers = {\n      \"Authorization\": f\"Bearer {open_ai_api_key}\",\n      \"Content-Type\": \"application/json\",\n      \"OpenAI-Beta\": \"assistants=v2\"\n  }\n\n  # Payload data\n  payload = {\n      \"assistant_id\": assistant_id\n  }\n\n  # Make the POST request\n  response = requests.post(url, headers=headers, json=payload)\n  print(\"run thread\",response)\n  if response.status_code == 200:\n    run_data = response.json()\n    print(\"run_data\", run_data)\n    run_id = run_data['id']\n    return run_id\n  else:\n    print(response.json())\n    return False\n\ndef retrive_run(thread_id,run_id ):\n  url = f\"https://api.openai.com/v1/threads/{thread_id}/runs/{run_id}\"\n\n  # Headers\n  headers = {\n      \"Authorization\": f\"Bearer {open_ai_api_key}\",\n      \"OpenAI-Beta\": \"assistants=v2\"\n  }\n  try:\n    # Make the GET request\n    response = requests.get(url, headers=headers)\n    # Handle the response\n    run_data = response.json()\n    return run_data\n  except:\n    return False\n\ndef get_message(thread_id):\n  url = f\"https://api.openai.com/v1/threads/{thread_id}/messages?order=desc\"\n\n  # Headers\n  headers = {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": f\"Bearer {open_ai_api_key}\",\n      \"OpenAI-Beta\": \"assistants=v2\"\n  }\n  try:\n    # Make the GET request\n    response = requests.get(url, headers=headers)\n    print(\"message\",response)\n    # Handle the response\n  \n    message_data = response.json()\n    return message_data\n  except:\n    return False\n\ndef get_file(file_id, chart_path):\n  url = f\"https://api.openai.com/v1/files/{file_id}/content\"\n\n  # Headers\n  headers = {\n      \"Authorization\": f\"Bearer {open_ai_api_key}\"\n  }\n\n  # Make the GET request\n  response = requests.get(url, headers=headers)\n  print(\"file\",response)\n  # Handle the response\n  if response.status_code == 200:\n      # Save the content to file.jsonl\n      with open(chart_path, 'wb') as f:\n          f.write(response.content)\n      print(f\"File content saved to file.jsonl\")\n      return True\n  else:\n    return False\n\ndef delete_file(file_id):\n  url = f\"https://api.openai.com/v1/files/{file_id}\"\n\n  # Headers\n  headers = {\n      \"Authorization\": f\"Bearer {open_ai_api_key}\"\n  }\n\n  # Make the DELETE request\n  response = requests.delete(url, headers=headers)\n  print(\"delete file\",response)\n  # Handle the response\n  if response.status_code == 200:\n      print(f\"File {file_id} deleted successfully.\")\n  elif response.status_code == 404:\n      print(f\"File {file_id} not found.\")\n  else:\n    print(\"Something Went Wrong\")\n\ndef delete_message(message_id, thread_id):\n  url = f\"https://api.openai.com/v1/threads/{thread_id}/messages/{message_id}\"\n\n  # Headers\n  headers = {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": f\"Bearer {open_ai_api_key}\",\n      \"OpenAI-Beta\": \"assistants=v2\"\n  }\n\n  # Make the DELETE request\n  response = requests.delete(url, headers=headers)\n\n  if response.status_code == 200:\n    print(\"Messages Deleted\")\n  else:\n    print(\"Something went wrong in messages deletion\")\n\ndef delete_thread(thread_id):\n  url = f\"https://api.openai.com/v1/threads/{thread_id}\"\n\n  # Headers\n  headers = {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": f\"Bearer {open_ai_api_key}\",\n      \"OpenAI-Beta\": \"assistants=v2\"\n  }\n\n  # Make the DELETE request\n  response = requests.delete(url, headers=headers)\n\n  if response.status_code == 200:\n    print(\"Thread Deleted\")\n  else:\n    print(\"Something went wrong in thread deletion\")\n    \n    \ndef get_charts(query, chart_path, final_data):\n  prompt = \"You are an expert at Chart Generation, for any given structure data, you will perform chart generation and return the chart based on the input data provided.\\n\\n\"\n  #query = f\"\"\"This is the qury text: \n#{query_text}\\n\"\"\"\n  ci_prompt = prompt + query+ \"\\nHere is the data\\n\" +str(final_data)\n\n  thread_id = create_thread(ci_prompt)\n\n  if not thread_id:\n    print(\"There is an error in generating the thread id\")\n    return\n  \n  run_id = run_thread(thread_id, \"asst_C9JRq8N5boAP8GvfpDHdM5Tx\")\n  \n  if not run_id:\n    print(\"There is an error in running the thread\")\n    return\n\n  while True:\n    run_data = retrive_run(thread_id,run_id )\n    print(\"retrive_run\", run_data)\n    if not run_data:\n      print(\"There is an error in getting the retrive data\")\n      return\n    \n    if run_data[\"status\"] == \"completed\":\n      messages = get_message(thread_id)\n      if not messages:\n        print(\"Something went wrong on getting reponse from LLM\")\n      \n      print(messages)\n      image_file_id = messages[\"data\"][0][\"content\"][0][\"image_file\"][\"file_id\"]\n      content_description = messages[\"data\"][0][\"content\"][1][\"text\"][\"value\"]\n\n      raw_response = get_file(image_file_id, chart_path)\n      if not raw_response:\n        print(\"Something went wrong on generating the charts\")\n        return\n      delete_file(image_file_id)\n      messages_id = [x[\"id\"] for x in messages[\"data\"]]\n      print(messages_id)\n      for i in messages_id:\n        delete_message(i,thread_id)\n      delete_thread(thread_id)\n      return 1\n      break\n\n    elif run_data[\"status\"] == \"failed\":\n      print(\"Unable to generate chart\",\"Error\")\n      return 0\n\n    # Wait for a short period before polling again to avoid hitting rate limits\n    time.sleep(3)\n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n    final_data = pd.read_csv(\"data/GENAI/GenAI-Applications/NLQ-Reports/nlq/result.csv\")#add custom logic\n    print(final_data)\n    print(final_data.columns)\n    if \"Status\" in final_data.columns and \"Message\" in final_data.columns:\n      return\n    query = '''${query}'''\n    uid = uuid.uuid4().hex\n    chart_path = f\"data/GENAI/GenAI-Applications/NLQ-Reports/charts{uid}.png\"\n    status = get_charts(query, chart_path, final_data)\n    if status == 1:\n      with open(chart_path, 'rb') as image_file:\n          binary_image = image_file.read()\n      base64_image = base64.b64encode(binary_image).decode('utf-8')\n      desired_width = 1000  # Width for medium display\n      desired_height = 650  # Height for medium display\n      html_template = f'''\n      <html>\n      <head>\n          <title>Generated Chart</title>\n      </head>\n      <body>\n          <h1>Generated Chart</h1>\n          <img src=\"data:image/png;base64,{base64_image}\" alt=\"Generated Chart\" style=\"width:{desired_width}px; height:{desired_height}px;\">\n      </body>\n      </html>\n      '''\n      workflowContext.outHTML(9, title=\"Generated chart\", text = html_template)\n    return",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [
    {
      "source": "1",
      "target": "2",
      "id": 1
    }
  ],
  "dataSetDetails": [],
  "engine": "pyspark"
}