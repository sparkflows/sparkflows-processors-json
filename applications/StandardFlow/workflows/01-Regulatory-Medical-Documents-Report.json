{
  "name": "01-Regulatory-Medical-Documents-Report",
  "uuid": "41c7d89c-30ec-4e0c-8f77-00b4ea167efd",
  "category": "Doc-Gen",
  "description": "-",
  "parameters": " --var textField=StandardFlow-Repo --var radio=eu --var destinationPath=/home/sparkflows/fire-data/data/GENAI/StandardFlow/StandardFlow-Repo/uploads/EU_Vaccine.pdf --var radio1=EU --var destinationPath1=/data/GENAI/StandardFlow/StandardFlow-Repo/uploads/ --var refresh=true --var outputPath=/data/GENAI/StandardFlow/StandardFlow-Repo/output/ --var refresh1=true",
  "nodes": [
    {
      "id": "1",
      "name": "PySpark",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "<h2>Pyspark Details</h2>\n<br>\nThis node receives receives an input pyspark dataframe in function called myfn.<br>\n<br>\nThe pyspark/python code processes it and returns one computed pyspark dataframe.<br>",
      "examples": "<h2>Pyspark Examples</h2>\n<br>\nInput Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> Add the house_type column</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\nhouse_type_udf = udf(lambda bedrooms: \"big house\" if int(bedrooms) >2 else \"small house\", StringType())<br>\nfiletr_df = inDF.select(\"id\", \"price\", \"lotsize\", \"bedrooms\")<br>\noutDF = filetr_df.withColumn(\"house_type\", house_type_udf(filetr_df.bedrooms))<br>\nreturn outDF<br>\n<br>\n<h4> Using pandas dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\n<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n# Convert the Spark DataFrame to a Pandas DataFrame<br>\npdf = inDF.select(\"*\").toPandas()<br>\n<br>\n# Display the result on the Executions page<br>\nworkflowContext.outStr(id, \"Outputting Pandas Dataframe\")<br>\n<br>\n# Display the dataframe on the Executions page<br>\nworkflowContext.outPandasDataframe(id, \"Pandas DataFrame\", pdf, 10)<br>\n<br>\n# Create a Spark DataFrame from a Pandas DataFrame<br>\ndf = spark.createDataFrame(pdf)<br>\n<br>\nreturn df<br>\n<br>\n<h4> Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nimport numpy as np<br>\nimport pandas as pd<br>\n<br>\nfrom fire.workflowcontext import *<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n<br>\n# Create the numpy 2d array<br>\nexample_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])<br>\n<br>\n# Convert to Pandas Dataframe<br>\npandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])<br>\n<br>\n# Convert Pandas Dataframe to Spark Dataframe<br>\nspark_dataframe = spark.createDataFrame(pandas_dataframe)<br>\nreturn spark_dataframe<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "219.778px",
      "y": "131.778px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \nimport pymupdf  # PyMuPDF\nfrom PIL import Image\nfrom xhtml2pdf import pisa\nimport io\nimport base64\nimport requests\nfrom io import BytesIO\nimport os\ndef html_to_pdf(html_string, output_filename):\n    with BytesIO() as pdf_buffer:\n        pisa.CreatePDF(BytesIO(html_string.encode('utf-8')), dest=pdf_buffer)\n        pdf_buffer.seek(0)\n        with open(output_filename, 'wb') as output_file:\n            output_file.write(pdf_buffer.read())\ndef convert_all_pages_to_base64(pdf_path):\n    pdf_document = pymupdf.open(pdf_path)\n    base64_images = []\n\n    for page_num in range(len(pdf_document)):  # Iterate through all pages\n        page = pdf_document.load_page(page_num)\n        pix = page.get_pixmap(dpi=300)  # You can adjust DPI based on quality\n        img_bytes = pix.tobytes()\n\n        # Convert bytes to PIL Image\n        img = Image.open(io.BytesIO(img_bytes))\n\n        # Save the image to bytes in PNG format\n        buffered = io.BytesIO()\n        img.save(buffered, format=\"PNG\")\n        img_bytes_png = buffered.getvalue()\n\n        # Encode bytes to base64\n        img_base64 = base64.b64encode(img_bytes_png).decode('utf-8')\n\n        # Append base64 image string to the list\n        base64_images.append(img_base64)\n        return base64_images\n\ndef extract_text_from_pdf(pdf_path):\n    # Open the PDF file\n    pdf_document = pymupdf.open(pdf_path)\n    text = \"\"\n    for page_num in range(len(pdf_document)):\n\n      # Iterate over each page\n\n      page = pdf_document.load_page(0)\n      text = text + \" \" + page.get_text()\n\n    return text\n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n  print(\"========================1==============================\")\n  document_path = '${destinationPath}'\n  if '${radio1}' == 'Australia':\n    standard_path = 'data/GENAI/StandardFlow/'+'${textField}'+'/'+'${radio1}'+'-Doc/Australia_Vaccine.pdf'\n  else:\n    standard_path = 'data/GENAI/StandardFlow/'+'${textField}'+'/'+'${radio1}'+'-Doc/EU_Vaccine.pdf'    \n  output_dir_path = \"/\".join(document_path.split(\"/\")[:-2]) + \"/output\"\n  print(\"============================================\")\n  print(output_dir_path)\n  if not os.path.exists(output_dir_path):\n    os.mkdir(output_dir_path)\n  print(document_path,standard_path,output_dir_path)\n  print(\"==============================2==============================\")\n  image = convert_all_pages_to_base64(standard_path)\n  text = extract_text_from_pdf(standard_path)\n  print(\"===============================3=============================\")\n  GOOGLE_API_KEY = \"AIzaSyAnVTY3xwxSyeyB5syiTxmz2RDvP9h7fEU\"\n  import json\n  image_inline = []\n  for i in image:\n    image_inline.append({\n            \"inlineData\": {\n              \"mimeType\": \"image/png\",\n              \"data\": i}})\n    \n  payload = {\n    \"contents\": [\n      {\n        \"parts\": image_inline + [\n          {\"text\": f\"\"\"In the image I am giving you a Australlia Vaccine Form. There you will et various fields. You have to fill those fields by taking the help from the below USA PDF context\n          \n     Here is the pdf text:\n     {text}\n\n    - USA FDA is similar to Australlia TGA\n    - You have to generate the answer based on the context and your skills\n    - You have to generate a excat HTML format based on the image I provided. Just in place of underline you will provide the answer.\n \"\"\"}\n        ]\n      }\n    ]\n  }\n  headers = {\n    'Content-Type': 'application/json'\n  }\n # model1 = \"gemini-1.5-pro-exp-0801\"\n  model1 = \"gemini-1.5-flash\"\n\n  response = requests.post(f\"https://generativelanguage.googleapis.com/v1beta/models/{model1}:generateContent?key={GOOGLE_API_KEY}\", headers=headers, data=json.dumps(payload))\n  print(\"===========4==========\")\n  print(response)\n  print(\"===========5============\")\n  output_html = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n  print(output_html)\n  if \"```\" in output_html:\n    output_html = output_html.replace(\"```\",\"\")[4:]\n  workflowContext.outHTML(9, title=\"Generated chart\", text = output_html)\n  html_to_pdf(output_html, output_dir_path +'/result.pdf')\n  return",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [],
  "dataSetDetails": [],
  "engine": "pyspark"
}