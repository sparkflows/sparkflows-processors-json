{
  "name": "01-Video-Analysis",
  "uuid": "9873e303-108d-4d12-bf3f-1695505cc113",
  "category": "-",
  "parameters": " --var destinationPath=/home/sparkflows/fire-data/data/GENAI/Video-Summarize-And-QnA/Uploads/A_Day_in_the_Life_of_an_Airport_Cleaner.mp4 --var option=1 --var deleteVideo=false --var question='Who is Stephen McCoy?' --var submit2=true",
  "nodes": [
    {
      "id": "1",
      "name": "File Uploaded",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "<h2>Pyspark Details</h2>\n<br>\nThis node receives receives an input pyspark dataframe in function called myfn.<br>\n<br>\nThe pyspark/python code processes it and returns one computed pyspark dataframe.<br>",
      "examples": "<h2>Pyspark Examples</h2>\n<br>\nInput Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> Add the house_type column</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\nhouse_type_udf = udf(lambda bedrooms: \"big house\" if int(bedrooms) >2 else \"small house\", StringType())<br>\nfiletr_df = inDF.select(\"id\", \"price\", \"lotsize\", \"bedrooms\")<br>\noutDF = filetr_df.withColumn(\"house_type\", house_type_udf(filetr_df.bedrooms))<br>\nreturn outDF<br>\n<br>\n<h4> Using pandas dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\n<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n# Convert the Spark DataFrame to a Pandas DataFrame<br>\npdf = inDF.select(\"*\").toPandas()<br>\n<br>\n# Display the result on the Executions page<br>\nworkflowContext.outStr(id, \"Outputting Pandas Dataframe\")<br>\n<br>\n# Display the dataframe on the Executions page<br>\nworkflowContext.outPandasDataframe(id, \"Pandas DataFrame\", pdf, 10)<br>\n<br>\n# Create a Spark DataFrame from a Pandas DataFrame<br>\ndf = spark.createDataFrame(pdf)<br>\n<br>\nreturn df<br>\n<br>\n<h4> Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nimport numpy as np<br>\nimport pandas as pd<br>\n<br>\nfrom fire.workflowcontext import *<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n<br>\n# Create the numpy 2d array<br>\nexample_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])<br>\n<br>\n# Convert to Pandas Dataframe<br>\npandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])<br>\n<br>\n# Convert Pandas Dataframe to Spark Dataframe<br>\nspark_dataframe = spark.createDataFrame(pandas_dataframe)<br>\nreturn spark_dataframe<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "197.8px",
      "y": "116.8px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom fire.workflowcontext import WorkflowContext \nimport os\nimport time\nimport requests\nimport mimetypes\nimport json\nimport pickle\n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n\n    # Constants\n    GOOGLE_API_KEY = \"$Gemini_Api\"\n    BASE_URL = \"https://generativelanguage.googleapis.com/upload/v1beta\"\n    #VIDEO_PATH = \"data/GENAI/Video-Summarize-And-QnA/Raw-Data/A_Day_in_the_Life_of_an_Airport_Cleaner.mp4\"\n    VIDEO_PATH = '${destinationPath}'\n    video_file_name = VIDEO_PATH.split(\"/\")[-1]\n    dic_path = \"/\".join(VIDEO_PATH.split(\"/\")[:-2]) + \"/Dictionary\"\n    print(VIDEO_PATH)\n    print(dic_path)\n    # Check if the path exists\n    dic = {}\n    if not os.path.exists(dic_path):\n        # Create the directories\n        os.makedirs(dic_path)\n        print(\"================================\")\n        print(f\"Path '{dic_path}' created.\")\n        print(\"================================\")\n          \n    else:\n        print(f\"Path '{dic_path}' already exists.\")\n        if os.path.exists(dic_path+\"/dic.pkl\"):\n          with open(dic_path+\"/dic.pkl\", \"rb\") as f:\n            dic = pickle.load(f)\n          print(\"================================\")\n          print(dic)\n          print(video_file_name)\n          print(\"================================\")\n          if video_file_name in dic.keys():\n             data = [(dic[video_file_name],)]\n\n          # Define the schema (column names)\n          columns = [\"file_id\"]\n\n          # Create a DataFrame\n          df = spark.createDataFrame(data, columns)\n          return df\n          \n    # Get MIME type and file size\n    \n    MIME_TYPE = mimetypes.guess_type(VIDEO_PATH)[0]\n    NUM_BYTES = os.path.getsize(VIDEO_PATH)\n    DISPLAY_NAME = os.path.basename(VIDEO_PATH)\n\n    # Step 1: Initialize resumable upload\n    metadata = {\n        \"file\": {\n            \"display_name\": DISPLAY_NAME\n        }\n    }\n\n    headers = {\n        \"X-Goog-Upload-Protocol\": \"resumable\",\n        \"X-Goog-Upload-Command\": \"start\",\n        \"X-Goog-Upload-Header-Content-Length\": str(NUM_BYTES),\n        \"X-Goog-Upload-Header-Content-Type\": MIME_TYPE,\n        \"Content-Type\": \"application/json\"\n    }\n\n    response = requests.post(\n        f\"{BASE_URL}/files?key={GOOGLE_API_KEY}\",\n        headers=headers,\n        json=metadata\n    )\n\n    if response.status_code != 200:\n        raise Exception(f\"Error initializing upload: {response.text}\")\n\n    upload_url = response.headers.get(\"X-Goog-Upload-URL\")\n    \n    # Step 2: Upload the video file\n    with open(VIDEO_PATH, \"rb\") as file:\n        headers = {\n            \"Content-Length\": str(NUM_BYTES),\n            \"X-Goog-Upload-Offset\": \"0\",\n            \"X-Goog-Upload-Command\": \"upload, finalize\"\n        }\n        response = requests.post(upload_url, headers=headers, data=file)\n\n    if response.status_code != 200:\n        raise Exception(f\"Error uploading file: {response.text}\")\n\n    file_info = response.json()\n    file_uri = file_info[\"file\"][\"uri\"]\n    state = file_info[\"file\"][\"state\"]\n    print(\"================\",file_uri)\n    \n    while \"PROCESSING\" in state:\n      workflowContext.outStr(id, \"Processing Video...\",\"Status\")\n      time.sleep(5)\n      response = requests.get(f\"https://generativelanguage.googleapis.com/v1beta/{file_info['file']['name']}?key={GOOGLE_API_KEY}\")\n      updated_file_info = response.json()\n      state = updated_file_info[\"state\"]\n    if state == \"ACTIVE\":\n      workflowContext.outStr(id, \"Video Uploaded Successfully\",\"Status\")\n      schema = StructType([StructField(\"file_uri\", StringType(), True)])\n      \n      data = [(file_uri.split(\"/\")[-1],)]\n\n      # Define the schema (column names)\n      columns = [\"file_id\"]\n      dic[video_file_name] = file_uri.split(\"/\")[-1]\n      print(\"=========================\")\n      print(dic)\n      print(\"==========================\")\n      with open(dic_path+\"/dic.pkl\", \"wb\") as file:\n        pickle.dump(dic, file)\n\n      # Create a DataFrame\n      df = spark.createDataFrame(data, columns)\n\n      return df\n    else:\n      data = [(\"Issue in file upload\",)]\n\n      # Define the schema (column names)\n      columns = [\"file_id\"]\n\n      # Create a DataFrame\n      df = spark.createDataFrame(data, columns)\n\n      return df",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "outputColNames",
          "value": "[\"file_id\"]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "outputColFormats",
          "value": "[\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        }
      ],
      "engine": "pyspark"
    },
    {
      "id": "2",
      "name": "Summarize and QnA",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "<h2>Pyspark Details</h2>\n<br>\nThis node receives receives an input pyspark dataframe in function called myfn.<br>\n<br>\nThe pyspark/python code processes it and returns one computed pyspark dataframe.<br>",
      "examples": "<h2>Pyspark Examples</h2>\n<br>\nInput Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> Add the house_type column</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\nhouse_type_udf = udf(lambda bedrooms: \"big house\" if int(bedrooms) >2 else \"small house\", StringType())<br>\nfiletr_df = inDF.select(\"id\", \"price\", \"lotsize\", \"bedrooms\")<br>\noutDF = filetr_df.withColumn(\"house_type\", house_type_udf(filetr_df.bedrooms))<br>\nreturn outDF<br>\n<br>\n<h4> Using pandas dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\n<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n# Convert the Spark DataFrame to a Pandas DataFrame<br>\npdf = inDF.select(\"*\").toPandas()<br>\n<br>\n# Display the result on the Executions page<br>\nworkflowContext.outStr(id, \"Outputting Pandas Dataframe\")<br>\n<br>\n# Display the dataframe on the Executions page<br>\nworkflowContext.outPandasDataframe(id, \"Pandas DataFrame\", pdf, 10)<br>\n<br>\n# Create a Spark DataFrame from a Pandas DataFrame<br>\ndf = spark.createDataFrame(pdf)<br>\n<br>\nreturn df<br>\n<br>\n<h4> Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nimport numpy as np<br>\nimport pandas as pd<br>\n<br>\nfrom fire.workflowcontext import *<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n<br>\n# Create the numpy 2d array<br>\nexample_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])<br>\n<br>\n# Convert to Pandas Dataframe<br>\npandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])<br>\n<br>\n# Convert Pandas Dataframe to Spark Dataframe<br>\nspark_dataframe = spark.createDataFrame(pandas_dataframe)<br>\nreturn spark_dataframe<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "317.8px",
      "y": "108.8px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \nimport requests\nGOOGLE_API_KEY = \"$Gemini_Api\"\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n    rows = inDF.collect()\n\n    # Get the first value from the first row\n    value = rows[0][\"file_id\"]\n    option = '${option}'\n    if value != \"Issue in file upload\":\n      file_url = f\"https://generativelanguage.googleapis.com/v1beta/files/{value}\"\n      print(file_url)\n      if option == '0':\n        print(\"================== Summarization Started =====================\")\n        payload = {\"contents\": [{\"parts\": [\n                      {\"text\": \"Summarize the video clip. Return the response in HTML format. Try to summarize the video by 500 words. Only return the HTML output , no other text\"},\n                      {\"file_data\": {\"mime_type\": \"video/mp4\", \"file_uri\": file_url}}\n                  ]\n              }]\n         }\n      else:\n        print(\"===================== QNA Started ===============================\")\n        question = \"${question}\"\n        payload = {\"contents\": [{\"parts\": [\n          {\"text\": f\"\"\"Give me the answer of the following question from the video. Return the response in HTML format. Only return the HTML output , no other text.\n                      question: {question}\"\"\"},\n          {\"file_data\": {\"mime_type\": \"video/mp4\", \"file_uri\": file_url}}\n                 ]\n             }]\n         }\n\n      headers = {\n        \"Content-Type\": \"application/json\"\n      }\n      response = requests.post(\n        f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GOOGLE_API_KEY}\",\n        headers=headers,\n        json=payload\n      )\n      print(f\"============ response.status_code : {response.status_code}\")\n      if response.status_code != 200:\n        print(\"===================== 1\")\n        workflowContext.outStr(id,\"Something Went Wrong\", \"Error Status\")\n      elif response.status_code == 200:\n        response_data = response.json()\n        print(response_data)\n        for candidate in response_data.get(\"candidates\", []):\n          for part in candidate.get(\"content\", {}).get(\"parts\", []):\n            if \"```\" in part.get(\"text\"):\n                result = part.get(\"text\").replace(\"```\",\"\")[4:]\n                workflowContext.outHTML(id,text=result, title=\"Summary Result\")\n            else:\n                workflowContext.outHTML(id,text=part.get(\"text\"), title=\"Summary Result\")\n               \n    else:\n      workflowContext.outStr(id,\"Something Went Wrong\", \"Error Status\")",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [
    {
      "source": "1",
      "target": "2",
      "id": 1
    }
  ],
  "dataSetDetails": [],
  "engine": "pyspark"
}