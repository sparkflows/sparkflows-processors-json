{
  "name": "01-Extract-Good-Items",
  "uuid": "aba906c4-495c-4606-aa3f-d9308e4ef9a3",
  "category": "test",
  "parameters": " --var getStarted=true --var database=item_validation --var table=items_master --var itemsFilter='101','102','103','104','105','106','107','108','109' --var retrieve=true --var destinationPath=/home/sparkflows/fire-data/data/CPG/Business-File-Loading-And-Validating/Upload/ --var itemsFilter='101','102','103','104','105','106','107','108','109','110' --var table=items_good --var table1=items_good --var table2=items_bad --var goBack=false",
  "nodes": [
    {
      "id": "1",
      "name": "Dataset Structured",
      "description": "This Node creates a DataFrame by reading data from HDFS, HIVE etc. The dataset was defined earlier in Fire by using the Dataset Feature. As a user, you just have to select the Dataset of your interest.",
      "details": "This Node creates a DataFrame by reading data from HDFS, HIVE etc.<br>\n<br>\nThe data has been defined earlier in Fire by using the Dataset Feature. As a user, you just have to select the Dataset of your interest.<br>",
      "examples": "<h2> Dataset Structured Node Example</h2>\n<br>\nScenario:<br>\n<br>\nLet's say you have multiple datasets available in your workflow and you want to select one of them as input for the next node. You can use the Dataset Structured node to choose the desired dataset.<br>\n<br>\nConfiguration:<br>\n<br>\n1. **Output Storage Level:** Select the desired storage level for the output DataFrame.<br>\n2. **Dataset:** Choose the dataset from the dropdown list.<br>\n<br>\nOutput:<br>\n<br>\nThe node will output the selected dataset as a DataFrame.<br>",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetStructured",
      "x": "76px",
      "y": "231px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "dataset",
          "value": "bf00e0bf-0646-4c37-bad7-8a9df02f8cdd",
          "widget": "dataset",
          "title": "Dataset",
          "description": "Selected Dataset",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "2",
      "name": "Sticky Note",
      "description": "Allows capturing Notes on the Workflow",
      "details": "",
      "examples": "",
      "type": "sticky",
      "nodeClass": "fire.nodes.doc.NodeStickyNote",
      "x": "52px",
      "y": "53px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "bgColor",
          "value": "gray",
          "widget": "textfield",
          "title": "Bg Color",
          "description": "Background of note",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "width",
          "value": "300px",
          "widget": "textfield",
          "title": "Width",
          "description": "Width of note",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "height",
          "value": "110px",
          "widget": "textfield",
          "title": "Height",
          "description": "Height of note",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "comment",
          "value": "<p>This workflow uses left join to extract the matching/good records that are available in the master table.</p>",
          "widget": "textarea_rich",
          "title": "Comment",
          "description": "Comments for the Workflow",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "6",
      "name": "Join On Columns",
      "description": "Joins the incoming Dataframes on the given columns",
      "details": "<h2>Join On Columns Node Details</h2>\n<br>\n<ul>\n<li> This node joins the incoming dataframes based on a specified column between the two dataframes.</li>\n<li> The new Dataframe will contain all the columns from both Dataframe.</li>\n</ul>\nJoining modes supported by this node are as follows:<br>\n<ul>\n<li> inner : The joined table will have records that have matching values in both tables.</li>\n<li> outer : The joined table contains either all the records from both the tables or fills in NULL values for missing matches on either side.</li>\n<li> left_outer  : Even if there are no matches in the right table it returns all the rows from the left table.</li>\n<li> right_outer : Even if there are no matches in the left table it returns all the rows from the right table.</li>\n<li> leftsemi : This gives only those rows in the left table that have a matching row in the right table.</li>\n</ul>",
      "examples": "<h2>Join On Columns Example</h2>\n<br>\n<h4> Incoming Dataframes</h4>\n<br>\n1st Incoming Dataframe table1 has the following rows:<br>\n<br>\nEMP_CD    |    EMP_NAME    |    DEPT_NO       <br>\n-------------------------------------------<br>\nE01       |    DAVID       |    10         <br>\nE02       |    JOHN        |    20      <br>\nE03       |    MARTIN      |    30  <br>\nE04       |    TONY        |    40  <br>\n<br>\n2nd Incoming Dataframe table2 has the following rows:<br>\n<br>\nDEPT_ID    |      DEPT_NAME   |    LOC       <br>\n-------------------------------------------<br>\n10         |      HR          |    IND  <br>\n20         |      SALES       |    AUS  <br>\n30         |      MARKETING   |    UK         <br>\n50         |      RESEARCH    |    NZ      <br>\n<br>\n<h4> Selected columns for joining are following </h4>\n<ul>\n<li> From table1 is DEPT_NO</li>\n<li> From table2 is DEPT_ID</li>\n</ul>\n<h4> When the Joining condition is `inner` we have</h4>\n<h4> Final Output</h4>\n<br>\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_ID    |      DEPT_NAME  |    LOC           <br>\n--------------------------------------------------------------------------------------<br>\nE01       |    DAVID       |    10        |     10       |      HR         |    IND<br>\nE02       |    JOHN        |    20        |     20       |      SALES      |    AUS<br>\nE03       |    MARTIN      |    30        |     30       |      MARKETING  |    UK<br>\n<br>\n<h4> When the Joining condition `outer` we have</h4>\n<h4> Final Output</h4>\n<br>\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_ID    |      DEPT_NAME  |    LOC           <br>\n--------------------------------------------------------------------------------------<br>\nE01       |    DAVID       |    10        |     10       |      HR         |    IND<br>\nE02       |    JOHN        |    20        |     20       |      SALES      |    AUS<br>\nE03       |    MARTIN      |    30        |     30       |      MARKETING  |    UK<br>\nE04       |    TONY        |    40        |              |                 |    <br>\n          |                |              |     50       |      RESEARCH   |    NZ<br>\n<br>\n<h4> When the Joining condition is `left_outer` we have</h4>\n<h4> Final Output</h4>\n<br>\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_ID    |      DEPT_NAME  |    LOC           <br>\n--------------------------------------------------------------------------------------<br>\nE01       |    DAVID       |    10        |     10       |      HR         |    IND<br>\nE02       |    JOHN        |    20        |     20       |      SALES      |    AUS<br>\nE03       |    MARTIN      |    30        |     30       |      MARKETING  |    UK<br>\nE04       |    TONY        |    40        |              |                 |    <br>\n<br>\n<h4> When the Joining condition `right_outer` we have</h4>\n<h4> Final Output</h4>\n          <br>\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_ID    |      DEPT_NAME  |    LOC           <br>\n--------------------------------------------------------------------------------------<br>\nE01       |    DAVID       |    10        |     10       |      HR         |    IND<br>\nE02       |    JOHN        |    20        |     20       |      SALES      |    AUS<br>\nE03       |    MARTIN      |    30        |     30       |      MARKETING  |    UK<br>\n          |                |              |     50       |      RESEARCH   |    NZ<br>",
      "type": "join2inputs",
      "nodeClass": "fire.nodes.etl.JoinOnColumns",
      "x": "291px",
      "y": "379px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "joinType",
          "value": "inner",
          "widget": "array",
          "title": "Join Type",
          "description": "Type of Join",
          "optionsArray": [
            "inner",
            "outer",
            "left_outer",
            "right_outer",
            "leftsemi"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "leftTableJoinColumn",
          "value": "[\"item_id\"]",
          "widget": "key_array_join",
          "title": "LeftTableJoinColumn",
          "description": "",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "rightTableJoinColumn",
          "value": "[\"item_id\"]",
          "widget": "value_array_join",
          "title": "RightTableJoinColumn",
          "description": "",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "scala"
    },
    {
      "id": "8",
      "name": "Drop Columns",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node creates a new DataFrame by dropping the specified columns",
      "details": "<h2>Drop Columns Node Details</h2>\n<br>\nThis node creates a new DataFrame by dropping the selected columns.<br>\n<br>\nIt drops the selected columns from the outgoing dataframe. <br>\n<br>\nColumns that need to be dropped are to be selected in the 'Selected' list. Multiple columns can be selected in the list that needs to be dropped.<br>",
      "examples": "<h2>Drop Columns Node Examples</h2>\n<br>\n<h4>Incoming Dataframe</h4>\n<br>\nIn this example we have considered an Incoming Dataframe having following columns:<br>\n<br>\n<ul>\n<li> CUST_CD</li>\n<li> CUST_NAME</li>\n<li> DOB</li>\n<li> ADDRESS</li>\n</ul>\n<h4>Drop Columns Node Configuration And Output</h4>\n<br>\n[DOB] and [ADDRESS] columns are selected to be dropped from the outgoing Dataframe. <br>\nOutgoing dataframe would contain only below two columns after dropping the selected columns:<br>\n<br>\n<ul>\n<li> DOB</li>\n<li> ADDRESS</li>\n</ul>",
      "type": "transform",
      "nodeClass": "fire.nodes.etl.NodeDropColumns",
      "x": "495.187px",
      "y": "394.188px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "dropCols",
          "value": "[\"left_item_id\",\"item_description\"]",
          "widget": "variables",
          "title": "Columns",
          "description": "The columns to be excluded from the output DataFrame",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "9",
      "name": "Columns Rename",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node creates a new DataFrame by renaming existing columns with the new name",
      "details": "This node creates a new DataFrame by renaming existing columns with the new name.<br>\n<br>\nFor the columns to be renamed, provide the new column name. The output dataframe would have the specified columns renamed to the new ones.<br>",
      "examples": "If incoming Dataframe has a column as [CUST NAME] that needs to be renamed to [CUST_NAME] then below details need to be provided in the node:<br>\n<br>\n<ul>\n<li> CURRENT COLUMN NAMES : CUST NAME</li>\n<li> COLUMNS NEW NAME : CUST_NAME</li>\n</ul>\nThis would result in output Dataframe having renameed column [CUST_NAME]. A seprate row needs to be added for each Column Rename requirement.<br>",
      "type": "transform",
      "nodeClass": "fire.nodes.etl.NodeColumnsRename",
      "x": "679.187px",
      "y": "405.194px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "currentColNames",
          "value": "[\"right_item_id\"]",
          "widget": "variables_list_select",
          "title": "Current Column Names",
          "description": "Current Column Names",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "newColNames",
          "value": "[\"item_id\"]",
          "widget": "variables_list_textfield",
          "title": "Columns New Name",
          "description": "New name for existing columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "bulkColumnRename",
          "value": "",
          "widget": "tab",
          "title": "BulkColumnRename",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "bulkType",
          "value": "",
          "widget": "array_single",
          "title": "Type",
          "description": "Select Type",
          "optionsArray": [
            "add_prefix",
            "add_suffix",
            "remove_extra_charcter"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "prefix",
          "value": "",
          "widget": "textfield",
          "wdgtCommon": "bulkType",
          "title": "Prefix",
          "description": "Prefix Value",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": "add_prefix"
        },
        {
          "name": "suffix",
          "value": "",
          "widget": "textfield",
          "wdgtCommon": "bulkType",
          "title": "Suffix",
          "description": "Suffix Value",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": "add_suffix"
        },
        {
          "name": "remove_extra_charcter",
          "value": "",
          "widget": "textfield",
          "wdgtCommon": "bulkType",
          "title": "Extra Charcter",
          "description": "extra_charcter",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": "remove_extra_charcter"
        },
        {
          "name": "inputCols",
          "value": "[]",
          "widget": "variables",
          "title": "Current Column Names",
          "description": "Current Column Names",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "12",
      "name": "Read CSV",
      "iconImage": "/images/icons/node-icon/csv.svg",
      "description": "It reads in CSV files and creates a DataFrame from it.",
      "details": "<h2>Read CSV Details</h2>\n<br>\nThis node reads CSV files and creates a DataFrame from them. It can read either a single file or a directory containing multiple files. The user can configure the below fields to parse the file.<br>\n<br>\nThe user can choose the <b>Output storage level</b> from the drop down. The options in the dropdown can be one of the following:<br>\n<ul>\n<li> <b>MEMORY_ONLY</b>          Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they are needed. This is the default level.</li>\n<li> <b>MEMORY_AND_DISK</b>       Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that do not fit on disk, and read them from there when they are needed.</li>\n<li> <b>MEMORY_ONLY_SER</b>        Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.</li>\n<li> <b>MEMORY_AND_DISK_SER</b>    Similar to MEMORY_ONLY_SER, but spill partitions that do not fit in memory to disk instead of recomputing them on the fly each time they're needed.</li>\n<li> <b>DISK_ONLY</b>              Store the RDD partitions only on disk.</li>\n<li> <b>MEMORY_ONLY_2, MEMORY_AND_DISK_2 others </b> . Same as the levels above, but replicate each partition on two cluster nodes.</li>\n<li> <b>OFF_HEAP</b>               Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled.</li>\n</ul>\nThe user needs to provide a data file <b>Path</b> to read the data from. This is a required field.<br>\n<br>\nThe user can choose the <b>Separator</b> used in the data file to parse it. The default separator is <b>( , )</b> comma.<br>\n<br>\nIn the <b>Header</b> field, one can choose:<br>\n<ul>\n<li> <b>true</b> if the data file has a header.</li>\n<li> <b>false</b> Otherwise.</li>\n</ul>\nIn the <b>Drop special character in column name</b> field, one can choose:<br>\n<ul>\n<li> <b>true</b> If you want to remove the special characters from column names.</li>\n<li> <b>false</b> Otherwise.</li>\n</ul>\nIn the <b>Mode</b> field, one can choose from the below options in the dropdown:<br>\n<ul>\n<li> <b>PERMISSIVE</b> When the parser meets a corrupt field in a record, it sets the value of the field to NULL and continues to the next record.</li>\n<li> <b>DROPMALFORMED</b> ignores the whole corrupted record.</li>\n<li> <b>FAILFAST</b> throws an exception when it meets corrupted records.</li>\n</ul>\nIn the <b>Enforce Schema</b> field, one can choose:<br>\n<ul>\n<li> <b>true</b> The specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored.</li>\n<li> <b>false</b> The schema will be validated against all headers in CSV files when the header option is set to <b>false</b>.</li>\n</ul>\nIn the <b>Whether to add input file as a column in DataFrame</b> field, one can choose:<br>\n<ul>\n<li> <b>true</b> There will be a new column added to the DataFrame at the end, which can be seen in the schema columns. One can enter the name of this column.</li>\n<li> <b>false</b> This functionality is disabled, and the DataFrame consists of only the columns read from the data file.</li>\n</ul>\nIn the <b>ENCODING</b> field, one can specify the encoding type to be used for reading the files. By default, it is set as <b>UTF-8</b>.<br>\n<br>\nThe <b>QUOTE</b> field sets a single character used for escaping quoted values where the separator can be part of the value. The default value for this is <b>( \" )</b>, a double quote.<br>\n<br>\nThe <b>ESCAPE</b> field sets a single character used for escaping quotes inside an already quoted value. The default value for this is <b>( \\ )</b>, a backslash.\t<br>\n<br>\nAfter the above options are set, one can click on <b>InferSchema</b> to see the final columns.<br>\nUsers can still add or delete columns using <b>+</b> button next to the InferSchema and <b>-</b> button next to the column names.<br>",
      "examples": "<h2> Read CSV Node Example</h2>\n<br>\nGiven a CSV file with the following data:<br>\n<br>\nSupplierID,SupplierName,Region,YearsInBusiness,LeadTime,PriceIndex,OrderFulfillmentTime,OverallCost,OrderCancellations,CustomerRating<br>\nS1,Supplier A,Region 1,10,5,1.2,3,100,2,4<br>\nS2,Supplier B,Region 2,15,7,1.1,4,120,1,5<br>\nIf you configure the Read CSV node as follows:<br>\n<br>\nPath: /path/to/your/file.csv<br>\nSeparator: ,<br>\nHeader: true<br>\nThe output would be a DataFrame with the following schema:<br>\n<br>\nColumn Name\tData Type<br>\nSupplierID\tString<br>\nSupplierName\tString<br>\nRegion\tString<br>\nYearsInBusiness\tInteger<br>\nLeadTime\tInteger<br>\nPriceIndex\tDouble<br>\nOrderFulfillmentTime\tInteger<br>\nOverallCost\tInteger<br>\nOrderCancellations\tInteger<br>\nCustomerRating\tInteger<br>",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetCSV",
      "x": "70.3812px",
      "y": "429.394px",
      "hint": "Infer the schema when a new file is selected or the file content has changed",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "path",
          "value": "${destinationPath}",
          "widget": "textfield",
          "title": "Path",
          "description": "Path of the file/directory",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "separator",
          "value": ",",
          "widget": "textfield",
          "title": "Separator",
          "description": "CSV Separator",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "header",
          "value": "true",
          "widget": "array",
          "title": "Header",
          "description": "Whether the file has a header row",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "dropSpecialCharacterInColumnName",
          "value": "false",
          "widget": "array",
          "title": "Drop Special Character In ColumnName",
          "description": "Whether to drop the Special Characters and Spaces in Column Name.",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "mode",
          "value": "PERMISSIVE",
          "widget": "array",
          "title": "Mode",
          "description": "Mode for dealing with corrupt records during parsing.",
          "optionsArray": [
            "PERMISSIVE",
            "DROPMALFORMED",
            "FAILFAST"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "enforceSchema",
          "value": "false",
          "widget": "array",
          "title": "Enforce Schema",
          "description": "If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to false, the schema will be validated against all headers in CSV files in the case when the header option is set to true.",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "addInputFileName",
          "value": "false",
          "widget": "array",
          "title": "Whether to Add Input File Name as Column in the Dataframe",
          "description": "Add the new field:input_file_name",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "encoding",
          "value": "UTF-8",
          "widget": "textfield",
          "title": "Encoding",
          "description": "Decodes the CSV files by the given encoding type",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "quote",
          "value": "\"",
          "widget": "textfield",
          "title": "Quote",
          "description": "Sets a single character used for escaping quoted values where the separator can be part of the value",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "escape",
          "value": "\\",
          "widget": "textfield",
          "title": "Escape",
          "description": "Sets a single character used for escaping quotes inside an already quoted value.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[\"item_id\",\"num_items\",\"price\",\"description\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Columns from CSV",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[\"INTEGER\",\"INTEGER\",\"DOUBLE\",\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "properties",
          "value": "",
          "widget": "tab",
          "title": "Properties",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "extraOptionsKeys",
          "value": "[]",
          "widget": "key_array",
          "title": "Properties Name",
          "description": "Extra options/properites available while executing in Read CSV.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "extraOptionsValues",
          "value": "[]",
          "widget": "value_array",
          "title": "Properties Value",
          "description": "Config Values for the Corresponding properites name",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "13",
      "name": "Print N Rows",
      "iconImage": "/images/icons/node-icon/print_n_rows.svg",
      "description": "Prints the specified number of records in the DataFrame. It is useful for seeing intermediate output",
      "details": "<h2>Print N Rows Node Details</h2>\n<br>\nThis node is used to print the first N rows from the incoming dataframe.<br>\n<br>\nThe Number of rows that needs to be printed can be configured in the node.<br>\n<br>\n<h4>Input Parameters</h4>\n<ul>\n<li> OUTPUT STORAGE LEVEL : Keep this as DEFAULT.</li>\n<li> TITLE : Enter a short description for the type of information being displayed.</li>\n<li> NUM ROWS TO PRINT : Set an integer value(N) which controls the number of rows to be displayed(Default N=10).</li>\n<li> DISPLAY DATA TYPE : Shows the output dataframe column datatypes by default.</li>\n</ul>\n<h4>Output</h4>\n<ul>\n<li> This node can be used to view, analyze and validate the output of the Dataframe.</li>\n</ul>",
      "examples": "when input 5 in no of rows,it will show first 5 rows of the table as follows<br>\n<br>\nPartID\tSupplierID\tPartName\t<br>\n<br>\nP9271\t  S798\t    Part_D\t<br>\nP523\t  S955\t    Part_K\t<br>\nP3201\t  S332\t    Part_M\t<br>\nP9634\t  S527\t    Part_G\t<br>\nP9345\t  S850\t    Part_M<br>",
      "type": "transform",
      "nodeClass": "fire.nodes.util.NodePrintFirstNRows",
      "x": "862.387px",
      "y": "447.388px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "title",
          "value": "Good Items",
          "widget": "textfield",
          "title": "Title",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "n",
          "value": "1000",
          "widget": "textfield",
          "title": "Num Rows to Print",
          "description": "number of rows to be printed",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "displayDataType",
          "value": "true",
          "widget": "array",
          "title": "Display Data Type",
          "description": "If true display rows DataType",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    }
  ],
  "edges": [
    {
      "source": "1",
      "target": "6",
      "id": 1
    },
    {
      "source": "8",
      "target": "9",
      "id": 2
    },
    {
      "source": "6",
      "target": "8",
      "id": 3
    },
    {
      "source": "12",
      "target": "6",
      "id": 4
    },
    {
      "source": "9",
      "target": "13",
      "id": 5
    }
  ],
  "dataSetDetails": [
    {
      "id": 2825,
      "uuid": "bf00e0bf-0646-4c37-bad7-8a9df02f8cdd",
      "header": true,
      "path": "/home/sparkflows/fire-data/data/CPG/Business-File-Loading-And-Validating/raw-data/sample_items_master.csv",
      "delimiter": ",",
      "datasetType": "CSV",
      "datasetSchema": "{\"colNames\":[\"item_id\",\"item_description\"],\"colTypes\":[\"INTEGER\",\"STRING\"],\"colFormats\":[\"\",\"\"],\"colMLTypes\":[\"NUMERIC\",\"TEXT\"]}"
    }
  ],
  "engine": "scala"
}