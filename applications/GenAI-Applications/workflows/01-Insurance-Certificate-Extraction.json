{
  "name": "01-Insurance-Certificate-Extraction",
  "uuid": "0240f5d7-b1da-4c07-8678-5d5c799a0054",
  "category": "Translation",
  "description": "-",
  "parameters": " --var workspace=test --var destinationPath=/home/sparkflows/fire-data/data/GENAI/GenAI-Applications/Insurance-Certificate-Extraction/test/Certificate-of-Insurance-Samples_1.pdf --var read=true --var execute2=true --var viewPdf1=false --var folderPath=/data/GENAI/GenAI-Applications/Insurance-Certificate-Extraction/test/excel/",
  "nodes": [
    {
      "id": "1",
      "name": "Form-Extractor",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "<h2>Pyspark Details</h2>\n<br>\nThis node receives receives an input pyspark dataframe in function called myfn.<br>\n<br>\nThe pyspark/python code processes it and returns one computed pyspark dataframe.<br>",
      "examples": "<h2>Pyspark Examples</h2>\n<br>\nInput Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> Add the house_type column</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\nhouse_type_udf = udf(lambda bedrooms: \"big house\" if int(bedrooms) >2 else \"small house\", StringType())<br>\nfiletr_df = inDF.select(\"id\", \"price\", \"lotsize\", \"bedrooms\")<br>\noutDF = filetr_df.withColumn(\"house_type\", house_type_udf(filetr_df.bedrooms))<br>\nreturn outDF<br>\n<br>\n<h4> Using pandas dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\n<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n# Convert the Spark DataFrame to a Pandas DataFrame<br>\npdf = inDF.select(\"*\").toPandas()<br>\n<br>\n# Display the result on the Executions page<br>\nworkflowContext.outStr(id, \"Outputting Pandas Dataframe\")<br>\n<br>\n# Display the dataframe on the Executions page<br>\nworkflowContext.outPandasDataframe(id, \"Pandas DataFrame\", pdf, 10)<br>\n<br>\n# Create a Spark DataFrame from a Pandas DataFrame<br>\ndf = spark.createDataFrame(pdf)<br>\n<br>\nreturn df<br>\n<br>\n<h4> Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nimport numpy as np<br>\nimport pandas as pd<br>\n<br>\nfrom fire.workflowcontext import *<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n<br>\n# Create the numpy 2d array<br>\nexample_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])<br>\n<br>\n# Convert to Pandas Dataframe<br>\npandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])<br>\n<br>\n# Convert Pandas Dataframe to Spark Dataframe<br>\nspark_dataframe = spark.createDataFrame(pandas_dataframe)<br>\nreturn spark_dataframe<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "217.889px",
      "y": "127.889px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \nimport fitz\nimport requests\nfrom PIL import Image\nimport io\nimport base64\nimport json\nimport pandas as pd\nimport re \nimport os\ndef get_latest_pdf(folder_path):\n    \"\"\"\n    Returns the path to the latest PDF file in a given folder.\n\n    Args:\n        folder_path (str): The path to the folder.\n\n    Returns:\n        str: The path to the latest PDF file, or None if no PDF files are found.\n    \"\"\"\n\n    # List all files in the directory\n    files = os.listdir(folder_path)\n\n    # Filter to include only PDF files\n    pdf_files = [file for file in files if file.endswith('.pdf')]\n\n    # If no PDF files are found, return None\n    if not pdf_files:\n        return None\n\n    # Full paths to the files\n    pdf_files_full_paths = [os.path.join(folder_path, file) for file in pdf_files]\n\n    # Get the latest file by modification time using sorted\n    latest_pdf = sorted(pdf_files_full_paths, key=os.path.getmtime)[-1]\n\n    return latest_pdf\n\ndef convert_first_page_to_base64(pdf_path):\n    pdf_document = fitz.open(pdf_path)\n\n    # Process only the first page\n    page = pdf_document.load_page(0)  # Page index starts at 0\n    pix = page.get_pixmap(dpi=500)\n\n    # Convert pixmap to bytes\n    img_bytes = pix.tobytes()\n\n    # Convert bytes to PIL Image\n    img = Image.open(io.BytesIO(img_bytes))\n    img.save(\"IMG.png\", format=\"PNG\")\n    # Save the image to bytes in PNG format\n    buffered = io.BytesIO()\n    img.save(buffered, format=\"PNG\")\n    img_bytes_png = buffered.getvalue()\n\n    # Encode bytes to base64\n    img_base64 = base64.b64encode(img_bytes_png).decode('utf-8')\n\n    pdf_document.close()\n    return img_base64\n  \ndef extract_text_from_image(image, api_key):\n    # url = 'https://api.openai.com/v1/engines/gpt-4-0/completions'  # Example URL; replace with actual if needed\n\n    headers = {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": f\"Bearer {api_key}\"\n    }\n\n    payload = {\n      \"model\": \"gpt-4o\",\n      \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\" Here is the sample output format that I want everytime from you as response. All the data is dummy. you havbe to follow this nstructure and get the data from image\n\n{\n  \"Certificate of Insurance\": {\n    \"Producer\": \"SAMPLE CERTIFICATE\",\n    \"Insurance Agency\": \"Insurance Agency Name & Address\",\n    \"Insured\": {\n      \"Name\": \"SAMPLE CERTIFICATE\",\n      \"Address\": \"Subcontractor name & complete address\"\n    },\n    \"Companies Affording Coverage\": {\n      \"A\": \"A Insurance Company\",\n      \"B\": \"B Insurance Company\",\n      \"C\": \"\",\n      \"D\": \"\"\n    },\n    \"Coverages\": [\n      {\n        \"CO LTR\": \"\",\n        \"Type of Insurance\": \"GENERAL LIABILITY\",\n        \"Policy Number\": \"\",\n        \"Policy Effective Date\": \"MM/DD/YY\",\n        \"Policy Expiration Date\": \"MM/DD/YY\",\n        \"Limits\": {\n          \"General Aggregate\": \"$\",\n          \"Products - Comp/Op Agg\": \"$\",\n          \"Personal & Adv Injury\": \"$\",\n          \"Each Occurrence\": \"$\",\n          \"Fire Damage (Any one fire)\": \"$\",\n          \"Med Exp (Any one person)\": \"$\"\n        },\n        \"Coverages\": {\n          \"COMMERCIAL GENERAL LIABILITY\": \"\",\n          \"CLAIMS MADE\": \"\",\n          \"OCCUR\": \"\",\n          \"OWNER'S & CONT PROT\": \"\"\n        }\n      },\n      {\n        \"CO LTR\": \"\",\n        \"Type of Insurance\": \"AUTOMOBILE LIABILITY\",\n        \"Policy Number\": \"\",\n        \"Policy Effective Date\": \"MM/DD/YY\",\n        \"Policy Expiration Date\": \"MM/DD/YY\",\n        \"Limits\": {\n          \"Combined Single Limit (Ea accident)\": \"$\",\n          \"Bodily Injury (Per person)\": \"$\",\n          \"Bodily Injury (Per accident)\": \"$\",\n          \"Property Damage\": \"$\"\n        },\n        \"Coverages\": {\n          \"ANY AUTO\": \"\",\n          \"ALL OWNED AUTOS\": \"\",\n          \"SCHEDULED AUTOS\": \"\",\n          \"HIRED AUTOS\": \"\",\n          \"NON-OWNED AUTOS\": \"\"\n        }\n      },\n      {\n        \"CO LTR\": \"\",\n        \"Type of Insurance\": \"EXCESS LIABILITY\",\n        \"Policy Number\": \"\",\n        \"Policy Effective Date\": \"MM/DD/YY\",\n        \"Policy Expiration Date\": \"MM/DD/YY\",\n        \"Limits\": {\n          \"Each Occurrence\": \"$\",\n          \"Aggregate\": \"$\"\n        },\n        \"Coverages\": {\n          \"UMBRELLA FORM\": \"\",\n          \"OTHER THAN UMBRELLA FORM\": \"\"\n        }\n      },\n      {\n        \"CO LTR\": \"\",\n        \"Type of Insurance\": \"WORKER'S COMPENSATION AND EMPLOYER'S LIABILITY\",\n        \"Policy Number\": \"\",\n        \"Policy Effective Date\": \"MM/DD/YY\",\n        \"Policy Expiration Date\": \"MM/DD/YY\",\n        \"Limits\": {\n          \"Statutory Limits\": \"\",\n          \"EACH ACCIDENT\": \"$\",\n          \"DISEASE - POLICY LIMIT\": \"$\",\n          \"DISEASE - EACH EMPLOYEE\": \"$\"\n        },\n        \"Coverages\": {\n          \"THE PROPRIETOR / PARTNERS/EXECUTIVE OFFICERS ARE_INCL\": \"\",\n          \"THE PROPRIETOR / PARTNERS/EXECUTIVE OFFICERS ARE_EXCL\": \"\"\n        }\n      }\n    ],\n    \"Description of Operations/Locations/Vehicles/Special Items\": \"Per project aggregate applies to General Liability Policy. Sample Construction, Inc., the Owner and all other parties as required by contract are named as an Additional Injured on a primary and noncontributing basis. (Please attached copy of Additional Insured form or indicate form number). A 30 day written notice of cancellation applies.\",\n    \"Certificate Holder\": {\n      \"Name\": \"Sample Construction, Inc.\",\n      \"Address\": \"123 Nuffield Avenue\\nSomewhere, USA 00124\"\n    },\n    \"Cancellation\": \"SHOULD ANY OF THE ABOVE DESCRIBED POLICIES BE CANCELLED BEFORE THE EXPIRATION DATE THEREOF, THE ISSUING COMPANY WILL ENDEAVOR TO MAIL 30 DAYS WRITTEN NOTICE TO THE CERTIFICATE HOLDER NAMED TO THE LEFT BUT FAILURE TO MAIL SUCH NOTICE SHALL IMPOSE NO OBLIGATION OR LIABILITY OF ANY KIND UPON THE COMPANY, ITS AGENTS OR REPRESENTATIVES.\",\n    \"Authorized Representative\": \"\"\n  }\n}\"\"\"\n        }, \n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\n              \"type\": \"text\",\n              \"text\": \"\"\"Please extract all the form fields from the certificate of insurance and present them in a hierarchical JSON format. Pay attention to all the details\n              - For each type of Insurance you will find some sub type with some value \"X\" and empty. Captrure them as well\n              - Only give the json object as a response. Do not provide other text except the json object\"\"\"},\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{image}\"\n              }\n            }\n          ]\n        }\n      ],\n      \"max_tokens\": 2000\n    }\n    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n    result = response.json()\n    print(result)\n    return result\n  \ndef clean_column_name(name):\n    # Replace special characters with underscores\n    name = re.sub(r'[^\\w\\s]', '_', name)  # Replace non-alphanumeric characters with _\n    name = re.sub(r'\\s+', '_', name)       # Replace spaces with underscores\n    return name\n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n  pdf_path = '${destinationPath}'\n  output_path = '${genaiHome}'+'${folderPath}'\n  workspace = '${workspace}'\n  # if workspace == \"test\":\n  #  pdf_path = get_latest_pdf(pdf_path)\n  #  print(\"================== \", pdf_path)\n  # if not os.path.exists(output_path):\n  #os.makedirs(output_path)\n  #  print(f\"Response folder '{output_path}' created successfully.\")\n  # else:\n  #  print(f\"Response folder '{output_path}' already exists.\")\n  #latest_path = get_latest_pdf(pdf_path)\n  api_key = '$Openai_Api'\n  #outDF = inDF  #add custom logic\n  image = convert_first_page_to_base64(pdf_path)\n  extracted_data = extract_text_from_image(image, api_key)\n  print(\"============================================\")\n  a = extracted_data[\"choices\"][0][\"message\"][\"content\"]\n  print(a)\n  print(\"=============================================\")\n  print(a.replace(\"```\",\"\")[4:])\n  print(\"=============================================\")\n  if \"```json\" in a:\n  \tdata = json.loads(a.replace(\"```\",\"\")[4:])\n  else:\n    data = json.loads(a)\n  print(\"================================================\")\n  producer = data[\"Certificate of Insurance\"][\"Insurance Agency\"]\n  insured = data[\"Certificate of Insurance\"][\"Insured\"][\"Address\"]\n  companies = data[\"Certificate of Insurance\"] [\"Companies Affording Coverage\"]\n  print(\"====================================\")\n  insurance_info_part1 = pd.DataFrame({\"Producer\": producer, \"Insured\": insured}, index= [0])\n  insurance_info_part2 = pd.DataFrame(companies,  index= [0])\n  print(\"====================================\")\n  insurance_info_part2.columns = [f\"Company {x}\" for x in list(insurance_info_part2.columns)]\n  print(\"====================================\")\n  insurance_info = pd.concat([insurance_info_part1,insurance_info_part2], axis=1)\n  print(insurance_info)\n  final_data = []\n  for i in data[\"Certificate of Insurance\"]['Coverages']:\n    co_ltr = i[\"CO LTR\"]\n    insurance_type = i[\"Type of Insurance\"]\n    policy_number = i[\"Policy Number\"]\n    policy_effective_date = i[\"Policy Effective Date\"]\n    policy_expiration_date = i[\"Policy Expiration Date\"]\n    limits= i[\"Limits\"]\n    coverages = i[\"Coverages\"]\n\n    insurance_coverage_part1 = pd.DataFrame({\"CO LTR\":co_ltr, \"Type of Insurance\": insurance_type,'Policy Number': policy_number,\"Policy Effective Date\": policy_effective_date,\"Policy Expiration Date\":  policy_expiration_date  }, index= [0])\n    insurance_coverage_part2 = pd.DataFrame(coverages, index = [0])\n    insurance_coverage_part3 = pd.DataFrame(limits, index=[0])\n\n    final_data.append(pd.concat([insurance_coverage_part1, insurance_coverage_part2, insurance_coverage_part3], axis=1))\n  df_combined = pd.concat([final_data[0], final_data[1], final_data[2], final_data[3]], axis=0, ignore_index=True)\n  html_table  = df_combined.to_html(classes='my_table', index=False)\n  html_output = f\"\"\"\n  <head>\n      <meta charset=\"UTF-8\">\n      <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n      <style>\n          .my_table {{\n              border-collapse: collapse;\n              width: 100%;\n          }}\n          .my_table th, .my_table td {{\n              border: 1px solid #ddd;\n              padding: 8px;\n          }}\n          .my_table th {{\n              color: #1e4684;  /* Color for column names */\n              text-align: center;\n          }}\n          .my_table td {{\n              text-align: center;\n          }}\n          .my_table tr:nth-child(even) {{\n              background-color: #f2f2f2;\n          }}\n          h2 {{\n              color: #0eb29a;  /* Color for the title */\n              font-weight: 400; /* Font weight */\n              font-size: 1.75rem; /* Font size */\n          }}\n      </style>\n  </head>\n  <body>\n      <h2>Insurance Coverage</h2>\n      {html_table}\n  </body>\n  </html>\n  \"\"\"\n  workflowContext.outPandasDataframe(1, \"Insurance Information\", insurance_info, 10, displayDataType = False)\n  workflowContext.outHTML(9, title=\"Insurance Coverage\", text = html_output.replace(\"<td>NaN</td>\",\"<td></td>\"))\n  #df_combined.columns = [clean_column_name(col) for col in df_combined.columns]\n  \n  #workflowContext.outPandasDataframe(2, \"Insurance Coverages\", df_combined, 10, displayDataType = True)\n  \n    # Write each DataFrame to a different worksheet\n  #insurance_info.to_excel(excel_writer = \"/home/sparkflows/fire-data/insurance-certificate-extraction/output.xlsx\",sheet_name='Insurance Info', index=False, engine = 'openpyxl')\n  #df_combined.to_excel(excel_writer = \"/home/sparkflows/fire-data/insurance-certificate-extraction/output.xlsx\", sheet_name='Insurance Coverage', index=False, engine = 'openpyxl')\n  '''with pd.ExcelWriter(f\"{output_path}output.xlsx\") as writer: # pylint: disable=abstract-class-instantiated\n    # Write each DataFrame to a different worksheet\n  \tinsurance_info.to_excel(writer, sheet_name='InsuranceInfo', index=False)\n  \tdf_combined.to_excel(writer, sheet_name='InsuranceCoverage', index=False)'''\n  df_combined.fillna(\"\", inplace=True)\n  df_combined = df_combined.astype(\"str\")\n  df_combined.columns = [clean_column_name(col) for col in df_combined.columns]\n  print(1)\n  outDf = spark.createDataFrame(df_combined)\n  print(2)\n  return outDf",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[\"Producer\",\"Insured\",\"Company_A\",\"Company_B\",\"Company_C\",\"Company_D\"]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [],
  "dataSetDetails": [],
  "engine": "pyspark"
}