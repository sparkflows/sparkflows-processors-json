{
  "name": "01-Create-Vector-Embedding",
  "uuid": "1ee98c61-437f-4741-992d-b484291aab4c",
  "category": "-",
  "description": "-",
  "parameters": "--var userQuery='' --var files='' --var directory='/home/sparkflows/fire-data/project_130/chatbot'",
  "nodes": [
    {
      "id": "1",
      "name": "PySpark",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "<h2>Pyspark Details</h2>\n<br>\nThis node receives receives an input pyspark dataframe in function called myfn.<br>\n<br>\nThe pyspark/python code processes it and returns one computed pyspark dataframe.<br>",
      "examples": "<h2>Pyspark Examples</h2>\n<br>\nInput Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> Add the house_type column</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\nhouse_type_udf = udf(lambda bedrooms: \"big house\" if int(bedrooms) >2 else \"small house\", StringType())<br>\nfiletr_df = inDF.select(\"id\", \"price\", \"lotsize\", \"bedrooms\")<br>\noutDF = filetr_df.withColumn(\"house_type\", house_type_udf(filetr_df.bedrooms))<br>\nreturn outDF<br>\n<br>\n<h4> Using pandas dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\n<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n# Convert the Spark DataFrame to a Pandas DataFrame<br>\npdf = inDF.select(\"*\").toPandas()<br>\n<br>\n# Display the result on the Executions page<br>\nworkflowContext.outStr(id, \"Outputting Pandas Dataframe\")<br>\n<br>\n# Display the dataframe on the Executions page<br>\nworkflowContext.outPandasDataframe(id, \"Pandas DataFrame\", pdf, 10)<br>\n<br>\n# Create a Spark DataFrame from a Pandas DataFrame<br>\ndf = spark.createDataFrame(pdf)<br>\n<br>\nreturn df<br>\n<br>\n<h4> Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nimport numpy as np<br>\nimport pandas as pd<br>\n<br>\nfrom fire.workflowcontext import *<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n<br>\n# Create the numpy 2d array<br>\nexample_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])<br>\n<br>\n# Convert to Pandas Dataframe<br>\npandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])<br>\n<br>\n# Convert Pandas Dataframe to Spark Dataframe<br>\nspark_dataframe = spark.createDataFrame(pandas_dataframe)<br>\nreturn spark_dataframe<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "509px",
      "y": "300px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import *\nfrom fire.workflowcontext import WorkflowContext\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import DirectoryLoader, PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nimport faiss\nimport numpy as np\nimport openai\nimport pymupdf\nimport os\nimport requests\nfrom PIL import Image\nimport io\nimport base64\nimport boto3\nfrom langchain.embeddings import BedrockEmbeddings\nimport requests\nimport uuid\nimport os\n\n\ndef convert_first_page_to_base64(pdf_path):\n    pdf_document = pymupdf.open(pdf_path)\n\n    # Process only the first page\n    page = pdf_document.load_page(0)  # Page index starts at 0\n    pix = page.get_pixmap(dpi=300)\n\n    # Convert pixmap to bytes\n    img_bytes = pix.tobytes()\n\n    # Convert bytes to PIL Image\n    img = Image.open(io.BytesIO(img_bytes))\n    img.save(\"IMG.png\", format=\"PNG\")\n    # Save the image to bytes in PNG format\n    buffered = io.BytesIO()\n    img.save(buffered, format=\"PNG\")\n    img_bytes_png = buffered.getvalue()\n\n    # Encode bytes to base64\n    img_base64 = base64.b64encode(img_bytes_png).decode('utf-8')\n\n    pdf_document.close()\n    return img_base64\n\n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n    image = []\n    pdf = []\n    # print(os.listdir(\"/home/vivek/Downloads/Cancer Drug Regulatory Precedents Library/\"))\n    paths = \"${directory}\"\n    Vector_Path = paths.split('/')[-2]\n    print(Vector_Path, \"Vector_Path\")\n    pdf_file = \"/home/sparkflows/fire-data/\"+Vector_Path+\"/chatbot/\"  \n    for i in os.listdir(pdf_file):\n      if i not in [\".config\",\"sample_data\" ]:\n        image.append(convert_first_page_to_base64(pdf_file + str(i)))\n        pdf.append(i)\n    \n    \n    GOOGLE_API_KEY=\"\"\n    import json\n    year = []\n    for i,j in zip(image,pdf):\n      payload = {\n          \"contents\": [\n              {\n                  \"parts\": [\n                      {\n                          \"inlineData\": {\n                              \"mimeType\": \"image/png\",\n                              \"data\": i\n                          }\n                      },\n                      {\"text\": f\"\"\"Extract the published year from the image in below format\n                      key would be the {j} and value will be the year\"\"\"}\n                  ]\n              }\n          ],\n          \"generationConfig\": { \"responseMimeType\": \"application/json\" }\n      }\n      headers = {\n          'Content-Type': 'application/json'\n      }\n      model1 = \"gemini-1.5-pro-exp-0801\"\n      model2 = \"gemini-1.5-flash\"\n    \n      response = requests.post(f\"https://generativelanguage.googleapis.com/v1beta/models/{model2}:generateContent?key={GOOGLE_API_KEY}\", headers=headers, data=json.dumps(payload))\n      # print(response.json())\n      year.append(response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"])\n    \n    year = [json.loads(i) for i in year]\n    year_dict = {}\n    for i in year:\n      year_dict.update(i)\n    \n    \n    loader = DirectoryLoader(pdf_file, glob=\"**/*.pdf\", show_progress=True, loader_cls=PyPDFLoader)\n    docs = loader.load()\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1250, chunk_overlap=150, add_start_index=True)\n    splits = text_splitter.split_documents(docs)\n    for i in splits:\n        source_file_name = i.metadata['source'].split('/')[-1]  # Extract the filename from the source path\n        year = year_dict.get(source_file_name, \"Unknown Year\")  # Get the year from the dictionary or default to \"Unknown Year\"    \n        i.metadata['year'] = year\n    # embeddings = OpenAIEmbeddings()\n    access_key = \"XAKYKXX\"\n    access_secret = \"XM6XXYYXX\"\n    bedrock_client = boto3.client(service_name='bedrock-runtime',\n                       region_name='us-east-1',\n                       aws_access_key_id=access_key,\n                       aws_secret_access_key=access_secret)\n    # bedrock_client = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')\n    bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)\n    \n    # Define the Pinecone API key and the endpoint for index creation\n    PINECONE_API_KEY = \"6595be70-d78a-4bca-8f5e-d3790e45b522\"\n    INDEX_HOST = \"https://api.pinecone.io/indexes\"\n    \n    # Function to create an index\n    def create_index(index_name, dimension=1536, metric=\"cosine\", cloud=\"aws\", region=\"us-east-1\"):\n        url = INDEX_HOST\n    \n        headers = {\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\",\n            \"Api-Key\": PINECONE_API_KEY,\n            \"X-Pinecone-API-Version\": \"2024-07\"\n        }\n    \n        data = {\n            \"name\": index_name,\n            \"dimension\": dimension,\n            \"metric\": metric,\n            \"spec\": {\n                \"serverless\": {\n                    \"cloud\": cloud,\n                    \"region\": region\n                }\n            }\n        }\n    \n        response = requests.post(url, headers=headers, json=data)\n        \n        if response.status_code == 201:\n            print(f\"Index {index_name} created successfully.\")\n        else:\n            print(f\"Failed to create index: {response.json()}\")\n    \n    # Function to check if index exists or create it\n    def get_db_index(index_name):\n        url = INDEX_HOST\n        \n        headers = {\n            \"Api-Key\": PINECONE_API_KEY\n        }\n        \n        response = requests.get(url, headers=headers)\n        \n        print(f\"Response status code: {response.status_code}\")\n        print(f\"Response content: {response.content}\")  # Print raw content to understand the issue\n        \n        try:\n            response_json = response.json()  # Attempt to parse as JSON\n            print(f\"Response JSON: {response_json}\")\n        except ValueError:\n            print(\"Failed to parse response as JSON\")\n            return\n    \n        if isinstance(response_json, dict) and 'indexes' in response_json:\n            indexes = [index['name'] for index in response_json['indexes']]\n            if index_name not in indexes:\n                create_index(index_name)\n            else:\n                print(f\"Index {index_name} already exists.\")\n        else:\n            print(\"Unexpected response format, expected a JSON list or dict with 'indexes'.\")\n        def get_host_by_name(data, search_name):\n            for index in data['indexes']:\n                if index['name'] == search_name:\n                    return index['host']\n            return None\n        host = get_host_by_name(response.json(), index_name)\n        return host\n    converted_string = Vector_Path.replace('_', '-')\n    index_name = converted_string\n    create_index(index_name)\n    indx_host = get_db_index(index_name)\n    # Replace with your actual API key and index host\n    PINECONE_API_KEY = \"6595be70-d78a-4bca-8f5e-d3790e45b522\"\n    INDEX_HOST = 'https://' + indx_host\n    BATCH_SIZE = 100  # Adjust the batch size as needed to keep the payload under 4 MB\n    \n    def prepare_vectors(splits):\n        text = [split.page_content for split in splits]\n        embedding = bedrock_embeddings.embed_documents(text)  # Replace with actual embedding logic\n        \n        vectors = []\n        for i, split in enumerate(splits):\n            metadata = {\n                'pdf_name': split.metadata['source'],\n                'page_num': split.metadata['page'],\n                'text': split.page_content,\n                \"year\": year_dict[os.path.basename(split.metadata['source'])]\n            }\n            \n            vectors.append({\n                'id': str(uuid.uuid4()),  # Generate a unique ID\n                'values': embedding[i],\n                'metadata': metadata\n            })\n        \n        return vectors\n    \n    # Function to send vectors in batches\n    def upsert_vectors_in_batches(vectors, batch_size):\n        url = f\"{INDEX_HOST}/vectors/upsert\"\n        headers = {\n            \"Api-Key\": PINECONE_API_KEY,\n            \"Content-Type\": \"application/json\",\n            \"X-Pinecone-API-Version\": \"2024-07\"\n        }\n    \n        session = requests.Session()\n        \n        for i in range(0, len(vectors), batch_size):\n            batch = vectors[i:i + batch_size]\n            data = {\"vectors\": batch}\n            \n            try:\n                response = session.post(url, headers=headers, json=data, timeout=30)\n                print(f\"Batch {i//batch_size + 1} Status Code: {response.status_code}\")\n                \n                try:\n                    print(f\"Response Body: {response.json()}\")\n                except ValueError:\n                    print(\"Response was not JSON format\")\n            except requests.exceptions.RequestException as e:\n                print(f\"An error occurred during batch {i//batch_size + 1}: {e}\")\n    \n        session.close()\n    \n    # Prepare the vectors\n    vectors = prepare_vectors(splits)\n    \n    # Upsert vectors in batches\n    upsert_vectors_in_batches(vectors, BATCH_SIZE)\n    \n    return",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "Schema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "disabled": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [],
  "dataSetDetails": [],
  "engine": "pyspark"
}