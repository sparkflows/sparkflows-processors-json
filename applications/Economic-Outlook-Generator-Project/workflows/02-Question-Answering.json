{
  "name": "02-Question-Answering",
  "uuid": "b0bd7bb1-3e17-42d1-87d8-1b01cf514717",
  "category": "-",
  "description": "-",
  "parameters": "--var destinationPath=/home/sparkflows/fire-data/data/GENAI/Economic-Outlook-Generator-Project/Ref-Document-Repo-2025-Q1/Uploads/",
  "nodes": [
    {
      "id": "1",
      "name": "PySpark",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "<h2>Pyspark Details</h2>\n<br>\nThis node receives receives an input pyspark dataframe in function called myfn.<br>\n<br>\nThe pyspark/python code processes it and returns one computed pyspark dataframe.<br>",
      "examples": "<h2>Pyspark Examples</h2>\n<br>\nInput Schema: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea<br>\n<br>\n<h4> Add the house_type column</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\nhouse_type_udf = udf(lambda bedrooms: \"big house\" if int(bedrooms) >2 else \"small house\", StringType())<br>\nfiletr_df = inDF.select(\"id\", \"price\", \"lotsize\", \"bedrooms\")<br>\noutDF = filetr_df.withColumn(\"house_type\", house_type_udf(filetr_df.bedrooms))<br>\nreturn outDF<br>\n<br>\n<h4> Using pandas dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\n<br>\nfrom fire.workflowcontext import *<br>\n<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n# Convert the Spark DataFrame to a Pandas DataFrame<br>\npdf = inDF.select(\"*\").toPandas()<br>\n<br>\n# Display the result on the Executions page<br>\nworkflowContext.outStr(id, \"Outputting Pandas Dataframe\")<br>\n<br>\n# Display the dataframe on the Executions page<br>\nworkflowContext.outPandasDataframe(id, \"Pandas DataFrame\", pdf, 10)<br>\n<br>\n# Create a Spark DataFrame from a Pandas DataFrame<br>\ndf = spark.createDataFrame(pdf)<br>\n<br>\nreturn df<br>\n<br>\n<h4> Numpy 2d array to pandas dataframe & pandas dataframe to spark dataframe</h4>\n<br>\nfrom pyspark.sql.types import StringType<br>\nfrom pyspark.sql.functions import *<br>\nfrom pyspark.sql import *<br>\nimport numpy as np<br>\nimport pandas as pd<br>\n<br>\nfrom fire.workflowcontext import *<br>\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict):<br>\n<br>\n# Create the numpy 2d array<br>\nexample_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])<br>\n<br>\n# Convert to Pandas Dataframe<br>\npandas_dataframe = pd.DataFrame(example_array, columns=['a', 'b', 'c', 'd'])<br>\n<br>\n# Convert Pandas Dataframe to Spark Dataframe<br>\nspark_dataframe = spark.createDataFrame(pandas_dataframe)<br>\nreturn spark_dataframe<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "367.889px",
      "y": "212.889px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"$openapi_key\"\nos.environ[\"GOOGLE_API_KEY\"] = \"$Gemini_API\"\n\nimport json\nimport base64\nimport openai\nimport fitz\nimport os\nfrom pdf2image import convert_from_path\nfrom langchain.document_loaders import DirectoryLoader, PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\n\nmodel = '${SelectModel}'\ndata_path = '${destinationPath}' #'${genaiHome}'+'${destinationPath}'\ndata_path =\"/\".join(data_path.split(\"/\")[:-1]) + \"/\"\ndatabase_directory = \"/\".join(data_path.split(\"/\")[:-2]) + \"/Database\"\nif not os.path.exists(database_directory):\n\tos.mkdir(database_directory)\n\n\n\nclass DocumentInsightGenerator:\n    def __init__(self, prompt):\n        self.images = []\n        self.base_prompt = [\n            {\n                \"role\": \"system\",\n                \"content\": \"you are a helpful assistant who's an expert at reading and creating financial documents and reports\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": prompt\n                    }\n                ]\n            }\n        ]\n\n    def encode_image(self, image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n\n    def add_image(self, image_path):\n        encoded_image = self.encode_image(image_path)\n        self.images.append(\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n                }\n            }\n        )\n\n    def generate_response(self, provider=\"openai\"):\n        # if not self.images:\n        #     raise ValueError(\"No images have been added.\")\n\n        # Append images to the user's content\n        self.base_prompt[1]['content'].extend(self.images)\n        if provider == \"openai\":\n            response = openai.ChatCompletion.create(\n                model=\"gpt-4o\",\n                response_format={ \"type\": \"json_object\" },\n                messages=self.base_prompt,\n                max_tokens=4096,\n            )\n        elif provider == \"gemini\":\n            #Setting the api key\n            gemini.api_key = GOOGLE_API_KEY\n            response = gemini.ChatCompletion.create(\n                model=\"gemini-1.5-pro-exp-0801\", \n                response_format={\"type\": \"json_object\"},\n                messages=self.base_prompt,\n                max_tokens=4096,\n            )\n        else:\n            raise ValueError(\"Unsupported provider\")\n        return response\n\n    def generate_insights(self):\n        response = self.generate_response()\n        return response.choices[0].message.content\n\n\nclass MultimodalPreprocessor():\n    def __init__(self, pdf_directory = None, output_directory = None, image_info_list = None, db = None):\n        self.pdf_directory = None\n        self.output_directory = None\n        self.image_info_list = []\n        self.db = None\n\n    def ingest_into_faiss(self, **faiss_params):\n        loader = DirectoryLoader(self.pdf_directory, glob=\"**/*.pdf\", show_progress=True, loader_cls=PyPDFLoader)\n        docs = loader.load()\n\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1250, chunk_overlap=150, add_start_index=True)\n        splits = text_splitter.split_documents(docs)\n\n        embeddings = OpenAIEmbeddings()\n        self.db = FAISS.from_documents(splits, embeddings)\n\n        return self.db\n\n    def convert_pdfs_to_images(self):\n            if not os.path.exists(self.output_directory):\n                os.makedirs(self.output_directory)\n\n            for pdf_file in os.listdir(self.pdf_directory):\n                if pdf_file.endswith(\".pdf\"):\n                    pdf_path = os.path.join(self.pdf_directory, pdf_file)\n                    doc = fitz.open(pdf_path)\n                    for page_num in range(len(doc)):\n                        page = doc.load_page(page_num)\n                        pix = page.get_pixmap()\n                        image_filename = f\"{os.path.splitext(pdf_file)[0]}_page_{page_num + 1}.png\"\n                        image_path = os.path.join(self.output_directory, image_filename)\n                        pix.save(image_path)\n                        self.image_info_list.append([image_path, pdf_file, page_num + 1])\n\n            return self.image_info_list\n\n    def preprocess(self, directory, save_on_disk = True): ## main method 1\n        self.pdf_directory = directory\n        self.output_directory = directory + '/images'\n        print(\"Ingesting into Faiss\")\n        self.ingest_into_faiss()\n        print(\"Converting PDFs to Images\")\n        self.convert_pdfs_to_images()\n        print(\"preprocessing Done!\")\n\n        if save_on_disk == True:\n            self.save_faiss_db()\n            self.save_image_db()\n\n        print(\"db saved on disk\")\n        # return self.db, self.image_info_list\n\n    def extract_page_numbers(self, search_results):\n        pg_nos = []\n        for doc in search_results:\n            pg_nos.append([doc.metadata['source'], doc.metadata['page']])\n        return pg_nos\n\n    def search_images(self, search_list):\n        result_list = []\n\n        for search_item in search_list:\n            search_pdf, search_page = search_item\n\n            for image_info in self.image_info_list:\n                image_path, source_pdf, page_number = image_info\n\n                if os.path.basename(search_pdf) == source_pdf and search_page == page_number:\n                    result_list.append(image_path)\n                    break\n\n        return result_list\n\n    def multimodal_query(self, query, k = 7):\n        search_results = self.db.similarity_search(query, k = k)\n        pg_nos = self.extract_page_numbers(search_results)\n        result_paths = self.search_images(pg_nos)\n        return search_results, result_paths\n\n    def save_faiss_db(self):\n        self.db.save_local(database_directory + \"/faiss_index\")\n\n    def load_faiss_db(self):\n        self.db = FAISS.load_local(database_directory + \"/faiss_index\", embeddings=OpenAIEmbeddings())\n        return self.db\n\n    def save_image_db(self):\n        with open(database_directory +\"/image_info_list.json\", \"w\") as f:\n            json.dump(self.image_info_list, f)\n    \n    def load_image_db(self):\n        with open(database_directory +\"/image_info_list.json\", \"r\") as f:\n            self.image_info_list = json.load(f)\n        return self.image_info_list\n\n    def format_docs(self, docs):\n        return \"\\n\\n\".join(doc.page_content+ \"source: \" + doc.metadata['source'] for doc in docs)\n\nclass MultimodalRAG(MultimodalPreprocessor):\n    def __init__(self, image_info_list = [], db = None, load_from_disk = True, path = None, faiss_index_path = None, image_info_path = None):\n        if db is None:\n            if os.path.exists(database_directory +\"/faiss_index\") and load_from_disk:\n                self.db = self.load_faiss_db()\n                self.image_info_list = self.load_image_db()\n                print(\"loaded from disk\")\n            #if faiss_index_path is not None:\n                #self.db = self.load_faiss_db(faiss_index_path)\n                #self.image_info_list = self.load_image_db(image_info_path)\n                #print(\"loaded DB from path\")\n            elif path is not None:\n                self.db, self.image_info_list = None, []\n                self.preprocess(path)\n                print(\"loaded Data from path\")\n            else:\n                super().__init__(image_info_list, db)\n        else:\n            self.image_info_list = image_info_list\n            self.db = db\n        \n        if self.db is None:\n            print(\"Vector Database not setup, initialize object with Data path\")\n\n    def create_base_prompt(self, context, query):\n        prompt = \"\"\"Use the following pieces of context to answer the question at the end.\n        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n        do not hallucinate\n\n        follow the below instructions strictly:\n        \n        you are an expert at question answering, you will be given some context based on which you will reply to the user's questions, if the question is not related to the context, then say you do not know the answer (try to use the given context to answer the question even if the context is minimal)\n\n        you will also be given images of the context, which may include charts and tables. use these values to generate the report (remember facts and figures are very important for a financial report)\n\n        return the content as a json of this format (follow this format strictly, the output has to be a json, do not include any character not supported in jsons)\n\n        {\n            \"answer\": generated answer in HTML format. The font size should be little bigger. Every answer should be well structured in HTML format\n        }\n\n        follow the instructions strictly (do not change formats, keep it consistent)\n\n        you will also include the source of the sentence you are using to generate the answer. (sources will be mentioned towards the end of sentences as citations, use only the filename as source, do not return the entire path )\"\"\" + f\"\"\"\n        {context}\n\n        Question: {query} \"\"\"\n\n        return prompt\n\n\n    def query(self, query):\n        # search_results, result_paths = self.multimodal_query(query)\n        docs1 = self.db.similarity_search(query, k = 7)\n        context = self.format_docs(docs1)\n        pg_nos = self.extract_page_numbers(docs1)\n        image_info_list = self.image_info_list\n        search_list = pg_nos\n        result_paths = self.search_images(search_list)\n\n        prompt = self.create_base_prompt(context, query)\n\n        generator = DocumentInsightGenerator(prompt)\n\n        for i in result_paths:\n            generator.add_image(i)\n\n        insights = generator.generate_insights()\n\n        ins = json.loads(insights)\n        return ins[\"answer\"]\n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n  if os.path.exists(database_directory + \"/faiss_index\"):\n    mm = MultimodalRAG()\n    print(\"loaded from disk\")\n  else:\n    mm = MultimodalRAG(path = data_path)\n    print(\"loaded from path\")\n    \n  selectQueryOption = '${selectQueryOptions}'\n  if str(selectQueryOption) == '1':\n    query_text = '${query}'\n  else:\n    query_text = '${selectSampleQuery}'\n  answer = mm.query(query_text + \"\\nPlease generate the answer with proper HTML format.The font size would be little bigger. Every answer should be well structured in HTML format\")  #add custom logic\n  workflowContext.outHTML(id, \"Answer\", answer)\n  return",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    },
    {
      "id": "2",
      "name": "Sticky Note",
      "iconImage": "fa fa-file-text",
      "description": "Allows capturing Notes on the Workflow",
      "details": "",
      "examples": "",
      "type": "sticky",
      "nodeClass": "fire.nodes.doc.NodeStickyNote",
      "x": "507px",
      "y": "300px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "bgColor",
          "value": "gray",
          "widget": "textfield",
          "title": "Bg Color",
          "description": "Background of note",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "width",
          "value": "300px",
          "widget": "textfield",
          "title": "Width",
          "description": "Width of note",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "height",
          "value": "110px",
          "widget": "textfield",
          "title": "Height",
          "description": "Height of note",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "comment",
          "value": "<p>The FAQ and custom queries are answered with the help of selected model.</p>",
          "widget": "textarea_rich",
          "title": "Comment",
          "description": "Comments for the Workflow",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    }
  ],
  "edges": [],
  "dataSetDetails": [],
  "engine": "pyspark"
}