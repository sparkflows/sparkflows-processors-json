{
  "name": "02-ETL-Payment-Allocation-Advanced",
  "uuid": "19fd09e1-fc43-4205-a298-aa967cfbd25d",
  "category": "ETL",
  "nodes": [
    {
      "id": "1",
      "name": "Dataset Structured",
      "iconImage": "fa fa-th-list",
      "description": "This Node creates a DataFrame by reading data from HDFS, HIVE etc. The dataset was defined earlier in Fire by using the Dataset Feature. As a user, you just have to select the Dataset of your interest.",
      "details": "This Node creates a DataFrame by reading data from HDFS, HIVE etc.<br>\n<br>\nThe data has been defined earlier in Fire by using the Dataset Feature. As a user, you just have to select the Dataset of your interest.<br>",
      "examples": "",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetStructured",
      "x": "60.165401px",
      "y": "113.165001px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "dataset",
          "value": "01974bac-e871-4566-9b7b-cda2590cd4b7",
          "widget": "dataset",
          "title": "Dataset",
          "description": "Selected Dataset",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        }
      ],
      "engine": "all"
    },
    {
      "id": "4",
      "name": "Drop Null Rows for Selected Columns",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node creates a new DataFrame by dropping rows containing null values for selected Columns",
      "details": "This node creates a new DataFrame by dropping rows containing NULL values in selected columns.<br>",
      "examples": "Incoming Dataframe has following rows:<br>\n<br>\nEMP_CD    |    EMP_NAME    |    DEPT       |    AGE<br>\n-------------------------------------------------------<br>\nE01       |    DAVID       |    HR         |    25<br>\nE05       |    MARK        |               |    25<br>\nE02       |    JOHN        |    SALES      |    35<br>\nE03       |    TONY        |    MARKETING  |    <br>\nE04       |    MARTIN      |    MARKETING  |    45<br>\n<br>\nIncoming Dataframe has NULL values for two rows. For selected Columns DEPT and AGE,<br>\nusing DropRowsWithNull node would result in below outgoing Dataframe created by dropping rows having NULL values for selected columns:<br>\n<br>\nEMP_CD    |    EMP_NAME    |    DEPT       |    AGE<br>\n-------------------------------------------------------<br>\nE01       |    DAVID       |    HR         |    25<br>\nE02       |    JOHN        |    SALES      |    35<br>\nE04       |    MARTIN      |    MARKETING  |    45<br>",
      "type": "transform",
      "nodeClass": "fire.nodes.etl.NodeDropRowsWithNullMultipleColumns",
      "x": "249px",
      "y": "110px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "columns_to_check",
          "value": "[\"card_number\",\"payment_transaction_id\"]",
          "widget": "variables",
          "title": "Columns to Check",
          "description": "Columns to be processed for missing values",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        }
      ],
      "engine": "all"
    },
    {
      "id": "5",
      "name": "Imputing With Mean Value",
      "iconImage": "fa fa-tumblr-square",
      "description": "Imputing the continuous variables by mean.",
      "details": "This node imputes the missing values in the specified columns by mean of the values in the column.<br>",
      "examples": "Incoming Dataframe has following rows and [AGE] column has missing value / [NULL] for some rows:<br>\n<br>\nCUST_CD    |    CUST_NAME    |    AGE<br>\n-----------------------------------------<br>\nCD01       |    DAVID        |    30<br>\nCD02       |    MARY         |    40<br>\nCD03       |    PAUL         |\t\t\t<br>\nCD04       |    MATT         |\t\t\t<br>\n<br>\nIf imputingwithmeanvalue node is configured to Impute [AGE] with mean value then missing values in [AGE] column would be replaced with 35 which is the mean of [AGE] column.<br>\nOutgoing Dataframe would result as below:<br>\n<br>\nCUST_CD    |    CUST_NAME    |    AGE<br>\n-----------------------------------------<br>\nCD01       |    DAVID        |    30<br>\nCD02       |    MARY         |    40<br>\nCD03       |    PAUL         |    35\t\t\t<br>\nCD04       |    MATT         |    35<br>",
      "type": "transform",
      "nodeClass": "fire.nodes.ml.NodeReplaceMissingValueWithMean",
      "x": "405px",
      "y": "112px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "inputCols",
          "value": "[\"payment_amount\",\"actual_balance\",\"adjusted_amount\",\"remaining_balance\"]",
          "widget": "variables",
          "title": "Column Names",
          "description": "Columns type should be continuous",
          "datatypes": [
            "integer",
            "long",
            "double",
            "float"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        }
      ],
      "engine": "all"
    },
    {
      "id": "6",
      "name": "Imputing With Constant",
      "iconImage": "fa fa-tumblr-square",
      "description": "It imputes missing value with constant value. It fills missing values (None) in selected columns with given constant value for the corresponding column, in the incoming DataFrame.",
      "details": "This node imputes the missing value with constant value.<br>\n<br>\nIt fills missing values (None) in selected columns with given constant value for the corresponding column, in the incoming DataFrame.<br>",
      "examples": "Incoming Dataframe has following rows and [AGE] column has missing values / [NULL] for some rows:<br>\n<br>\nCUST_CD    |    CUST_NAME    |    AGE<br>\n-----------------------------------------<br>\nCD01       |    DAVID        |    30<br>\nCD02       |    MARY         |    40<br>\nCD03       |    PAUL         |\t\t\t<br>\nCD04       |    MATT         |\t\t\t<br>\n<br>\nIf imputingwithconstatnt node is configured to Impute [AGE] with 45 then missing values in [AGE] column would be replaced with 45.<br>\nOutgoing Dataframe would result as below:<br>\n<br>\nCUST_CD    |    CUST_NAME    |    AGE<br>\n-----------------------------------------<br>\nCD01       |    DAVID        |    30<br>\nCD02       |    MARY         |    40<br>\nCD03       |    PAUL         |    45\t\t\t<br>\nCD04       |    MATT         |    45<br>",
      "type": "transform",
      "nodeClass": "fire.nodes.ml.NodeReplaceMissingValueWithConstant",
      "x": "529px",
      "y": "111px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "colNames",
          "value": "[\"card_ref_number\",\"additional_data_1\",\"additional_data_2\",\"additional_data_3\",\"additional_data_4\",\"additional_data_5\"]",
          "widget": "variables_list_select",
          "title": "Columns",
          "description": "Columns to be processed for missing values",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "constants",
          "value": "[\"'XXXXX\\\"\",\"'default data'\",\"'default data'\",\"'default data'\",\"'default data'\",\"'default data'\"]",
          "widget": "variables_list_textfield",
          "title": "Constants",
          "description": "Missing value will be replaced with constant",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        }
      ],
      "engine": "all"
    },
    {
      "id": "7",
      "name": "Validate Fields Simple",
      "iconImage": "fa fa-tumblr-square",
      "description": "Validation Node",
      "details": "<h2>Validate Fields Simple Node Details</h2>\n<br>\nThis node creates two outgoing Dataframes based on the result of Validation Rules on the incoming Dataframe. <br>\nFirst outgoing Dataframe consists of rows that succeeds the Validation Rules and second one consists of failed rows.   <br>\n<br>\nThis node facilitates checking of one condition in each rule.<br>\n<br>\nMultiple validation rules can be defined for multiple columns. Succeeded and Failed Dataframes are created by applying all the validation rules on the incoming Dataframes.<br>",
      "examples": "<h2>Validate Fields Simple Node Examples</h2>\n<br>\nIncoming Dataframe has following rows:<br>\n<br>\nEMP_CD       |    EMP_NAME       |    DEPT       |    SALARY       |    AGE       |    DATE_OF_JOINING       |    EMAIL<br>\n------------------------------------------------------------------------------------------------------------------------------------<br>\nE01          |    ANTHONY        |    HR         |    50000        |    40        |    2020-12-12            |    anthony@hrdept.com<br>\nE02          |    LISA           |    HR         |    35000        |    35        |    2021-02-30            |    lisa@hrdept.com<br>\nE03          |    MARTIN         |    HR         |    20000        |    25        |    2020-01-01            |    martin@nodept.com<br>\nE04          |    DAVID          |    SALES      |    55000        |    40        |    2021-13-13            |    david@salesdept.com<br>\nE05          |    MARK           |    SALES      |    60000        |    27        |    2020-01-01            |    mark@salesdept.com<br>\nE06          |    JOE            |    SALES      |    40000        |    31        |    2010-01-01            |    joe@salesdept.com<br>\nE07          |    BELLA          |    HR         |    60000        |    24        |    2001-12-12            |    bella@hrdept.com<br>\n<br>\nif following Validation Rules are defined in Validation node:<br>\n<br>\nCOLUMNS           |    FUNCTION                 |    VALUES \t<br>\n--------------------------------------------------------------------------<br>\nDATE_OF_JOINING   |    IS_VALID_DATE_FORMAT     |    yyyy-MM-dd<br>\nAGE               |    VALUE_LESS_THAN          |    36<br>\n<br>\nthen outgoing Dataframes would be created as below:<br>\n<br>\n<h4> Succeeded Dataframe consisting of rows that succeeded Validation Rules:</h4>\n<br>\nEMP_CD       |    EMP_NAME       |    DEPT       |    SALARY       |    AGE       |    DATE_OF_JOINING       |    EMAIL<br>\n------------------------------------------------------------------------------------------------------------------------------------<br>\nE03          |    MARTIN         |    HR         |    20000        |    25        |    2020-01-01            |    martin@nodept.com<br>\nE05          |    MARK           |    SALES      |    60000        |    27        |    2020-01-01            |    mark@salesdept.com<br>\nE06          |    JOE            |    SALES      |    40000        |    31        |    2010-01-01            |    joe@salesdept.com<br>\nE07          |    BELLA          |    HR         |    60000        |    24        |    2001-12-12            |    bella@hrdept.com<br>\n<br>\n<h4> Failed Dataframe consisting of rows that failed Validation Rules:</h4>\n<br>\nEMP_CD       |    EMP_NAME       |    DEPT       |    SALARY       |    AGE       |    DATE_OF_JOINING       |    EMAIL<br>\n------------------------------------------------------------------------------------------------------------------------------------<br>\nE01          |    ANTHONY        |    HR         |    50000        |    40        |    2020-12-12            |    anthony@hrdept.com<br>\nE02          |    LISA           |    HR         |    35000        |    35        |    2021-02-30            |    lisa@hrdept.com<br>\nE04          |    DAVID          |    SALES      |    55000        |    40        |    2021-13-13            |    david@salesdept.com<br>",
      "type": "transform",
      "nodeClass": "fire.nodes.etl.NodeValidation",
      "x": "689px",
      "y": "115px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "description",
          "value": "",
          "widget": "textfield",
          "title": "Description",
          "description": "Validations being Performed",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "inputCols",
          "value": "[\"actual_balance\",\"balance_type_code\",\"balance_type_name\"]",
          "widget": "variables_list_select",
          "title": "Columns",
          "description": "Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "functions",
          "value": "[\"VALUE_LESS_THAN\",\"IS_NOT_NULL\",\"IS_NOT_NULL\"]",
          "widget": "variables_list_array",
          "title": "Function",
          "description": "Validation Function to apply",
          "optionsArray": [
            "LENGTH_LESS_THAN",
            "LENGTH_EQUALS",
            "LENGTH_GREATER_THAN",
            "VALUE_LESS_THAN",
            "VALUE_EQUALS",
            "VALUE_GREATER_THAN",
            "IS_NULL",
            "IS_NOT_NULL",
            "IS_VALID_EMAIL_ADDRESS",
            "IS_VALID_DATE_FORMAT"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "values",
          "value": "[\"2000\",\"\",\"\"]",
          "widget": "variables_list_textfield",
          "title": "Values",
          "description": "Values",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        }
      ],
      "engine": "scala"
    },
    {
      "id": "9",
      "name": "Write To Snowflake",
      "iconImage": "fa fa-tumblr-square",
      "description": "",
      "details": "<h2> Write To Snowflake Node Details</h2>\n<br>\nThis node saves the rows of the incoming dataframe into the specified table in Snowflake.<br>\n<br>\n<h4> Parameters to be set:</h4>\n<ul>\n<li> OUTPUT STORAGE LEVEL : Keep this as DEFAULT.</li>\n<li> CONNECTION : Select the desired snowflake connection to be used.</li>\n<li> SNOWFLAKE WAREHOUSE : Specify the virtual warehouse to use for the connection.</li>\n<li> SNOWFLAKE DATABASE : Specify the database to use once connected.</li>\n<li> SNOWFLAKE SCHEMA : Specify the schema to use for the specified database once connected.</li>\n<li> SNOWFLAKE TABLE : Specify the table from which data is to be read.</li>\n<li> SAVE MODE : Select the mode of operation on the table. </li>\n</ul>\nOverwrite: If table already exists, existing data is overwritten by the new content. <br>\nErrorIfExists: If data already exists, an exception is thrown and operation stops.<br>\nIgnore: If table already exists, the save operation is ignored.<br>",
      "examples": "<h2> Write To Snowflake Node Examples</h2>\n<br>\n<h4> The below example will save the input dataframe to the `CUST_BASIC_LA` table.</h4>\n<br>\n<ul>\n<li> CONNECTION : SNOWFLAKE_DEV_ENV_NCUS</li>\n<li> SNOWFLAKE WAREHOUSE : SNOWFLAKE_BI_VWH</li>\n<li> SNOWFLAKE DATABASE : CUSTOMER_SALES_NCUS</li>\n<li> SNOWFLAKE SCHEMA : INT_NA_CUSTSALES</li>\n<li> SNOWFLAKE TABLE : CUST_BASIC_LA</li>\n<li> SAVE MODE : Overwrite</li>\n</ul>",
      "type": "transform",
      "nodeClass": "fire.nodes.snowflake.NodeWriteToSnowFlake",
      "x": "868.161987px",
      "y": "107.165001px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "authType",
          "value": "UserCredential",
          "widget": "array",
          "title": "Auth Type",
          "description": "Authentication Type. Possible value is OAUTH or USER_CREDENTIAL",
          "optionsArray": [
            "UserCredential",
            "OAuth"
          ],
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "connection",
          "value": "1",
          "widget": "object_array",
          "title": "Connection",
          "description": "The Snowflake connection to connect",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "sfWarehouse",
          "value": "COMPUTE_WH",
          "widget": "textfield",
          "title": "Snowflake Warehouse",
          "description": "Warehouse for connecting to the Snowflake",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "sfDatabase",
          "value": "GBANK_SAMPLEDB",
          "widget": "textfield",
          "title": "Snowflake Database",
          "description": "Database for connecting to the Snowflake",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "sfSchema",
          "value": "LOAN_MANAGER_SCHEMA",
          "widget": "textfield",
          "title": "Snowflake Schema",
          "description": "Schema for connecting to the Snowflake",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "dbtable",
          "value": "PAYMENT_ALLOCATION1",
          "widget": "textfield",
          "title": "Snowflake Table",
          "description": "Snowflake Table from which to write the data",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        },
        {
          "name": "saveMode",
          "value": "Overwrite",
          "widget": "array",
          "title": "Save Mode",
          "description": "Whether to Append, Overwrite or Error if the table Exists",
          "optionsArray": [
            "Append",
            "Overwrite",
            "ErrorIfExists",
            "Ignore"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%"
        }
      ],
      "engine": "all"
    }
  ],
  "edges": [
    {
      "source": "1",
      "target": "4",
      "id": 1
    },
    {
      "source": "4",
      "target": "5",
      "id": 2
    },
    {
      "source": "5",
      "target": "6",
      "id": 3
    },
    {
      "source": "6",
      "target": "7",
      "id": 4
    },
    {
      "source": "7",
      "target": "9",
      "id": 5
    }
  ],
  "dataSetDetails": [
    {
      "id": 264,
      "uuid": "01974bac-e871-4566-9b7b-cda2590cd4b7",
      "header": true,
      "readOptions": "{\"mode\":\"PERMISSIVE\"}",
      "path": "/home/sparkflows/fire-data/data/BFSI/Credit-Card-Transaction/bank_payment_allocation.csv",
      "delimiter": ",",
      "datasetType": "CSV",
      "datasetSchema": "{\"colNames\":[\"card_number\",\"card_ref_number\",\"payment_transaction_id\",\"payment_amount\",\"balance_type_code\",\"balance_type_name\",\"actual_balance\",\"adjusted_amount\",\"remaining_balance\",\"additional_data_1\",\"additional_data_2\",\"additional_data_3\",\"additional_data_4\",\"additional_data_5\"],\"colTypes\":[\"DOUBLE\",\"STRING\",\"STRING\",\"DOUBLE\",\"STRING\",\"STRING\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\"],\"colFormats\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"],\"colMLTypes\":[\"NUMERIC\",\"TEXT\",\"TEXT\",\"NUMERIC\",\"TEXT\",\"TEXT\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\",\"TEXT\",\"TEXT\",\"TEXT\",\"TEXT\",\"TEXT\"]}"
    }
  ],
  "engine": "scala"
}