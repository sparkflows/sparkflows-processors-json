{
  "name": "01-Extract Table of Content",
  "uuid": "319ffa1d-f3eb-467a-9660-cfa132bd8ec5",
  "category": "PLPS-ICF Generator",
  "description": "-",
  "parameters": " --var textField1=CSP --var destinationPath1=/home/sparkflows/fire-data/data/GENAI/Regulatory-Navigator/CSP/uploads/Prot_000.pdf --var textField=CSP --var refresh=true --var view4=true --var folderPath=/home/sparkflows/fire-data/data/GENAI/Regulatory-Navigator/CSP/uploads/Prot_000.pdf --var folderPath3='/home/sparkflows/fire-data/data/GENAI/Regulatory-Navigator/CSP/uploads/Prot_000.pdf' --var run=true --var destinationPath=/data/GENAI/Regulatory-Navigator/CSP/uploads/ --var all=false --var generateIcf1=false --var generateIcf=false --var selectExecution=1 --var refresh1=false --var folderPath2=/data/GENAI/Regulatory-Navigator/CSP/output/PLPS --var folderPath1=/data/GENAI/Regulatory-Navigator/CSP/output/ICF/ --var checkbox2=false --var view2=false --var download=false",
  "nodes": [
    {
      "id": "1",
      "name": "TOC Extractor",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "453.5px",
      "y": "298.487px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext\nfrom bs4 import BeautifulSoup\nimport pymupdf\nimport pandas as pd\nimport re\nimport os\n\ndef save_list_to_txt(file_path1, string_list):\n    with open(file_path1, 'w') as file:\n        for item in string_list:\n            file.write(item + ', \\n')\n\ndef extract_all_text_between_headings(text, start_heading, end_heading):\n    # Regular expression to extract all text between headings\n    pattern = re.compile(\n        rf'{re.escape(start_heading)}(.*?){re.escape(end_heading)}',\n        re.DOTALL | re.IGNORECASE  # Case-insensitive matching\n    )\n    \n    # Find all matches\n    matches = pattern.findall(text)\n    # Clean up the matches (strip leading/trailing whitespace)\n    cleaned_matches = [match.strip() for match in matches]\n    \n    return cleaned_matches\n\ndef extract_text_from_pdf(pdf_path):\n    # Open the PDF file\n    document = pymupdf.open(pdf_path)\n    text = \"\"\n    # Iterate over each page in the PDF\n    for page_num in range(document.page_count):\n        # Get a page\n        page = document[page_num]\n        # Extract text from the page\n        text += page.get_text()\n    # Close the PDF file\n    document.close()\n    return text\ndef extract_outer_list_items(html_string):\n    # Parse the HTML string\n    soup = BeautifulSoup(html_string)\n    \n    # Find the outermost <ul> or <ol> tag\n    outer_list = soup.find(['ul', 'ol'])\n    \n    # Extract the direct child <li> elements and get their text (ignoring nested lists)\n    outer_list_items = [li.contents[0].strip() for li in outer_list.find_all('li', recursive=False)]\n    \n    return outer_list_items\n  \ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n  # Path to your PDF file\n  pdf_path = \"${folderPath}\"\n  # Extract text\n  pdf_text = extract_text_from_pdf(pdf_path)\n  \n  \n  pdf_path =\"/\".join(pdf_path.split(\"/\")[:-1]) + \"/\"\n  output_dir_path = \"/\".join(pdf_path.split(\"/\")[:-2]) + \"/output/TOC_output\"\n  print(\"111111111111\")\n  print(output_dir_path)\n  #output_latest_dir_path = \"/\".join(pdf_path.split(\"/\")[:-2]) + \"/Output-Latest\"\n  #temp_image = \"/\".join(pdf_path.split(\"/\")[:-2]) + \"/Temp-Image\"\n  #database_directory = \"/\".join(pdf_path.split(\"/\")[:-2]) + \"/Database\"\n\n  if not os.path.exists(output_dir_path):\n    os.makedirs(output_dir_path)\n  #if not os.path.exists(output_latest_dir_path):\n\t  #os.mkdir(output_latest_dir_path)\n  #if not os.path.exists(temp_image):  \n\t#  os.mkdir(temp_image)\n  #if not os.path.exists(database_directory): \n  #    os.mkdir(database_directory)\n  GOOGLE_API_KEY='$Gemini_API_reg' \n  import json\n  import requests\n  payload = {\n      \"contents\": [\n          {\n              \"parts\": [\n                  {\"text\": f\"\"\" You are a helpful assistant, \n                  from the given {pdf_text} text extracted from a PDF document, potentially containing various headings (H1, H2, H3, etc.) and sub-headings mixed with regular text. The task is to:\n                  1. Detect and extract all headings and sub-headings from the text.\n                  2. Automatically create a Table of Contents (TOC) in HTML format based on the heading hierarchy.\n                  3. If an existing TOC is present in the text, extract it and return it in HTML format.\n                  4. Ensure the TOC follows a structured hierarchy with nested lists, where each heading links to its respective section ID.\n                  **Guidelines:**\n                  - Detect headings based on formatting cues (e.g., capitalization, bold text indicators, or numbered sections).\n                  - Nest sub-headings correctly within the TOC.\n                  - Format the output as valid only HTML for web use.\n                  - Only give output of table of content no other explainations.\n                  - remove all links available in table of content and main heading such as Table of content should be in H1, and others headings and sub-heading starting from H2 and H3 and so on.. and maintain proper indentation.  \n                  \"\"\"}\n              ]\n          }\n      ],\n  }\n  # headers = {\n    # 'Content-Type': 'application/json'\n    # }\n  model1 = \"gemini-1.5-pro-exp-0801\"\n  model2 = \"gemini-1.5-flash\"\n  response = requests.post(f\"https://generativelanguage.googleapis.com/v1beta/models/{model2}:generateContent?key={GOOGLE_API_KEY}\", data=json.dumps(payload))\n  dta = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].replace(\"\\n\",\"\")\n  lines = dta.splitlines()\n  unique_lines = list(dict.fromkeys(lines))\n  cleaned_html_content = \"\\n\".join(unique_lines)\n  filenames = 'Table_of_content.txt' # .format(pd.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\"))\n  output_path = '${genaiHome}'+'/data/GENAI/Regulatory-Navigator/'+'${textField}'+'/output/TOC_output/'+filenames  # save_list_to_txt(output_path, tochtml)\n  with open(output_path, \"w\") as text_file:\n    text_file.write(cleaned_html_content)\n  workflowContext.outStr(3,\"Saved file at : \"+output_path,\"Saved Summaries\")\n  return",
          "widget": "textarea_large",
          "type": "python",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [],
  "dataSetDetails": [],
  "engine": "pyspark"
}