{
  "name": "03-PLPS",
  "uuid": "bdeada1a-b4d2-46d3-8430-1d2f670c1968",
  "category": "PLPS-ICF Generator",
  "parameters": " --var textField1=CSP --var destinationPath1=/home/sparkflows/fire-data/Spotline/CSP/uploads/Gilead_Study_Prot.pdf --var textField=CSP --var refresh=true --var view4=false --var folderPath=/home/sparkflows/fire-data/Spotline/CSP/uploads/Gilead_Study_Prot.pdf --var folderPath3='/home/sparkflows/fire-data/Spotline/CSP/uploads/Gilead_Study_Prot.pdf' --var destinationPath=/home/sparkflows/fire-data/Spotline/CSP/uploads/ --var all=true --var toc='TABLE OF CONTENTS','PROTOCOL SYNOPSIS','GLOSSARY OF ABBREVIATIONS AND DEFINITION OF TERMS','1. INTRODUCTION','2. OBJECTIVES','3. STUDY DESIGN','4. SUBJECT POPULATION','5. INVESTIGATIONAL MEDICINAL PRODUCTS (IMP)','6. STUDY PROCEDURES','7. ADVERSE EVENTS AND TOXICITY MANAGEMENT','8. STATISTICAL CONSIDERATIONS','9. RESPONSIBILITIES','10. REFERENCES','11. APPENDICES' --var generateIcf=false --var generateIcf1=true --var selectExecution=1 --var refresh1=false --var folderPath2=/home/sparkflows/fire-data/Spotline/CSP/output/PLPS --var folderPath1=/home/sparkflows/fire-data/Spotline/CSP/output/ICF/ --var checkbox2=false --var view2=false --var download=false",
  "nodes": [
    {
      "id": "1",
      "name": "PySpark",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "details": "",
      "examples": "",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "523px",
      "y": "300px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext\nfrom bs4 import BeautifulSoup\nimport pymupdf\nimport pandas as pd\nimport re\nfrom xhtml2pdf import pisa\nfrom io import BytesIO\n\ndef html_to_pdf(html_string, output_filename):\n    with BytesIO() as pdf_buffer:\n        pisa.CreatePDF(BytesIO(html_string.encode('utf-8')), dest=pdf_buffer)\n        pdf_buffer.seek(0)\n        with open(output_filename, 'wb') as output_file:\n            output_file.write(pdf_buffer.read())\n\n\ndef save_list_to_txt(file_path1, string_list):\n    with open(file_path1, 'w') as file:\n        for item in string_list:\n            file.write(item + ', \\n')\n\ndef extract_text_from_pdf(pdf_path):\n    # Open the PDF file\n    document = pymupdf.open(pdf_path)\n    text = \"\"\n    # Iterate over each page in the PDF\n    for page_num in range(document.page_count):\n        # Get a page\n        page = document[page_num]\n        # Extract text from the page\n        text += page.get_text()\n    # Close the PDF file\n    document.close()\n    return text\n  \ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n      # Path to your PDF file\n      # pdf_path = '/home/sparkflows/fire-data/Spotline/Clinical-Study-Protocols/Prot_000.pdf'\n    import glob\n    # mylist = [f for f in glob.glob(\"/home/sparkflows/fire-data/Spotline/CSP/uploads/*.pdf\")]\n    # pdf_path = \"${folderPath3}\"# mylist[0]\n      # Extract text\n    pdf_text = extract_text_from_pdf(\"${folderPath}\")\n    with open('${genaiHome}'+'/data/GENAI/Regulatory-Navigator/'+'${textField}'+'/output/TOC_output/Table_of_content.txt', 'r', encoding='utf-8') as file:\n        html_content = file.read()\n    h2_pattern = r'<h2[^>]*>(.*?)</h2>'\n    h2_tags = re.findall(h2_pattern, html_content, re.DOTALL)\n    List1 = []\n    for h2 in h2_tags:\n        List1.append(h2.strip())    \n    input_string = \"${toc}\"\n    # print(List1, \"List1\")\n    extracted_text = re.findall(r\"'(.*?)'\", input_string)\n    select_topic = extracted_text\n    # ['TRIAL SUMMARY','TRIAL DESIGN','OBJECTIVE(S) & HYPOTHESIS(ES)','BACKGROUND & RATIONALE','METHODOLOGY','TRIAL FLOW CHART','TRIAL PROCEDURES','TRIAL GOVERNANCE AND OVERSIGHT','STATISTICAL ANALYSIS PLAN','LABELING, PACKAGING, STORAGE AND RETURN OF CLINICAL SUPPLIES','ADMINISTRATIVE AND REGULATORY DETAILS','LIST OF REFERENCES','APPENDICES','SIGNATURES'] \n    # [\"${selectBoxes}\"]\n    #'METHODOLOGY', 'STATISTICAL ANALYSIS PLAN']\n    \n    \n    # Extract the last line after the final full stop\n    last_line = pdf_text.split('.')[-1].strip().split('\\n')[-1].strip()\n    \n    # Create the list of lists by matching the selected topics\n    output = []\n    for topic in select_topic:\n        if topic in List1:\n            index = List1.index(topic)\n            # Ensure there's a next element in the list\n            if index + 1 < len(List1):\n                output.append([List1[index], List1[index + 1]])\n            else:\n                # For the last item, add the extracted last line instead of the full sentence\n                output.append([List1[index], last_line])\n    \n    def extract_all_text_between_headings(text, start_heading, end_heading):\n        # Regular expression to extract all text between headings\n        pattern = re.compile(\n            rf'{re.escape(start_heading)}(.*?){re.escape(end_heading)}',\n            re.DOTALL | re.IGNORECASE  # Case-insensitive matching\n        )\n        \n        # Find all matches\n        matches = pattern.findall(text)\n        # Clean up the matches (strip leading/trailing whitespace)\n        cleaned_matches = [match.strip() for match in matches]\n        \n        return cleaned_matches\n    \n    selected_text = []\n    # Define your headings\n    for i in output:\n        start_heading = i[0]\n        end_heading = i[1]\n        \n        # Extract all occurrences of text between headings\n        all_texts = extract_all_text_between_headings(pdf_text, start_heading, end_heading)\n        \n        def get_max_length_sentence(sentences):\n            \"\"\"\n            Returns the longest sentence (by character length) from a list of sentences.\n            Parameters:\n            sentences (list): A list of sentences (strings).\n            Returns:\n            str: The sentence with the maximum length. Returns None if the list is empty.\n            \"\"\"\n            if not sentences:  # Check if the list is empty\n                return None  # Return None if the list is empty\n            # Find the sentence with the maximum length manually\n            max_sentence = sentences[0]\n            for sentence in sentences[1:]:\n                if len(sentence) > len(max_sentence):\n                    max_sentence = sentence\n            return max_sentence\n        \n        longest_sentence = get_max_length_sentence(all_texts)\n        selected_text.append(longest_sentence)\n    GOOGLE_API_KEY='$Gemini_API'\n    import json\n    import requests\n    payload = {\n            \"contents\": [\n                {\n                    \"parts\": [\n                        {\"text\": f\"\"\" You are a helpful assistant,\n                        Create a detailed Plain Language Protocol Synopsis (PLPS) that summarizes the clinical trial described in the attached document. The synopsis should be accessible for non-expert audiences and aim for a comprehensive explanation across approximately 2000 words. Structure the summary to cover the following areas, with clear and visually appealing HTML formatting:\n        \n        Introduction and Background (200-400 words):\n        \n        Use <h1> or <h2> tags for the main headings and subheadings.\n        Write the introduction using <p> tags for each paragraph.\n        Provide context with bullet points or ordered lists using <ul> or <ol> tags for key points and background information.\n        Study Objectives (200-300 words):\n        \n        Use subheadings for main objectives and explain each in separate paragraphs.\n        Break down key objectives into bullet points for clarity.\n        Study Design (300-500 words):\n        \n        Use descriptive headings and divide content into easily readable sections.\n        Include lists or tables for different phases of the study or participant groups if needed.\n        Study Population (300-500 words):\n        \n        Use descriptive subheadngs and break down the criteria using lists.\n        Include details on participant demographics in bullet points.\n        Intervention (200-400 words):\n        \n        Use paragraph tags for detailed descriptions of interventions.\n        Highlight key information like dosage and duration in bullet points or tables.\n        Study Procedures (300-500 words):\n        \n        Use subheadings to describe each step and process.\n        Use ordered lists to walk readers through sequential procedures.\n        Risks and Benefits (200-400 words):\n        \n        Use subheadings and break content into manageable sections.\n        Display potential risks and benefits using lists or tables for easy comparison.\n        Data Collection and Analysis (200-400 words):\n        \n        Use clear paragraph structure and subheadings for data collection and analysis methods.\n        Consider tables to summarize statistical methods or types of data collected.\n        Participant Rights and Safety (200-400 words):\n        \n        Use heading tags and structure content in a visually clear manner.\n        Include lists to highlight participant rights and safety measures.\n        Contact Information and Support (100-200 words):\n        \n        Provide contact details using list tags.\n        Highlight key information with inline styles like <strong> or <em> where appropriate.\n        For the overall layout, use:\n        \n        <h1>, <h2>, <h3> for headings.\n        <p> for paragraphs.\n        <ul>, <ol> for bullet or numbered lists.\n        <table>, <tr>, <td> for any tabular data if needed.\n        Inline CSS for basic styling if possible, ensuring good spacing and readability.\n        Ensure the language is simple, clear, and accessible for a general audience, while maintaining visually appealing formatting through appropriate use of HTML.\n                         from the given {selected_text} \"\"\"}\n                    ]\n                }\n            ],\n        }\n        \n    model1 = \"gemini-1.5-pro-exp-0801\"\n    model2 = \"gemini-1.5-flash\"\n    response = requests.post(f\"https://generativelanguage.googleapis.com/v1beta/models/{model2}:generateContent?key={GOOGLE_API_KEY}\", data=json.dumps(payload))\n    ahtml = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].replace(\"\\n\",\"\")\n    filenames = \"PLPS_\"+\"${folderPath}\".split(\"/\")[-1] # .format(pd.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\"))\n    output_path = '${genaiHome}'+'/data/GENAI/Regulatory-Navigator/'+'${textField}'+'/output/PLPS/'+filenames\n    html_to_pdf(ahtml, output_path)\n    output_path1 = '${genaiHome}'+'/data/GENAI/Regulatory-Navigator/'+'${textField}'+'/output/PLPS_Txt_Output/PLPS_Data.txt'\n    with open(output_path1, \"w\") as text_file:\n      text_file.write(ahtml)\n    workflowContext.outStr(3,\"Saved file at : \"+output_path1,\"Saved Summaries\")\n    return",
          "widget": "textarea_large",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "Schema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [],
  "dataSetDetails": [],
  "engine": "pyspark"
}