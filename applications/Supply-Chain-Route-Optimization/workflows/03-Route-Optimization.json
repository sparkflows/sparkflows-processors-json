{
  "name": "03-Route-Optimization",
  "uuid": "e31573a2-8fbe-4f82-9214-eee09b9d3902",
  "category": "Route Optimization",
  "description": "-",
  "parameters": " --var destinationPath=projects-customers/MANUFACTURING/Supply-Chain-Route-Optimization/Upload/Weather/ --var uploadweather=false --var destinationPath1=projects-customers/MANUFACTURING/Supply-Chain-Route-Optimization/Upload/Market/ --var uploadmarket=false --var processData=false --var start_location=Mumbai --var end_location=Dubai --var month=2 --var selectProduct=mobile --var exploreData=true",
  "nodes": [
    {
      "id": "1",
      "name": "Read CSV",
      "iconImage": "/images/icons/node-icon/csv.svg",
      "description": "It reads in CSV files and creates a DataFrame from it.",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetCSV",
      "x": "21.7778px",
      "y": "220.778px",
      "hint": "Infer the schema when a new file is selected or the file content has changed",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "path",
          "value": "data/MANUFACTURING/Supply-Chain-Route-Optimization/Output/Weather-Processed-Data/",
          "widget": "textfield",
          "title": "Path",
          "description": "Path of the file/directory",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "separator",
          "value": ",",
          "widget": "textfield",
          "title": "Separator",
          "description": "CSV Separator",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "header",
          "value": "true",
          "widget": "array",
          "title": "Header",
          "description": "Whether the file has a header row",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "dropSpecialCharacterInColumnName",
          "value": "false",
          "widget": "array",
          "title": "Drop Special Character In ColumnName",
          "description": "Whether to drop the Special Characters and Spaces in Column Name.",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "mode",
          "value": "PERMISSIVE",
          "widget": "array",
          "title": "Mode",
          "description": "Mode for dealing with corrupt records during parsing.",
          "optionsArray": [
            "PERMISSIVE",
            "DROPMALFORMED",
            "FAILFAST"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "enforceSchema",
          "value": "false",
          "widget": "array",
          "title": "Enforce Schema",
          "description": "If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to false, the schema will be validated against all headers in CSV files in the case when the header option is set to true.",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "addInputFileName",
          "value": "false",
          "widget": "array",
          "title": "Whether to Add Input File Name as Column in the Dataframe",
          "description": "Add the new field:input_file_name",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "encoding",
          "value": "UTF-8",
          "widget": "textfield",
          "title": "Encoding",
          "description": "Decodes the CSV files by the given encoding type",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "quote",
          "value": "\"",
          "widget": "textfield",
          "title": "Quote",
          "description": "Sets a single character used for escaping quoted values where the separator can be part of the value",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "escape",
          "value": "\\",
          "widget": "textfield",
          "title": "Escape",
          "description": "Sets a single character used for escaping quotes inside an already quoted value.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[\"Location\",\"date_new_year\",\"date_new_month\",\"Weather_Risk\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Columns from CSV",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"INTEGER\",\"INTEGER\",\"DOUBLE\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "properties",
          "value": "",
          "widget": "tab",
          "title": "Properties",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "extraOptionsKeys",
          "value": "[]",
          "widget": "key_array",
          "title": "Properties Name",
          "description": "Extra options/properites available while executing in Read CSV.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "extraOptionsValues",
          "value": "[]",
          "widget": "value_array",
          "title": "Properties Value",
          "description": "Config Values for the Corresponding properites name",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "2",
      "name": "Row Filter",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node creates a new DataFrame containing the rows that satisfy the given condition",
      "type": "transform",
      "nodeClass": "fire.nodes.etl.NodeRowFilter",
      "x": "160.667px",
      "y": "213.667px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "conditionExpr",
          "value": "date_new_month = ${month}",
          "widget": "code_editor",
          "title": "Conditional Expression",
          "description": "The filtering condition. Rows not satisfying given condition will be excluded from output DataFrame. eg: usd_pledged_real > 0 and (category = 1 or category == 2) and goal > 100",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "3",
      "name": "Dataset Structured",
      "iconImage": "fa fa-th-list",
      "description": "This Node creates a DataFrame by reading data from HDFS, HIVE etc. The dataset was defined earlier in Fire by using the Dataset Feature. As a user, you just have to select the Dataset of your interest.",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetStructured",
      "x": "31.5278px",
      "y": "380.556px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "dataset",
          "value": "87314785-05e3-4d74-949e-3796032a4743",
          "widget": "dataset",
          "title": "Dataset",
          "description": "Selected Dataset",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "4",
      "name": "Join Using SQL",
      "iconImage": "fa fa-stumbleupon",
      "description": "This node registers the incoming DataFrames as temporary tables and executes the SQL provided",
      "type": "join",
      "nodeClass": "fire.nodes.etl.NodeJoinUsingSQL",
      "x": "199.667px",
      "y": "374.667px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "tempTables",
          "value": "[\"tempTable1\",\"tempTable2\"]",
          "widget": "array_of_values",
          "title": "Temp Table Names",
          "description": "Temp Table Name to be used",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "sql",
          "value": "select t1.*, t2.Weather_Risk as start_weather_risk from tempTable1 t1 join tempTable2 t2 on t1.Start_Location = t2.Location",
          "widget": "code_editor",
          "title": "SQL",
          "description": "SQL to be run",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[\"Start_Location\",\"End_Location\",\"Distance_km\",\"Travel_Time_hr\",\"Route_Cost\",\"start_weather_risk\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"STRING\",\"INTEGER\",\"DOUBLE\",\"INTEGER\",\"DOUBLE\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "5",
      "name": "Join Using SQL",
      "iconImage": "fa fa-stumbleupon",
      "description": "This node registers the incoming DataFrames as temporary tables and executes the SQL provided",
      "type": "join",
      "nodeClass": "fire.nodes.etl.NodeJoinUsingSQL",
      "x": "357.667px",
      "y": "287.667px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "tempTables",
          "value": "[\"tempTable1\",\"tempTable2\"]",
          "widget": "array_of_values",
          "title": "Temp Table Names",
          "description": "Temp Table Name to be used",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "sql",
          "value": "select t1.*, t2.Weather_Risk as end_weather_risk from tempTable1 t1 join tempTable2 t2 on t1.End_Location = t2.Location",
          "widget": "code_editor",
          "title": "SQL",
          "description": "SQL to be run",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[\"Start_Location\",\"End_Location\",\"Distance_km\",\"Travel_Time_hr\",\"Route_Cost\",\"Location\",\"date_new_year\",\"date_new_month\",\"Weather_Risk\",\"start_weather_risk\",\"end_weather_risk\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"STRING\",\"INTEGER\",\"DOUBLE\",\"INTEGER\",\"STRING\",\"INTEGER\",\"INTEGER\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "6",
      "name": "Select Columns",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node creates a new DataFrame that contains only the selected columns",
      "type": "transform",
      "nodeClass": "fire.nodes.etl.NodeColumnFilter",
      "x": "473.778px",
      "y": "283.778px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputCols",
          "value": "[\"Start_Location\",\"End_Location\",\"Distance_km\",\"Travel_Time_hr\",\"Route_Cost\",\"start_weather_risk\",\"end_weather_risk\"]",
          "widget": "variables",
          "title": "Columns",
          "description": "Columns to be included in the output DataFrame",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "8",
      "name": "Read CSV",
      "iconImage": "/images/icons/node-icon/csv.svg",
      "description": "It reads in CSV files and creates a DataFrame from it.",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetCSV",
      "x": "565.208px",
      "y": "99.2222px",
      "hint": "Infer the schema when a new file is selected or the file content has changed",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "path",
          "value": "data/MANUFACTURING/Supply-Chain-Route-Optimization/Output/Market-Risk-Processed-Data/",
          "widget": "textfield",
          "title": "Path",
          "description": "Path of the file/directory",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "separator",
          "value": ",",
          "widget": "textfield",
          "title": "Separator",
          "description": "CSV Separator",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "header",
          "value": "true",
          "widget": "array",
          "title": "Header",
          "description": "Whether the file has a header row",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "dropSpecialCharacterInColumnName",
          "value": "false",
          "widget": "array",
          "title": "Drop Special Character In ColumnName",
          "description": "Whether to drop the Special Characters and Spaces in Column Name.",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "mode",
          "value": "PERMISSIVE",
          "widget": "array",
          "title": "Mode",
          "description": "Mode for dealing with corrupt records during parsing.",
          "optionsArray": [
            "PERMISSIVE",
            "DROPMALFORMED",
            "FAILFAST"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "enforceSchema",
          "value": "false",
          "widget": "array",
          "title": "Enforce Schema",
          "description": "If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to false, the schema will be validated against all headers in CSV files in the case when the header option is set to true.",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "addInputFileName",
          "value": "false",
          "widget": "array",
          "title": "Whether to Add Input File Name as Column in the Dataframe",
          "description": "Add the new field:input_file_name",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "encoding",
          "value": "UTF-8",
          "widget": "textfield",
          "title": "Encoding",
          "description": "Decodes the CSV files by the given encoding type",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "quote",
          "value": "\"",
          "widget": "textfield",
          "title": "Quote",
          "description": "Sets a single character used for escaping quoted values where the separator can be part of the value",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "escape",
          "value": "\\",
          "widget": "textfield",
          "title": "Escape",
          "description": "Sets a single character used for escaping quotes inside an already quoted value.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[\"Location\",\"market_risk\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Columns from CSV",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"DOUBLE\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "properties",
          "value": "",
          "widget": "tab",
          "title": "Properties",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "extraOptionsKeys",
          "value": "[]",
          "widget": "key_array",
          "title": "Properties Name",
          "description": "Extra options/properites available while executing in Read CSV.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "extraOptionsValues",
          "value": "[]",
          "widget": "value_array",
          "title": "Properties Value",
          "description": "Config Values for the Corresponding properites name",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "9",
      "name": "Join Using SQL",
      "iconImage": "fa fa-stumbleupon",
      "description": "This node registers the incoming DataFrames as temporary tables and executes the SQL provided",
      "type": "join",
      "nodeClass": "fire.nodes.etl.NodeJoinUsingSQL",
      "x": "602.555px",
      "y": "273.542px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "tempTables",
          "value": "[\"tempTable1\",\"tempTable2\"]",
          "widget": "array_of_values",
          "title": "Temp Table Names",
          "description": "Temp Table Name to be used",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "sql",
          "value": "select t1.*, t2.market_risk as start_market_risk from tempTable1 t1 join tempTable2 t2 on t1.Start_Location = t2.Location",
          "widget": "code_editor",
          "title": "SQL",
          "description": "SQL to be run",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[\"Start_Location\",\"End_Location\",\"Distance_km\",\"Travel_Time_hr\",\"Route_Cost\",\"start_weather_risk\",\"end_weather_risk\",\"start_market_risk\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"STRING\",\"INTEGER\",\"DOUBLE\",\"INTEGER\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "10",
      "name": "Join Using SQL",
      "iconImage": "fa fa-stumbleupon",
      "description": "This node registers the incoming DataFrames as temporary tables and executes the SQL provided",
      "type": "join",
      "nodeClass": "fire.nodes.etl.NodeJoinUsingSQL",
      "x": "728.667px",
      "y": "205.667px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "tempTables",
          "value": "[\"tempTable1\",\"tempTable2\"]",
          "widget": "array_of_values",
          "title": "Temp Table Names",
          "description": "Temp Table Name to be used",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "sql",
          "value": "select t1.*, t2.market_risk as end_market_risk from tempTable1 t1 join tempTable2 t2 on t1.End_Location = t2.Location",
          "widget": "code_editor",
          "title": "SQL",
          "description": "SQL to be run",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[\"Start_Location\",\"End_Location\",\"Distance_km\",\"Travel_Time_hr\",\"Route_Cost\",\"start_weather_risk\",\"end_weather_risk\",\"start_market_risk\",\"end_market_risk\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"STRING\",\"INTEGER\",\"DOUBLE\",\"INTEGER\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "12",
      "name": "PySpark",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node runs any given PySpark code. The input dataframe is passed into the function myfn as a parameter.",
      "type": "pyspark",
      "nodeClass": "fire.nodes.etl.NodePySpark",
      "x": "852.667px",
      "y": "105.667px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDF: input pyspark dataframe \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \nimport pandas as pd\nfrom pulp import LpProblem, LpVariable, lpSum, LpMinimize\n\ndef find_routes_with_reasons(routes_df, start_location, end_location):\n    # Create a route graph as a dictionary\n    routes_graph = {}\n    \n    for _, row in routes_df.iterrows():\n        start = row['Start_Location']\n        end = row['End_Location']\n        routes_graph[(start, end)] = {\n            'Distance_km': row['Distance_km'],\n            'Travel_Time_hr': row['Travel_Time_hr'],\n            'Route_Cost': row['Route_Cost'],\n            'Weather_Risk': row['start_weather_risk'] + row['end_weather_risk'],\n            'Market_Risk': row['start_market_risk'] + row['end_market_risk']\n        }\n\n    # Create the optimization problem\n    prob = LpProblem(\"Route_Optimization\", LpMinimize)\n\n    # Decision variables for routes\n    route_vars = LpVariable.dicts(\"Route\", \n                                   [(start, end) for (start, end) in routes_graph.keys()], \n                                   lowBound=0, upBound=1, cat='Binary')\n\n    # Objective Function: Minimize Total Cost\n    prob += lpSum(routes_graph[(start, end)]['Route_Cost'] * route_vars[(start, end)] \n                   for (start, end) in route_vars.keys()), \"Total_Cost\"\n\n    # Constraints: Ensure that we start from the start_location\n    prob += lpSum(route_vars[(start_location, end)] for end in routes_df['End_Location'] \n                   if (start_location, end) in route_vars) >= 1, \"Start_Constraint\"\n\n    # Ensure we can reach the end_location\n    prob += lpSum(route_vars[(start, end_location)] for start in routes_df['Start_Location'] \n                   if (start, end_location) in route_vars) >= 1, \"End_Constraint\"\n\n    # Flow conservation for all intermediate locations\n    all_locations = set(routes_df['Start_Location']).union(set(routes_df['End_Location']))\n    for location in all_locations:\n        if location != start_location and location != end_location:\n            prob += lpSum(route_vars[(start, location)] for start in all_locations \n                           if (start, location) in route_vars) \\\n                    - lpSum(route_vars[(location, end)] for end in all_locations \n                             if (location, end) in route_vars) == 0, f\"Flow_Constraint_{location}\"\n\n    # Solve the optimization problem\n    prob.solve()\n\n    # Collect the optimized route\n    optimized_route = []\n    current_location = start_location\n    while current_location != end_location:\n        for (start, end) in route_vars.keys():\n            if start == current_location and route_vars[(start, end)].varValue == 1:\n                optimized_route.append((start, end))\n                current_location = end\n                break\n\n    # Collect all possible routes between the start and end location\n    alternative_routes = []\n    reasons = []\n    \n    # Function to find valid paths from start to end\n    def find_paths(current_location, end_location, path):\n        if current_location == end_location:\n            return [path]\n        \n        paths = []\n        for (start, end) in routes_graph.keys():\n            if start == current_location:\n                # Recurse to find paths from the next location\n                new_paths = find_paths(end, end_location, path + [end])\n                paths.extend(new_paths)\n        \n        return paths\n\n    # Get distinct alternative routes\n    distinct_paths = find_paths(start_location, end_location, [start_location])\n    print(\"distinct path\")\n    print(distinct_paths)\n    COST_THRESHOLD = 0.1  # 10%\n    DISTANCE_THRESHOLD = 0.1  # 10%\n    MARKET_RISK_THRESHOLD = 0.08  # 10%\n    WEATHER_RISK_THRESHOLD = 0.08 \n    \n    for path in distinct_paths:\n        route_segments = []\n        for i in range(len(path) - 1):\n            route_segments.append((path[i], path[i + 1]))\n        \n        route_costs = []\n        reasons_for_path = []\n        \n        # Initialize minimum values for comparisons\n        min_route_cost = float('inf')\n        min_distance = float('inf')\n        min_market_risk = float('inf')\n        min_weather_risk = float('inf')\n\n        # Calculate minimums first\n        for segment in route_segments:\n            min_route_cost = min_route_cost if min_route_cost < routes_graph[segment]['Route_Cost'] else routes_graph[segment]['Route_Cost']\n            min_distance = min_distance if  min_distance < routes_graph[segment]['Distance_km'] else routes_graph[segment]['Distance_km']\n            min_market_risk = min_market_risk if  min_market_risk < routes_graph[segment]['Market_Risk'] else routes_graph[segment]['Market_Risk']\n            min_weather_risk = min_weather_risk if  min_weather_risk < routes_graph[segment]['Weather_Risk'] else routes_graph[segment]['Weather_Risk']\n        print(min_route_cost,min_distance,min_market_risk,min_weather_risk)\n        for segment in route_segments:\n            route_cost = routes_graph[segment]['Route_Cost']\n            route_costs.append(route_cost)\n            if segment in optimized_route:\n                continue\n            else:\n                print(1)\n                # Use pre-calculated minimums for comparisons\n                # Compare each metric to the minimums with thresholds\n                if route_cost > min_route_cost * (1 + COST_THRESHOLD):\n                    reasons_for_path.append(\"Higher Cost\")\n                if routes_graph[segment]['Distance_km'] > min_distance * (1 + DISTANCE_THRESHOLD):\n                    reasons_for_path.append(\"Longer Distance\")\n                if routes_graph[segment]['Market_Risk'] > min_market_risk * (1 + MARKET_RISK_THRESHOLD):\n                    reasons_for_path.append(\"Higher Market Risk\")\n                if routes_graph[segment]['Weather_Risk'] > min_weather_risk * (1 + WEATHER_RISK_THRESHOLD):\n                    reasons_for_path.append(\"Higher Weather Risk\")\n                print(2)\n\n\n        # Handle case where there are no reasons found\n        if not reasons_for_path:\n            reasons_for_path.append(\"No significant difference\")\n\n        # Add the current path's segments and reasons\n        alternative_routes.append(route_segments)\n        reasons.append(reasons_for_path)\n\n    return optimized_route, alternative_routes, reasons\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDF: DataFrame, cust_dict:dict):\n    print(\"=============================== Started =========================\")\n    routes_df = inDF.toPandas()  #add custom logic\n    start_location = \"${start_location}\"\n    end_location = \"${end_location}\"\n    if start_location == \"New_York\":\n      start_location = \"New York\"\n    if end_location == \"New_York\":\n      end_location = \"New York\"\n    print(\"====================\",start_location, end_location,\"===========================\")\n    optimized_route, alternative_routes, reasons = find_routes_with_reasons(routes_df, start_location, end_location)\n    print(\"==================== Optimization Ended ============================\")\n    print(\"Optimized Route:\", optimized_route)\n    print(\"Alternative Routes:\", alternative_routes)\n    for route, reason in zip(alternative_routes, reasons):\n        print(f\"Route: {route}, Reasons: {reason}\")\n    routes = [optimized_route] + alternative_routes\n    print(routes)\n    types = [\"Optimized\"] + [\"Alternate\"]* len(alternative_routes)\n    print(types)\n    print(reasons)\n    print(len(routes),len(types),len(reasons))\n    # Format the routes to use full location names\n    formatted_routes = [\" -> \".join([pair[0] for pair in route] + [route[-1][1]]) for route in routes]\n\n    # Remove duplicates in reasons and join them into strings\n    formatted_reasons = [\"Optimized\"] +[\"; \".join(set(reason)) for reason in reasons]\n\n    # Create DataFrame with Type as the first column\n    df = pd.DataFrame({\n        'Type': types,\n        'Route': formatted_routes,\n        'Reason': formatted_reasons\n    })\n    outDF = spark.createDataFrame(df)\n    return outDF",
          "widget": "textarea_large",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[\"Type\",\"Route\",\"Reason\"]",
          "widget": "schema_col_names",
          "title": "Column Names",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"STRING\",\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    },
    {
      "id": "13",
      "name": "Print N Rows",
      "iconImage": "fa fa-tumblr-square",
      "description": "Prints the specified number of records in the DataFrame. It is useful for seeing intermediate output",
      "type": "transform",
      "nodeClass": "fire.nodes.util.NodePrintFirstNRows",
      "x": "1136.44px",
      "y": "176.444px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "title",
          "value": "Routes",
          "widget": "textfield",
          "title": "Title",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "n",
          "value": "10",
          "widget": "textfield",
          "title": "Num Rows to Print",
          "description": "number of rows to be printed",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "displayDataType",
          "value": "true",
          "widget": "array",
          "title": "Display Data Type",
          "description": "If true display rows DataType",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "14",
      "name": "Row Filter",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node creates a new DataFrame containing the rows that satisfy the given condition",
      "type": "transform",
      "nodeClass": "fire.nodes.etl.NodeRowFilter",
      "x": "1010.56px",
      "y": "180.542px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "conditionExpr",
          "value": "Reason != \"No significant difference\"",
          "widget": "code_editor",
          "title": "Conditional Expression",
          "description": "The filtering condition. Rows not satisfying given condition will be excluded from output DataFrame. eg: usd_pledged_real > 0 and (category = 1 or category == 2) and goal > 100",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "15",
      "name": "MultiInputPySpark",
      "iconImage": "fa fa-tumblr-square",
      "description": "This node runs any given PySpark code. The input dataframe is passed in the variable inDFs. The output dataframe is passed back by registering it as a temporary table.",
      "type": "pyspark2inputs",
      "nodeClass": "fire.nodes.code.NodeMultiInputPySpark",
      "x": "787.667px",
      "y": "392.667px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDFs: array of input pyspark dataframes \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport os\nimport base64\n\n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDFs: [DataFrame], cust_dict:dict):\n  df_routes = inDFs[1].toPandas()\n  df_optimized = inDFs[0].toPandas()\n  \n  # Create a directed graph from all nodes and distances\n  G = nx.DiGraph()\n\n  # Add nodes for each location\n  all_locations = set(df_routes['Start_Location']).union(df_routes['End_Location'])\n  G.add_nodes_from(all_locations)\n\n  # Add only optimized path connections\n  optimized_path = df_optimized[df_optimized['Type'] == 'Optimized']['Route'].values[0].split(\" -> \")\n  for i in range(len(optimized_path) - 1):\n      start, end = optimized_path[i], optimized_path[i + 1]\n      distance = df_routes[(df_routes['Start_Location'] == start) & (df_routes['End_Location'] == end)]['Distance_km'].values[0]\n      G.add_edge(start, end, weight=distance)\n\n  # Position nodes in a circular layout\n  positions = nx.circular_layout(G)\n\n  # Draw nodes and labels\n  nx.draw_networkx_nodes(G, pos=positions, node_size=500, node_color=\"skyblue\")\n  nx.draw_networkx_labels(G, pos=positions, font_size=5, font_weight=\"bold\")\n\n  # Draw only the optimized path edges with labels\n  optimized_edges = [(optimized_path[i], optimized_path[i + 1]) for i in range(len(optimized_path) - 1)]\n  nx.draw_networkx_edges(G, pos=positions, edgelist=optimized_edges, edge_color=\"red\", width=2)\n  edge_labels = {(start, end): f\"{data['weight']*100} km\" for start, end, data in G.edges(data=True)}\n  nx.draw_networkx_edge_labels(G, pos=positions, edge_labels=edge_labels, font_color=\"darkgreen\")\n\n  # Directory and file setup\n  output_dir = \"/home/sparkflows/fire-data/data/Supply_Chain_Route_Optimization/graph_image/\"\n  output_file = os.path.join(output_dir, \"optimized_route_plot.png\")\n\n  # Check if directory exists, if not, create it\n  if not os.path.exists(output_dir):\n      os.makedirs(output_dir)\n      \n  # Save the plot as an image\n  plt.title(\"Optimized Route Visualization\")\n  plt.axis(\"off\")\n  plt.savefig(output_file, format=\"png\", dpi=300)  # Save as PNG file with 300 DPI for quality\n  plt.close()  # Close the plot to free memory\n  with open(output_file, 'rb') as image_file:\n     binary_image = image_file.read()\n  base64_image = base64.b64encode(binary_image).decode('utf-8')\n  print(7)\n  desired_width = 1000  # Width for medium display\n  desired_height = 650  # Height for medium display\n  html_template = f'''\n<html>\n<head>\n    <title>Generated Chart</title>\n</head>\n\n<body>\n    <div style=\"display: flex; align-items: flex-start;\">\n        <img src=\"data:image/png;base64,{base64_image}\" alt=\"Generated Chart\" style=\"width:{desired_width}px; height:{desired_height}px; margin-right: 20px;\">\n    </div>\n</body>\n</html>\n\t'''\n  workflowContext.outHTML(9, title=\"Generated chart\", text = html_template)\n  return",
          "widget": "textarea_large",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    },
    {
      "id": "16",
      "name": "MultiInputPySpark",
      "description": "This node runs any given PySpark code. The input dataframe is passed in the variable inDFs. The output dataframe is passed back by registering it as a temporary table.",
      "type": "pyspark2inputs",
      "nodeClass": "fire.nodes.code.NodeMultiInputPySpark",
      "x": "1010.49px",
      "y": "396.044px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "code",
          "value": "'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDFs: array of input pyspark dataframes \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \nimport requests\nimport json\n\nGOOGLE_API_KEY=\"$Gemini_Api\"\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDFs: [DataFrame], cust_dict:dict):\n  route_df = inDFs[0].toPandas()\n  optimized_df = inDFs[1].toPandas()  #get the first dataframe\n  \n  payload = {\n      \"contents\": [\n          {\n              \"parts\": [\n                  {\"text\": f\"\"\"I am providing the below Route dataframe and Optimized Path dataframe\nRoute dataframe\n{route_df.reset_index(drop=True).to_string()}\n\nOptimization Path and Alternate path dataframe\n{optimized_df.reset_index(drop=True).to_string()}\n\n\nNow from the above table can you please give me a detailed report based on below points\n\n1. Which one is the optimized path \n2. Expand on the optimized path,reasons for choosing it  and give a detailed analysis\n3. Which are the Alternate Paths (Give the reason for each. try to make a table in proper html format with border)\n4. Why the Alternate paths are not taken as the best path\n5.have propper spacing and do not include unnecassary data or reponse that is not needed\n\n\nTry to Give a detailed report with suitable Title and Sub Title (use black coilor for Title and use in tag styling) and also use you inteeligence to extract more information from the data.\nPlease generate in HTML format.\nDo not use any markdown symbols. Like do not use '#' to increase the size.. similarly do not use '**' to bold. Use <strong> tag for bold. Use in tag style to increase the font size for title.\n\"\"\"}\n              ]\n          }\n      ]\n  }\n  headers = {\n      'Content-Type': 'application/json'\n  }\n  model1 = \"gemini-1.5-pro-exp-0801\"\n  model2 = \"gemini-1.5-flash\"\n  response = requests.post(f\"https://generativelanguage.googleapis.com/v1beta/models/{model2}:generateContent?key={GOOGLE_API_KEY}\", headers=headers, data=json.dumps(payload))\n  print(response.json())\n  html_text = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n  if \"```\" in html_text:\n    html_text = html_text.replace(\"```\",\"\")[4:]\n  print(html_text)\n  workflowContext.outHTML(id, title=\"Report\", text = html_text)\n  return",
          "widget": "textarea_large",
          "title": "PySpark",
          "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "InferSchema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColNames",
          "value": "[]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColTypes",
          "value": "[]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputColFormats",
          "value": "[]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [
    {
      "source": "1",
      "target": "2",
      "id": 1
    },
    {
      "source": "3",
      "target": "4",
      "id": 2
    },
    {
      "source": "2",
      "target": "4",
      "id": 3
    },
    {
      "source": "4",
      "target": "5",
      "id": 4
    },
    {
      "source": "2",
      "target": "5",
      "id": 5
    },
    {
      "source": "5",
      "target": "6",
      "id": 6
    },
    {
      "source": "6",
      "target": "9",
      "id": 7
    },
    {
      "source": "8",
      "target": "9",
      "id": 8
    },
    {
      "source": "9",
      "target": "10",
      "id": 9
    },
    {
      "source": "8",
      "target": "10",
      "id": 10
    },
    {
      "source": "10",
      "target": "12",
      "id": 11
    },
    {
      "source": "12",
      "target": "14",
      "id": 12
    },
    {
      "source": "14",
      "target": "13",
      "id": 13
    },
    {
      "source": "14",
      "target": "15",
      "id": 14
    },
    {
      "source": "10",
      "target": "15",
      "id": 15
    },
    {
      "source": "10",
      "target": "16",
      "id": 16
    },
    {
      "source": "14",
      "target": "16",
      "id": 17
    }
  ],
  "dataSetDetails": [
    {
      "id": 2175,
      "uuid": "87314785-05e3-4d74-949e-3796032a4743",
      "header": true,
      "path": "data/MANUFACTURING/Supply-Chain-Route-Optimization/Raw-Data/Routes-Data.csv",
      "delimiter": ",",
      "datasetType": "CSV",
      "datasetSchema": "{\"colNames\":[\"Start_Location\",\"End_Location\",\"Distance_km\",\"Travel_Time_hr\",\"Route_Cost\"],\"colTypes\":[\"STRING\",\"STRING\",\"INTEGER\",\"DOUBLE\",\"INTEGER\"],\"colFormats\":[\"\",\"\",\"\",\"\",\"\"],\"colMLTypes\":[\"TEXT\",\"TEXT\",\"NUMERIC\",\"NUMERIC\",\"NUMERIC\"]}"
    }
  ],
  "engine": "pyspark"
}