{
  "name": "QA_TestCases_Generator",
  "uuid": "043f28dd-864a-4fc1-8bdf-8dfb72ccb853",
  "category": "-",
  "description": "-",
  "parameters": " --var getStarted=true --var destinationPath=/data/GENAI/Agentic-App-Dataset/Uploads --var execute=true --var goBack=false",
  "nodes": [
    {
      "id": "1",
      "name": "Document To Text",
      "iconImage": "/images/icons/node-icon/PDF.svg",
      "description": "The DocumentToText node extracts text content from documents, including PDF, TXT, DOCX, and image files, located in a specified file path or directory. It processes either a single file or an entire directory, producing a structured DataFrame with columns for the extracted text, file name, page numbers, and optionally, base64-encoded data for PDFs and images.",
      "details": "<h2>DocumentToText Node Details</h2><br>\nThe DocumentToText node extracts text content from documents, including PDF, TXT, DOCX, and image files, located in a specified file path or directory. It processes either a single file or an entire directory, producing a structured DataFrame with columns for the extracted text, file name, page numbers, and optionally, base64-encoded data for PDFs and images.<br>\n<br>\n<br>\n<h4>General:</h4><br>\n<br>\n<h5>Directory/File Path:</h5> Specifies the path to a single document file or a directory containing multiple documents. This field is required and must be accessible to the PySpark engine.<br>\n<br>\n<h5>Document Type:</h5> Selects the types of documents to process. Options include:<br>\n<br>\n<ul>\n<li> pdf: Processes PDF files, extracting text and optionally converting pages to base64-encoded images.</li>\n<li> txt: Processes plain text files, extracting text only.</li>\n<li> docx: Processes Microsoft Word documents, extracting text only.</li>\n<li> image: Processes image files (e.g., PNG, JPEG) for text extraction via OCR, with optional base64 encoding.</li>\n</ul>\nIf left empty, the node processes all supported file types (PDF, TXT, DOCX, and images) in the specified path.<br>\n<br>\n<br>\n<h5>Select Image Column:</h5> Specifies a column containing paths to image files when processing images. This is optional and only required if image paths are provided in a DataFrame column rather than the directory path.<br>\n<br>\n<h5>Image Encoding:</h5> Determines whether to include a column with base64-encoded data for PDFs and images. Options are:<br>\n<ul>\n<li> true: Adds a column with base64-encoded representations of PDF pages and image files.</li>\n<li> false: Does not include base64-encoded data (default).</li>\n</ul>\nNote: TXT and DOCX files are not converted to base64 encodings, even if this option is enabled.<br>\n<br>\n<br>\n<h4>Recursive Processing:</h4><br>\n<h5>Recursive:</h5> Controls whether the node processes documents in subdirectories. Options are:<br>\n<ul>\n<li> true: Recursively processes all documents in the specified directory and its subdirectories.</li>\n<li> false: Processes only documents directly in the specified directory (default).</li>\n</ul>\n<br>\n<h4>Output Storage:</h4><br>\n<h5>Save Images Directory Path:</h5> Specifies a directory path to save extracted images (for PDFs and image files) when Image Encoding is enabled. This is optional and relevant for storing extracted image data.<br>\n<br>\n<h4>Output:</h4><br>\nThe node outputs a DataFrame with the following default columns:<br>\n<br>\n<ul>\n<li> fileName: The name of the source file.</li>\n<li> content: The extracted text content from the document.</li>\n<li> pageNumber:> The page number of the extracted content (for multi-page documents like PDFs; single-page documents like TXT, DOCX, and images use page number 1).</li>\n<li> If Image Encoding is set to true, a base64ImageData column is included for PDFs and images, containing base64-encoded representations of the pages or images. TXT and DOCX files will have null in this column.</li>\n</ul>",
      "examples": "<h2> Example: DocumentToText Node</h2>\n<br>\n<h3> Input:</h3>\nA directory /data/documents/ contains the following files:<br>\n- report.pdf (a 2-page PDF document)<br>\n- notes.txt (a plain text file)<br>\n- proposal.docx (a Microsoft Word document)<br>\n- chart.png (an image file with text)<br>\n<br>\nThe DocumentToText node is configured as follows:<br>\n<ul>\n<li> Directory/File Path: /data/documents/</li>\n<li> Document Type: [\"pdf\", \"txt\", \"docx\", \"image\"] (process all supported types)</li>\n<li> Select Image Column: Empty (no specific image column; uses file path for images)</li>\n<li> Image Encoding: true (includes base64-encoded data for PDFs and images)</li>\n<li> Recursive: false (processes only files in the specified directory)</li>\n<li> Save Images Directory Path: /data/output/images/</li>\n</ul>\n<h3> Output:</h3>\nThe node processes the files and produces a DataFrame with the following structure:<br>\n<br>\nfileName       | content                              | pageNumber | base64ImageData<br>\n---------------|--------------------------------------|------------|----------------------------------<br>\nreport.pdf     | This is page 1 of the report...      | 1          | iVBORw0KGgoAAAANSUhEUg...<br>\nreport.pdf     | This is page 2 of the report...      | 2          | iVBORw0KGgoAAAANSUhEUg...<br>\nnotes.txt      | Meeting notes: discuss project...    | 1          | null<br>\nproposal.docx  | Proposal for new project...          | 1          | null<br>\nchart.png      | Sales: Q1 2025...                    | 1          | iVBORw0KGgoAAAANSUhEUg...<br>\n<br>\n<h3> Explanation:</h3>\n- The report.pdf file is processed, extracting text from both pages, resulting in two rows (one per page). With Image Encoding set to true, each page is also converted to a base64-encoded image in the base64ImageData column.<br>\n- The notes.txt file is processed as a single-page document, with its text extracted into the content column. No base64 encoding is applied, so base64ImageData is null.<br>\n- The proposal.docx file is processed, extracting its text content into a single row. No base64 encoding is applied, so base64ImageData is null.<br>\n- The chart.png file is processed using OCR to extract text, and its base64-encoded image data is included in the base64ImageData column.<br>\n- Extracted images from the PDF and PNG files are saved to /data/output/images/.<br>\n- Since Recursive is set to false, only files directly in /data/documents/ are processed.<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.gai.NodeDocumentToText",
      "x": "182.017px",
      "y": "249.017px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "filePath",
          "value": "/home/sparkflows/fire-data/data/GENAI/Agentic-App-Dataset/Uploads",
          "widget": "textfield",
          "title": "Directory/File Path",
          "description": "Select a Pdf/Text/Docx File or Directory",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "fileType",
          "value": "pdf",
          "widget": "array",
          "title": "Document Type",
          "description": "Choose a Document Type.If Empty all four types of files will be processed.",
          "optionsArray": [
            "pdf",
            "txt",
            "docx",
            "image"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "recursive",
          "value": "false",
          "widget": "array",
          "title": "Recursive",
          "description": "Recursively process the documents in the given Directory",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "imgPathCols",
          "value": "[]",
          "widget": "variables",
          "title": "Select Image Column",
          "description": "",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "isImage",
          "value": "true",
          "widget": "array",
          "title": "Image Encoding",
          "description": "Adds a column for base64 encoded pages",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "saveImageDir",
          "value": "",
          "widget": "textfield",
          "title": "Save Images Directory Path",
          "description": "The file path to save the output",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "columns",
          "value": "",
          "widget": "tab",
          "title": "Rename Output Cols",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "fileNameCol",
          "value": "fileName",
          "widget": "textfield",
          "title": "File Name Column",
          "description": "Rename File Name Column. Defaults to fileName",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "contentCol",
          "value": "content",
          "widget": "textfield",
          "title": "Content Column",
          "description": "Rename Content Column. Defaults to content",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "pageNumberCol",
          "value": "pageNumber",
          "widget": "textfield",
          "title": "Page Number Column",
          "description": "Rename Page Number Column. Defaults to pageNumber",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "base64ImageCol",
          "value": "base64ImageCol",
          "widget": "textfield",
          "title": "Base64 Image Column",
          "description": "Rename Image Column. Defaults to base64ImageCol",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    },
    {
      "id": "2",
      "name": " Multi LLM Query",
      "iconImage": "/images/icons/node-icon/Graph_group_by_column.svg",
      "description": "The Multi LLM Query node is designed to query multiple large language models (LLMs) from providers such as OpenAI, Bedrock, and Gemini, using a DataFrame as input. It processes user queries, text content, and/or base64-encoded images to generate responses based on the selected model and task, producing a structured DataFrame output.",
      "details": "<h2>Multi LLM Query Node Details</h2>\n<br>\nThe Multi LLM Query node is designed to query multiple large language models (LLMs) from providers such as OpenAI, Bedrock, and Gemini, using a DataFrame as input. It processes user queries, text content, and/or base64-encoded images to generate responses based on the selected model and task, producing a structured DataFrame output.<br>\n<br>\n<h3>General:</h3>\n<br>\n<h4>Select Task:</h4>\nSpecifies the task to perform. Options include:<br>\n- summary: Generates a summary of the content in bullet points.<br>\n- translation: Translates the content to English.<br>\n- topic_extraction: Extracts key topics from the content.<br>\n- other: Allows for a custom task defined by the user.<br>\n<br>\n<h4>Prompt:</h4>\nAllows users to provide a custom prompt / instructions for the selected task.<br>\n<br>\n<h4>Content Column:</h4>\nSpecifies the DataFrame column containing the text content to be processed. Required for text or text+image modes.<br>\n<br>\n<h4>Model Selection:</h4>\nSelects the LLM provider to use. Options are:<br>\n- openai: Uses OpenAI models (e.g., gpt-4o).<br>\n- bedrock: Uses AWS Bedrock models (e.g., Claude models).<br>\n- gemini: Uses Google Gemini models (e.g., gemini-1.5-flash-latest).<br>\n<br>\n<h4>Select Connection:</h4>\nSpecifies the connection details for the selected LLM provider (e.g., API keys for OpenAI/Gemini, AWS credentials for Bedrock). Required to authenticate and access the chosen model.<br>\n<br>\n<h4>Temperature:</h4>\nControls the randomness of the LLM's output. Default is 0.7. Higher values increase creativity, while lower values ensure more deterministic responses.<br>\n<br>\n<h4>Image Column:</h4>\nSpecifies the DataFrame column containing base64-encoded images. Required for image or text+image modes.<br>\n<br>\n<h4>Mode Selection:</h4>\nDetermines the input mode for the LLM. Options are:<br>\n- text: Processes text-only input from the content column or custom prompt.<br>\n- image: Processes base64-encoded images from the image column.<br>\n- text+image: Processes both text and base64-encoded images.<br>\n<br>\n<h3>Advanced:</h3>\n<br>\n<h4>Aggregate Response:</h4>\nSpecifies how to aggregate input data before processing. Options are:<br>\n- none: Processes each row individually, retaining fileName and pageNumber (if provided).<br>\n- all: Aggregates all rows into a single response.<br>\n- perfile: Aggregates rows by fileName, producing one response per file.<br>\n<br>\n<h4>Number of Partitions:</h4>\nSpecifies the number of Spark partitions for distributed processing. Default is 3.<br>\n<br>\n<h4>File Name Column:</h4>\nSpecifies the DataFrame column containing file names. Required for perfile aggregation mode.<br>\n<br>\n<h4>Page Number Column:</h4>\nSpecifies the DataFrame column containing page numbers (e.g., for PDFs). Optional, used for row-wise processing with none aggregation mode.<br>\n<br>\n<h3>Output:</h3>\nThe node outputs a DataFrame with columns based on the aggregation mode:<br>\n- none: Includes fileName (if provided), pageNumber (if provided), and response.<br>\n- perfile: Includes fileName and response.<br>\n- all: Includes only the response column.<br>\nThe response column contains the LLM-generated text or error messages if the API call fails.<br>",
      "examples": "<h2>Multi LLM Query Node Examples</h2>\n<br>\n<h3>Input:</h3>\nA DataFrame contains the following data:<br>\n- fileName: [\"doc1.pdf\", \"doc1.pdf\", \"doc2.pdf\"]<br>\n- pageNumber: [\"1\", \"2\", null]<br>\n- content: [\"Article about climate change...\", \"Climate change impacts...\", \"Renewable energy report...\"]<br>\n- imageBase64: [null, \"iVBORw0KGgoAAAANSUhEUg...\", null]<br>\n<br>\nThe Multi LLM Query node is configured as follows:<br>\n- Select Task: summary<br>\n- Prompt: \"Summarize the content in bullet points.\"<br>\n- Content Column: content<br>\n- Model Selection: openai<br>\n- Select Connection: Configured with valid OpenAI API key<br>\n- Temperature: 0.7<br>\n- Image Column: imageBase64<br>\n- Mode Selection: text+image<br>\n- Aggregate Response: perfile<br>\n- Number of Partitions: 3<br>\n- File Name Column: fileName<br>\n- Page Number Column: pageNumber<br>\n<br>\n<h3>Output:</h3>\nThe node processes the DataFrame and produces a DataFrame with the following structure:<br>\n- fileName: doc1.pdf<br>\nresponse: - Climate change effects on ecosystems<br>\n- Rising temperatures<br>\n- fileName: doc2.pdf<br>\nresponse: - Renewable energy advancements<br>\n- Solar and wind adoption<br>",
      "type": "pyspark",
      "nodeClass": "fire.nodes.gai.NodeMultiLLMQuery",
      "x": "511.5px",
      "y": "253.5px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "llmConnection",
          "value": "openai-api-connection",
          "widget": "object_array",
          "title": "Select Connection",
          "description": "Select Connection",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "temperature",
          "value": "0.2",
          "widget": "textfield",
          "title": "Temperature",
          "description": "Temperature setting for the model (default: 0).",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "contentCol",
          "value": "content",
          "widget": "variable",
          "title": "Content Column",
          "description": "Column name for the text content.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "imageCol",
          "value": "base64ImageCol",
          "widget": "variable",
          "title": "Image Column",
          "description": "Column name for the base 64 image.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "inputMode",
          "value": "text+image",
          "widget": "array",
          "title": "Mode Selection",
          "description": "Select the model to use (text, image, text+image).",
          "optionsArray": [
            "text",
            "image",
            "text+image"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "Prompt",
          "value": "",
          "widget": "tab",
          "title": "Prompt",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "task",
          "value": "other",
          "widget": "key_value_array",
          "title": "Select Prompt",
          "description": "Specify the task to perform: summary, translation, topic extraction, or other.",
          "optionsMap": {
            "summary": "Generate a concise, multi-level bullet-point summary capturing key facts, insights, and implications from the content. Preserve structure and section hierarchy.",
            "translation": "Translate the content into fluent, formal English while preserving tone, context, cultural nuances, and domain-specific terminology (e.g., legal, medical, technical).",
            "topic_extraction": "Identify and extract key topics, subtopics, and entities. Categorize them with tags (e.g., Person, Location, Concept) and provide brief descriptions or summaries of each.",
            "other": ""
          },
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "customPrompt",
          "value": "**Final Combined Prompt – Structured QA Test Cases in HTML Table Format**\n\n\n\n\nYou are an experienced enterprise QA Engineer. Your task is to generate a complete, professional test suite based on the following Software Requirements Specification (SRS) document.\n\n\n\n\nEach test case must follow a structured format and be ready for inclusion in a formal QA test management system (e.g., TestRail, Zephyr, QMetry). Do not include any conversational text, explanations, or filler content. Simply return the test cases, cleanly and precisely formatted.\n\n\n\n\n---\n\n\n\n\n### Format for Each Test Case:\n\n\n\n\n* **Test Case ID:** TC\\_\\<module/feature>\\_<3-digit number>\n\n* **Description:** One-line summary of what is being tested.\n\n* **Pre-conditions:** Any setup or pre-state required before executing the test.\n\n* **Test Steps:** A clearly numbered list of manual steps to execute the test.\n\n* **Expected Result:** Specific, testable result or system behavior.\n\n* **Related Requirement:** The SRS requirement ID or section. Use `N/A` if unknown.\n\n\n\n\n---\n\n\n\n\n### Output Format – Use HTML Table\n\n\n\n\nRender all test cases together inside a single `<table>`. Each test case should occupy **one row** in the table. Use this column structure:\n\n\n\n\n* **Test Case ID**\n\n* **Description**\n\n* **Pre-conditions**\n\n* **Test Steps** (inside `<ol>`)\n\n* **Expected Result**\n\n* **Related Requirement**\n\n\n\n\nUse this exact HTML table structure:\n\n\n\n\n\n<table border=\"1\" cellpadding=\"6\" style=\"border-collapse: collapse; width: 100%; margin-top: 10px;\">\n\n  <thead style=\"background-color: #f2f2f2;\">\n\n    <tr>\n\n      <th>Test Case ID</th>\n\n      <th>Description</th>\n\n      <th>Pre-conditions</th>\n\n      <th>Test Steps</th>\n\n      <th>Expected Result</th>\n\n      <th>Related Requirement</th>\n\n    </tr>\n\n  </thead>\n\n  <tbody>\n\n    <!-- One <tr> per test case -->\n\n    <tr>\n\n      <td>TC_Module_001</td>\n\n      <td>Brief summary</td>\n\n      <td>Preconditions text</td>\n\n      <td>\n\n        <ol style=\"margin: 0; padding-left: 20px;\">\n\n          <li>Step 1</li>\n\n          <li>Step 2</li>\n\n        </ol>\n\n      </td>\n\n      <td>Expected behavior</td>\n\n      <td>FR1</td>\n\n    </tr>\n\n  </tbody>\n\n</table>\n\n```\n\n\n\n\nEach `<td>` must be strictly aligned with its header. All test steps must be formatted as `<ol><li>Step</li></ol>` inside the `Test Steps` cell.\n\n\n\n\n---\n\n\n\n\n### Coverage Requirements:\n\n\n\n\nEnsure your test cases comprehensively cover:\n\n\n\n\n####  Functional Requirements\n\n\n\n\n* Valid and invalid user actions (happy paths and edge cases)\n\n* Input field validations\n\n* State transitions and business rule enforcement\n\n* Alternate flows, error handling, and exceptions\n\n\n\n\n#### Non-Functional Requirements\n\n\n\n\n* Performance benchmarks (response time, concurrency)\n\n* Security (encryption, unauthorized access prevention)\n\n* Usability and accessibility (WCAG compliance, intuitive layout)\n\n* Compatibility (cross-browser and device support)\n\n\n\n\n---\n\n\n\n\n### Output Constraints:\n\n\n\n\n* Begin **directly** with the `<table>` tag and end with `</table>`.\n\n* Do **not** include any headings, markdown, prose, or meta text.\n\n* Do **not** wrap content in code blocks or add any explanatory lines.\n\n* Use only clean, valid HTML output.\n \n\n\n\n---\n\n\n\n\n### Example Row (for guidance only):\n\n\n\n\n\n\n<tr>\n\n  <td>TC_Login_001</td>\n\n  <td>Login with valid username and password</td>\n\n  <td>User account is active</td>\n\n  <td>\n\n    <ol style=\"margin: 0; padding-left: 20px;\">\n\n      <li>Navigate to login page</li>\n\n      <li>Enter valid credentials</li>\n\n      <li>Click on 'Login'</li>\n\n    </ol>\n\n  </td>\n\n  <td>User is redirected to the dashboard</td>\n\n  <td>SRS-Auth-1.1</td>\n\n</tr>\n\n\n\n\n\n\n\nNow, based on the following SRS document, generate all relevant QA test cases using the structure and constraints above. The output must be a single clean HTML table containing all test cases as separate rows.\n\n\nmake sure that the table has all the rows and columns filled by the html tag , for all the test cases , validate and give the final output based on above rules \n\n\n\n\nSRS Document:\n\n{content}",
          "widget": "textareafield",
          "title": "Prompt",
          "description": "Custom prompt to override the default instructions.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "userQueryCol",
          "value": "",
          "widget": "variable",
          "title": "User Query Column",
          "description": "Column name for user query, (if the query is in a column)",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "Advanced",
          "value": "",
          "widget": "tab",
          "title": "Advanced",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "aggregateMode",
          "value": "ALL",
          "widget": "enum",
          "title": "Aggregate Response",
          "optionsMap": {
            "NONE": "Do not aggregate rows",
            "ALL": "Aggregate all rows",
            "PERFILE": "Aggregate per file"
          },
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "numPartitions",
          "value": "3",
          "widget": "textfield",
          "title": "Number of Partitions",
          "description": "Number of Partitions",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "fileNameCol",
          "value": "",
          "widget": "variable",
          "title": "File Name Column",
          "description": "Select File Name Column",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "pageNumberCol",
          "value": "",
          "widget": "variable",
          "title": "Page Number Column",
          "description": " Select Page Number column.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    },
    {
      "id": "4",
      "name": "Save PDF",
      "iconImage": "fa fa-tumblr-square",
      "description": "Saves DataFrame responses as PDF files",
      "details": "<h2>Save PDF Node Details</h2><br>\nThe Save PDF Node is designed to save DataFrame responses as PDF files, either locally or to an S3 bucket. It supports saving content from a specified DataFrame column, with options to combine responses into a single file, save individual files, or group by page number. The node processes text, HTML, or markdown content, converting it to PDF format using libraries like WeasyPrint and FPDF. This node is ideal for generating structured PDF outputs in data pipelines.<br>\n<br>\n<h4>General:</h4><br>\n<br>\n<h5>Output Path:</h5> Specifies the file path where the PDF files will be saved. This can be a local path or an S3 path (e.g., s3://bucket_name/prefix/). This field is required.<br>\n<br>\n<h5>Content Column:</h5> Specifies the DataFrame column containing the content to be saved as PDF. This can include text, HTML, or markdown content. This field is optional, but required if no default content is provided.<br>\n<br>\n<h5>Save Mode:</h5> Determines the behavior when the output path already exists. Options are:<br>\n<ul>\n<li> Overwrite: Overwrites existing files at the specified path.</li>\n<li> ErrorIfExists: Throws an error if the output path already exists.</li>\n<li> Ignore: Skips saving if the output path already exists.</li>\n</ul>\n<h5>Default File Name:</h5> Specifies the default file name for the PDF output (e.g., \"pdf_output\"). Used when no file name column is provided or when saving a single combined file.<br>\n<br>\n<h5>Save Option:</h5> Specifies how the responses are saved. Options are:<br>\n<ul>\n<li> ALL: Combines all responses into a single PDF file.</li>\n<li> PERFILE: Saves each response as a separate PDF file, based on the file name column.</li>\n<li> NONE: Saves each page as a separate PDF file, grouped by page number.</li>\n</ul>\n<h5>File Name Column:</h5> Specifies the DataFrame column containing file names for the output PDF files. This is required when Save Option is set to PERFILE or NONE.<br>\n<br>\n<h5>Page Number Column:</h5> Specifies the DataFrame column containing page numbers for multi-page documents. This is required when Save Option is set to NONE.<br>\n<br>\n<h5>Translate File Name:</h5> Determines whether non-English file names should be translated to English before saving. Options are:<br>\n<ul>\n<li> true: Translates file names to English using an external translation service (e.g., Google Translator).</li>\n<li> false: Retains original file names without translation.</li>\n</ul>\n<h4>System Prompt Configuration:</h4><br>\n<h5>System Prompt:</h5> An optional tab for advanced configurations. Currently, no specific system-level prompt is used, but this can be extended for future customization of PDF generation behavior.<br>\n<br>\n<h4>Output:</h4><br>\nThe node does not modify the input DataFrame but saves the content from the specified column as PDF files to the designated output path. The output PDF files may include:<br>\n<ul>\n<li> A single combined PDF file (if Save Option is ALL).</li>\n<li> Individual PDF files for each response (if Save Option is PERFILE).</li>\n<li> Separate PDF files for each page (if Save Option is NONE).</li>\n<li> The generated PDFs may include formatted text, HTML, or markdown content, with proper styling for headers, lists, and paragraphs.</li>\n</ul>",
      "examples": "<h2>Example: Save PDF Node</h2><br>\n<br>\n<h3>Input:</h3><br>\nA DataFrame contains the following data:<br>\n- content: [\"# Climate Change\\n- Rising sea levels\\n- Extreme weather\", \"<html><body>Renewable Energy<p>Solar and wind advancements...</p></body></html>\", \"AI Study\\n- Machine learning\\n- Neural networks\"]<br>\n- fileName: [\"climate_report\", \"energy_report\", \"ai_study\"]<br>\n- pageNumber: [1, 1, 2]<br>\n<br>\nThe Save PDF Node is configured as follows:<br>\n<ul>\n<li> Output Path: /data/output/pdfs/</li>\n<li> Content Column: content</li>\n<li> Save Mode: Overwrite</li>\n<li> Default File Name: pdf_output</li>\n<li> Save Option: PERFILE</li>\n<li> File Name Column: fileName</li>\n<li> Page Number Column: pageNumber</li>\n<li> Translate File Name: false</li>\n<li> System Prompt: Empty (no advanced configuration used)</li>\n</ul>\n<h3>Output:</h3><br>\nThe node processes the DataFrame and saves PDF files to /data/output/pdfs/ with the following structure:<br>\n<br>\n- climate_report.pdf: Contains formatted content with a header \"Climate Change\" and a bullet list.<br>\n- energy_report.pdf: Contains formatted HTML content with a header \"Renewable Energy\" and a paragraph.<br>\n- ai_study.pdf: Contains formatted content with a header \"AI Study\" and a bullet list.<br>\n<br>\n<h3>Explanation:</h3><br>\n- The first row processes the markdown content \"# Climate Change\\n- Rising sea levels\\n- Extreme weather\", converting it to a PDF with a bold header and bullet points.<br>\n- The second row processes the HTML content, rendering it directly as a PDF with proper formatting for the header and paragraph.<br>\n- The third row processes the markdown content \"AI Study\\n- Machine learning\\n- Neural networks\", converting it to a PDF with a bold header and bullet points.<br>\n- Since Save Option is set to PERFILE, each response is saved as a separate PDF file named after the fileName column (e.g., climate_report.pdf).<br>\n- The Page Number Column is used to track page numbers but does not affect the output since Save Option is PERFILE.<br>\n- Translate File Name is set to false, so file names are used as-is without translation.<br>\n- Save Mode is set to Overwrite, so any existing files in /data/output/pdfs/ are overwritten.<br>\n- The output PDFs are saved to the local path /data/output/pdfs/ with proper formatting for headers, lists, and text, using WeasyPrint for HTML/markdown content and FPDF for plain text.<br>",
      "type": "transform",
      "nodeClass": "fire.nodes.save.NodeSavePdf",
      "x": "831.067px",
      "y": "143.067px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "outputPath",
          "value": "/home/sparkflows/fire-data/Agentic-App-Datasets/Outputs",
          "widget": "textfield",
          "title": "Output Path",
          "description": "Path where to save the PDF files (local or S3)",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "content",
          "value": "response",
          "widget": "variable",
          "title": "Content Column",
          "description": "DataFrame column containing filenames for output files",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "saveMode",
          "value": "Overwrite",
          "widget": "array",
          "title": "Save Mode",
          "description": "Whether to Append, Overwrite, Error if the path Exists, or Ignore",
          "optionsArray": [
            "Overwrite",
            "ErrorIfExists",
            "Ignore"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "System Prompt",
          "value": "",
          "widget": "tab",
          "title": "Advanced",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "generic_file_name",
          "value": "pdf_output",
          "widget": "textfield",
          "title": "Default File Name",
          "description": "Default pdf file name where the PDF will be saved",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "saveOption",
          "value": "ALL",
          "widget": "enum",
          "title": "Save Option",
          "optionsMap": {
            "ALL": "Combine into single file",
            "PERFILE": "Save as individual files",
            "NONE": "Group by page number"
          },
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "fileNameCol",
          "value": "",
          "widget": "variable",
          "title": "File Name Column",
          "description": "DataFrame column containing filenames for output files (used in separate files or page number modes)",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "pageNumberCol",
          "value": "",
          "widget": "variable",
          "title": "Page Number Column",
          "description": "DataFrame column containing page numbers for multi-page documents (used in page number mode)",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "translateFileName",
          "value": "false",
          "widget": "array",
          "title": "Translate File Name",
          "description": "Translates file names to be saved from any language to English",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "all"
    },
    {
      "id": "6",
      "name": "Output Formatter",
      "description": "This node formats output from Columns.",
      "details": "<h2> Output Formatter Node Details</h2>\nThe Output Formatter node formats data from a specified column in an input DataFrame and outputs it with a user-defined key. It is designed for use in PySpark-based data processing pipelines to extract and present data in a structured format, typically for downstream use or display. The node processes a single column from the input DataFrame, formats the content, and sends it as a JSON message with a specified key.<br>\n<br>\n<h4> General:</h4>\n<br>\nh5: Select Column:<br>\nSpecifies the column in the input DataFrame from which to extract data. This field is required and must correspond to a valid column name in the DataFrame.<br>\n<br>\nh5: Key:<br>\nDefines a key name for the formatted output. This field is required and is used to label the extracted column value in the output JSON message.<br>\n<br>\n<h4> Output:</h4>\nThe node does not modify the input DataFrame but instead generates a JSON-formatted message containing the following:<br>\n<ul>\n<li> id: The node’s ID.</li>\n<li> name: The node’s name (\"Output Formatter\").</li>\n<li> title: The display title (\"Output Formatter\").</li>\n<li> type: The node type (\"formatter\").</li>\n<li> resultType: Set to 3, indicating the output is a formatted message.</li>\n<li> visibility: Set to \"EXPANDED\" for display purposes.</li>\n<li> text: A nested structure containing:</li>\n<li> key: The user-specified key name.</li>\n<li> string: The value extracted from the selected column (from the first row of the DataFrame).</li>\n<li> format: Set to \"plaintext\" for the output format.</li>\n</ul>\nThe JSON message is sent to the workflow context for further processing or display. The input DataFrame is passed through unchanged as the node’s output schema.<br>",
      "examples": "<h2> Example: Output Formatter Node</h2>\n<br>\n<h3> Input:</h3>\nA DataFrame with the following structure, containing a single row of data:<br>\n<br>\n| summary_text                     |<br>\n|----------------------------------|<br>\n| Project meeting: Plan Q1 goals...|<br>\n<br>\nThe Output Formatter node is configured as follows:<br>\n<ul>\n<li> Select Column: summary_text</li>\n<li> Key: meeting_summary</li>\n</ul>\n<h3> Output:</h3>\nThe node processes the DataFrame and generates a JSON-formatted message sent to the workflow context, with the following structure:<br>\n<br>\n```json<br>\n{<br>\n\"id\": \"11\",<br>\n\"name\": \"Output Formatter\",<br>\n\"title\": \"Output Formatter\",<br>\n\"type\": \"formatter\",<br>\n\"resultType\": 3,<br>\n\"visibility\": \"EXPANDED\",<br>\n\"text\": {<br>\n\"key\": \"meeting_summary\",<br>\n\"string\": \"Project meeting: Plan Q1 goals...\",<br>\n\"format\": \"plaintext\"<br>\n}<br>\n}<br>\n```<br>\n<br>\nThe input DataFrame is passed through unchanged as the node’s output schema.<br>\n<br>\n<h3> Explanation:</h3>\n<ul>\n<li> The summary_text column is selected, and the value from its first row (\"Project meeting: Plan Q1 goals...\") is extracted.</li>\n<li> The key field is set to \"meeting_summary\", which is used to label the extracted value in the output JSON.</li>\n<li> The node formats the extracted value into a JSON message with a nested text object, specifying the key, string value, and format (\"plaintext\").</li>\n<li> The JSON message is sent to the workflow context for further processing or display.</li>\n<li> The original DataFrame is returned as the output schema without modification.</li>\n</ul>",
      "type": "transform",
      "nodeClass": "fire.nodes.graph.NodeOutputFormatter",
      "x": "780.067px",
      "y": "245.067px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "column",
          "value": "response",
          "widget": "variable",
          "title": " Select Column",
          "description": "Select Column to format",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        },
        {
          "name": "key",
          "value": "genAiResponse",
          "widget": "textfield",
          "title": "Key",
          "description": "Specify a key Name",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false,
          "header": "",
          "size": "100%",
          "toggle": ""
        }
      ],
      "engine": "pyspark"
    }
  ],
  "edges": [
    {
      "source": "2",
      "target": "4",
      "id": 1
    },
    {
      "source": "2",
      "target": "6",
      "id": 2
    },
    {
      "source": "1",
      "target": "2",
      "id": 3
    }
  ],
  "dataSetDetails": [],
  "engine": "pyspark"
}