{
  "id": "1",
  "name": "EMR Workflow",
  "description": "This node adds the workflow in the project as an EMR step and executes it.",
  "type": "workflow",
  "nodeClass": "fire.pipelineNodes.EMRWorkflow",
  "fields" : [
    {"name": "Name", "value":"", "required":true, "widget": "textfield", "title": "Task Name", "description": "Unique name of the task in Airflow DAG"},
    {"name": "ClusterId", "value":"", "required":false, "widget": "textfield", "title": "Cluster ID", "description": "If Cluster ID is empty, the step tries to pick the cluster ID from previous create EMR node(task)" },
    {"name": "ActionOnFailure","value":"CONTINUE", "required":true, "widget": "array", "optionsArray": ["CANCEL_AND_WAIT", "CONTINUE", "TERMINATE_JOB_FLOW", "TERMINATE_CLUSTER"], "title": "Action On Failure", "description": "Action to be taken on Failure" },
    {"name": "SparkConf", "value":"", "required":false, "widget": "textfield", "title": "Spark Conf", "description": "Add spark conf values in comma separated as key=value" , "expandable": true},
    {"name": "Jars", "value":"", "required":false, "widget": "textfield", "title": "JARS", "description": "Comma seperated list of Jars" , "expandable": true},
    {"name": "Packages", "value":"", "required":false, "widget": "textfield", "title": "Packages", "description": "Comma seperated list of Packages" , "expandable": true},
    {"name": "workflowUUID", "value":"", "required":true, "widget": "workflow", "title": "Workflow ID", "description": "Select the appropriate workflow" },
    {"name": "retries", "value":"", "required":false, "widget": "textfield", "title": "Retries", "description": "Retries" },
    {"name": "retry_delay", "value":"", "required":false, "widget": "textfield", "title": "Retry Delay", "description": "Retry Delay(seconds)" },
    {"name": "max_retry_delay", "value":"", "required":false, "widget": "textfield", "title": "Max Retry Delay", "description": "Maximum delay interval between retries(seconds)" },
    {"name": "retry_exponential_backoff", "value":"", "required":false, "widget": "array", "optionsArray": ["True","False"], "title": "Retry Exponential Backoff", "description": "Allow progressive longer waits between retries by using exponential backoff algorithm on retry delay" },
    {"name": "aws_conn_id", "value":"", "required":false, "widget": "textfield", "title": "AWS Connection ID", "description": "AWS Connection ID" },
    {"name": "deploy-mode", "value":"cluster", "widget": "array", "optionsArray": ["cluster","client"], "title": "Deploy Mode", "description": "Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client)", "required": true},
    {"name": "parameters", "value":"[]",  "widget": "variablesList", "optionsArray": ["Key","Value"], "title": "Parameters", "description": "List of parameters to be passed to the workflow", "expandable":true  },
    {"name": "trigger_rule", "value":"all_success", "required":true, "widget": "array", "optionsArray": ["all_success","all_failed","all_done","all_skipped","one_failed","one_success","none_failed","none_failed_min_one_success","none_skipped","always"], "title": "Trigger Rule", "description": "Trigger Rule to be used" }
  ]
}
start-details:

h2:EMR Workflow

This node adds the workflow in the project as an EMR step and executes it.

end-details:

start-examples:

h2: EMR Workflow Examples

h4: Example of Adding an EMR Workflow

{
"Name": "MyEMRWorkflow",
"ClusterId": "j-1234567890ABCDE",
"ActionOnFailure": "CONTINUE",
"SparkConf": "spark.executor.instances=2,spark.executor.memory=4g",
"Jars": "s3://my-bucket/my-jar.jar,s3://my-bucket/my-other-jar.jar",
"workflowUUID": "123e4567-e89b-12d3-a456-426614174000",
"aws_conn_id": "aws_default",
"deploy-mode": "client",
"parameters": [
{"Key": "inputPath", "Value": "s3://my-bucket/input/"},
{"Key": "outputPath", "Value": "s3://my-bucket/output/"}
],
"trigger_rule": "all_success"
}

h4: Explanation
* This example demonstrates how to define an EMR workflow with specific parameters, including the task name, action on failure, Spark configurations, and the workflow UUID.

h4: Usage
* Task ID: "emr_workflow_task"
* Workflow ID: 123e4567-e89b-12d3-a456-426614174000

h4: Example of Adding an EMR Workflow with Specified Cluster ID

{
"Name": "DataProcessingWorkflow",
"ClusterId": "j-0987654321XYZ",
"ActionOnFailure": "TERMINATE_JOB_FLOW",
"SparkConf": "spark.executor.instances=4,spark.executor.memory=8g",
"Jars": "s3://my-bucket/data-processing-jar.jar",
"workflowUUID": "987e6543-e21b-43d3-a654-426614174111",
"aws_conn_id": "my_aws_connection",
"deploy-mode": "cluster",
"parameters": [
{"Key": "inputPath", "Value": "s3://my-bucket/data/"},
{"Key": "outputPath", "Value": "s3://my-bucket/data/output/"}
],
"trigger_rule": "one_success"
}

h4: Explanation
* This example adds an EMR workflow with a specified cluster ID and a different action on failure. It demonstrates how to adjust parameters for different use cases.

h4: Usage
* Task ID: "data_processing_workflow_task"
* Workflow ID: 987e6543-e21b-43d3-a654-426614174111

end-examples: