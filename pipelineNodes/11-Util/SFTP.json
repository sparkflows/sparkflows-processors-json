{
  "id": "7",
  "name": "SFTP",
  "description": "Secure file transfer protocol",
  "type": "Node",
  "nodeClass": "fire.nodes.sftp.NodeSftp",
  "fields": [
    {"name": "Name", "value":"", "required":true, "widget": "textfield", "title": "Task Name", "description": "Unique name of the task in Airflow DAG"},
    {"name": "ClusterId", "value":"", "required":false, "widget": "textfield", "title": "Cluster ID", "description": "If Cluster ID is empty, the step tries to pick the cluster ID from previous create EMR node(task)" },
    {"name": "ActionOnFailure","value":"CONTINUE", "required":true, "widget": "array", "optionsArray": ["CANCEL_AND_WAIT", "CONTINUE", "TERMINATE_JOB_FLOW", "TERMINATE_CLUSTER"], "title": "Action On Failure", "description": "Action to be taken on Failure" },
    {"name": "SparkConf", "value":"", "required":false, "widget": "textfield", "title": "Spark Conf", "description": "Add spark conf values in comma separated as key=value", "expandable": true },
    {"name": "aws_conn_id", "value":"", "required":false, "widget": "textfield", "title": "AWS Connection ID", "description": "AWS Connection ID" },
    {"name": "deploy-mode", "value":"cluster", "widget": "array", "optionsArray": ["cluster","client"], "title": "Deploy Mode", "description": "Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client)", "required": true},
    {"name": "trigger_rule", "value":"all_success", "required":true, "widget": "array", "optionsArray": ["all_success","all_failed","all_done","all_skipped","one_failed","one_success","none_failed","none_failed_min_one_success","none_skipped","always"], "title": "Trigger Rule", "description": "Trigger Rule to be used" },
    {"name": "nodeconfiguration", "value":"", "widget": "tab", "title": "NodeConfiguration"},
    {"name":"sftpConnection", "value":"", "widget": "connections", "title": "Connection", "description": "The SFTP connection to connect" ,"required":"true"},
    {"name": "sftpRemoteLocation", "value": "", "widget": "textfield", "title": "SFTP Remote Location", "description": "User directory path (File take from)", "required": "true"},
    {"name": "targetLocation", "value": "", "widget": "textfield", "title": "Target Path", "description": "S3 Path to upload data."}
  ]
}

start-details:

h2: SFTP Node Configuration Details

The SFTP node enables secure file transfers from a specified remote location to a target location.

* Task Name: Assign a unique name for the task within the Airflow DAG.
* Connection: Select the SFTP connection to establish a secure transfer.
* SFTP Remote Location: Specify the directory path on the SFTP server where the file is located.
* Target Path: Provide the S3 path or other destination path where the file should be uploaded.
* Action on Failure: Determine the action if the task fails (e.g., CONTINUE, CANCEL_AND_WAIT).
* ClusterId: The unique identifier for the cluster associated with this task. If the preceding node is 'Create EMR JobFlow,' the ClusterId will be automatically set to the newly created cluster ID from the 'Create EMR JobFlow' step, unless specified otherwise. If not, the ClusterId must be provided explicitly.

Use the Trigger Rule setting to control when this node executes, based on the success, failure, or completion of prior tasks.

end-details:

start-examples:

h2: SFTP Node Configuration Examples

h4: Example 1 - Daily Sales Data Transfer

* Task Name: 'TransferDailySalesData'
* Connection: 'salesDataSFTP' (defined in connections)
* SFTP Remote Location: '/data/sales/daily/'
* Target Path: 's3://company-bucket/sales-data/daily/'
* Action on Failure: 'CONTINUE'
* Trigger Rule: 'all_success'

This setup transfers daily sales files from the SFTP directory /data/sales/daily/ to the S3 bucket s3://company-bucket/sales-data/daily/. If the transfer fails, the workflow will continue, as specified by the action on failure.

h4: Example 2 - Weekly Backup to S3 with Conditional Termination

* Task Name: 'WeeklyBackup'
* Connection: 'backupSFTPConnection'
* SFTP Remote Location: '/backups/weekly/'
* Target Path: 's3://company-backups/weekly/'
* Action on Failure: 'TERMINATE_JOB_FLOW'
* Trigger Rule: 'one_failed'

In this scenario, the task transfers weekly backups from the SFTP server directory /backups/weekly/ to the specified S3 bucket. If any previous task in the workflow fails, this task will still attempt to execute. If this task fails, it will terminate the job flow to prevent further execution.

end-examples:

