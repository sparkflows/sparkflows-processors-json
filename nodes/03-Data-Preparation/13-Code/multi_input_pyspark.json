{
  "id": "11",
  "name": "MultiInputPySpark",
  "description": "This node runs any given PySpark code. The input dataframe is passed in the variable inDFs. The output dataframe is passed back by registering it as a temporary table.",
  "input": "The input dataframe is passed in the variable inDFs.",
  "output": "The output dataframe is passed back by registering it as a temporary table",
  "type": "pyspark2inputs",
  "engine": "pyspark",
  "nodeClass": "fire.nodes.code.NodeMultiInputPySpark",
  "fields" : [
    {"name":"code", "value":"'''\nInput:\n  spark: spark session \n  workflowContext:  workflowcontext object \n  id: node number \n  inDFs: array of input pyspark dataframes \n  cust_dict: Dictionary of the workflow variables passed from previous node \nOutput:\n  outDF: return pyspark dataframe i.e outDF \n ''' \n \nfrom pyspark.sql.types import * \nfrom pyspark.sql.functions import * \nfrom pyspark.sql import * \nfrom fire.workflowcontext import WorkflowContext \n\ndef myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDFs: [DataFrame], cust_dict:dict):\n  outDF = inDFs[0]  #get the first dataframe\n  return outDF", "widget": "textarea_large", "type": "python", "title": "PySpark", "description": "PySpark code to be run. Input dataframe : \"inDF\", SparkContext : \"sc\", SQLContext : \"sqlContext\",  Output/Result dataframe should be registered as a temporary table - df.registerTempTable(\"outDF\")"},

    {"name": "schema", "value":"", "widget": "tab", "title": "Schema"},

    {"name":"outputColNames", "value":"[]", "widget": "schema_col_names", "title": "Column Names for the CSV", "description": "New Output Columns of the SQL"},
    {"name":"outputColTypes", "value":"[]", "widget": "schema_col_types", "title": "Column Types for the CSV", "description": "Data Type of the Output Columns"},
    {"name":"outputColFormats", "value":"[]", "widget": "schema_col_formats", "title": "Column Formats for the CSV", "description": "Format of the Output Columns"}

  ]
}

start-details:

h2:Pyspark Details

This node receives input pyspark dataframes in function called myfn.

The pyspark/python code processes it and returns one computed pyspark dataframe.

end-details:

start-examples:

h2:Pyspark Examples

Input Schema of dataframe.

Input Schema of first dataframe: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea

Input Schema of second dataframe: id, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea

h4: Add the house_type column

from pyspark.sql.types import StringType
from pyspark.sql.functions import *
from pyspark.sql import *
from fire.workflowcontext import *

def myfn(spark: SparkSession, workflowContext: WorkflowContext, id: int, inDFs:[DataFrame], cust_dict:dict):

#get the first dataframe
df1 = inDFs[0]

#get the second dataframe
df2 = inDFs[1]

# Join the two dataframes
outdf = df1.join(df2, ['id'])

return outdf

end-examples:
