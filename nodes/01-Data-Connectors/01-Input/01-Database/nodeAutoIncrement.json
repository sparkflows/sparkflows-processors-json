{
  "id": "11",
  "name": "AutoIncrement",

  "description": "This node reads data from Relational Databases using JDBC and creates a DataFrame from it",
  "input": "It reads data from Relational Databases",
  "output": "It creates a DataFrame from the data read and sends it to its output",

  "type": "dataset",
  "nodeClass": "fire.nodes.dataset.NodeAutoIncrement",
  "fields" : [
    {"name":"connection", "value":"", "widget": "object_array", "title": "Connection", "description": "The JDBC connection to connect" ,"required":"true"},

    {"name":"database", "value":"", "widget": "textfield", "title": "Database Name", "required":"true", "description": ""},
    {"name":"table", "value":"", "widget": "textfield", "title": "Table Name", "required":"true", "description": ""},

    {"name":"keycolumnName", "value":"", "widget": "textfield", "title": "Key Column Name", "description": "key column name","required":"true"},
    {"name":"keycolumntype", "value":"", "widget": "array", "optionsArray": ["index","timestamp","date"],  "title": "Key Column Type", "description": "index, timestamp or date type supported","required":"true"},
    {"name":"keycolumnformat", "value":"", "widget": "textfield", "title": "Key Column Format", "description": "timestamp column format"},

    {"name": "schema", "value":"", "widget": "tab", "title": "Schema"},
    {"name":"outputColNames", "value":"[]", "widget": "schema_col_names", "title": "Column Names of the Table", "description": "Output Columns Names of the Table"},
    {"name":"outputColTypes", "value":"[]", "widget": "schema_col_types", "title": "Column Types of the Table", "description": "Output Column Types of the Table"},
    {"name":"outputColFormats", "value":"[]", "widget": "schema_col_formats", "title": "Column Formats", "description": "Output Column Formats"}
  ,
    {"name": "properties", "value":"", "widget": "tab", "title": "Properties"},
    {"name":"pushDownPredicate", "value":"true", "widget": "array", "title": "Push Down Predicate", "optionsArray": ["true","false"], "description": "Enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. " },
    {"name":"pushDownAggregate", "value":"false", "widget": "array", "title": "Push Down Aggregate", "optionsArray": ["true","false"], "description": "Enable or disable aggregate push-down in V2 JDBC data source. The default value is false, in which case Spark will not push down aggregates to the JDBC data source. Aggregate push-down is usually turned off when the aggregate is performed faster by Spark than by the JDBC data source. Please note that aggregates can be pushed down if and only if all the aggregate functions and the related filters can be pushed down." },
    {"name":"fetchsize", "value":"", "widget": "textfield", "title": "Fetch Size", "description": "The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows)." },
    {"name":"queryTimeout", "value":"", "widget": "textfield", "title": "Query Timeout", "description": "The number of seconds the driver will wait for a Statement object to execute. Zero means there is no limit." },
    {"name":"sessionInitStatement", "value":"", "widget": "textfield", "title": "Session Init Statement", "description": "After each database session is opened to the remote DB and before starting to read data, this parameter executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example: option(\"sessionInitStatement\", \"BEGIN execute immediate 'alter session set \"_serial_direct_read\"=true'; END;\").", "expandable": true},

    {"name": "performance", "value":"", "widget": "tab", "title": "Performance"},
    {"name":"initialLoadQuery", "value":"", "widget": "textarea_small", "title": "Initial Load Query", "required":"", "description": "Query used in first run of the AutoIncrement. By default full load of table is configured."},
    {"name":"dynamicBoundValues", "value":"false", "widget": "array", "title": "Dynamic Bound Values", "optionsArray": ["true","false"], "description": "Enable or disable of DynmaicBoundValues for subsequent run(not in initial load) to get upperbound and lowerbound values. And partitionColumn & numPartitions values should be there." },
    {"name":"partitionColumn", "value":"", "widget": "textfield", "title": "Partition Column Name", "description": "PartitionColumn must be a numeric, date, or timestamp column from the table" },
    {"name":"partitionColType", "value":"", "widget": "array", "optionsArray": ["index","timestamp","date"],  "title": "Partition Column Type", "description": "index, timestamp or date type supported"},
    {"name":"lowerBound", "value":"", "widget": "textfield", "title": "Lower Bound", "description": " LowerBound and UpperBound are just used to decide the partition stride, not for filtering the rows in the table. All rows in the table will be partitioned and returned. This option applies only in initial reading" },
    {"name":"upperBound", "value":"", "widget": "textfield", "title": "Upper Bound", "description": " LowerBound and UpperBound are just used to decide the partition stride, not for filtering the rows in the table. All rows in the table will be partitioned and returned. This option applies only to initial reading" },
    {"name":"numPartitions", "value":"", "widget": "textfield", "title": "Num Partitions", "description": "The maximum number of partitions that can be used for parallelism in table reading" }
  ]
}
start-details:

This node reads data from Relational Databases using JDBC and creates a DataFrame from it.

end-details:
