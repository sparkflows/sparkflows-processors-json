{
  "id": "4",
  "name": "Hashing TF",
  "type": "ml-transformer",
  "description":"Maps a sequence of terms to term frequencies using the hashing trick.",

  "input": "It takes in a DataFrame as input and transforms it to another DataFrame",
  "output": "A new column is added to the input DataFrame containing hashing of the bag of words into a feature vector",

  "nodeClass": "fire.nodes.ml.NodeHashingTF",
  "fields" : [
    {"name":"inputCol", "value":"", "widget": "variable", "title": "Input Column",
     "description": "Contains sets of terms. In text processing, a 'set of terms' might be a bag of words",
     "datatypes":["array"] ,"required":"true"
    },
    {"name":"outputCol", "value":"ner", "widget": "textfield", "required":"true", "title": "Output Column", "description": "Output column name"}
  ]
}


start-details:

h2: Hashing TF Node Details

The Hashing TF Node maps a sequence of terms to their term frequencies using the hashing trick. Currently we use Austin Applebyâ€™s MurmurHash 3 algorithm (MurmurHash3_x86_32) to calculate the hash code value for the term object.

Hashing TF converts documents to vectors of fixed size. The default feature dimension is 262,144. The terms are mapped to indices using a Hash Function. The term frequencies are computed with respect to the mapped indices.

h4:Input Parameters
* OUTPUT STORAGE LEVEL : Keep this as DEFAULT.
* INPUT COLUMN : Select the input token column from the incoming schema. 
* OUTPUT COLUMN : Set a name of the output transformed column.

end-details:

start-examples:

h2: Hashing TF Node Example

Consider the below <b>Hashing TF</b> vector output for the <b>tokenizer</b> column.

|-----------------------------------------------------------------------------|
|       Title         |       tokenizer         |           hashTF            |
|-----------------------------------------------------------------------------|
| Learning Sparkflows | (learning, sparkflows)  |   (1000,[72,990],[1.0,1.0]) |


end-examples:
