{
  "id": "5",
  "name": "Naive Bayes",
  "description": "Creates a NaiveBayes model. Supports both Multinomial NB which can handle finitely supported discrete data. For example, by converting documents into TF-IDF vectors, it can be used for document classification. By making every vector a binary (0/1) data, it can also be used as Bernoulli NB.The input feature values must be nonnegative",
  "type": "ml-estimator",
  "engine": "all",
  "nodeClass": "fire.nodes.ml.NodeNaiveBayes",
  "fields" : [
    {"name": "featuresCol", "value":"", "widget": "variable", "title": "Features Column", "description": "Features column of type vectorUDT for model fitting", "datatypes":["vectorudt"]},

    {"name": "labelCol", "value":"", "widget": "variable", "title": "Label Column", "description": "The label column for model fitting", "datatypes":["double"]},

    {"name": "predictionCol", "value":"", "widget": "textfield", "title": "Prediction Column", "description": "The prediction column created during model scoring"},

    {"name":"path", "value":"", "widget": "textfield", "title": "Path", "description": "Save Confusion Matrix to Path"},

    {"name": "modelType", "value":"multinomial", "widget": "array", "title": "modelType", "optionsArray": ["multinomial","bernoulli"], "description": "The model type. Supported options: multinomial and bernoulli. (default = multinomial)"},

    {"name": "smoothing", "value":"1.0", "widget": "textfield", "title": "Smoothing", "description": "The smoothing parameter."},

    {"name": "weightCol", "value":"", "widget": "variable", "title": "Weight Column", "description": "Weight Column", "datatypes":["double"]}
  ]
}

start-details:

Naive Bayes classifiers are a family of simple probabilistic, multiclass classifiers based on applying Bayes’ theorem with strong (naive) independence assumptions between every pair of features.

Naive Bayes can be trained very efficiently. With a single pass over the training data, it computes the conditional probability distribution of each feature given each label. For prediction, it applies Bayes’ theorem to compute the conditional probability distribution of each label given an observation.

More details are available at Apache Spark ML docs page:

http://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes

end-details:

start-examples:
Below example is available at : https://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes

import org.apache.spark.ml.classification.NaiveBayes
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

// Load the data stored in LIBSVM format as a DataFrame.
val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

// Split the data into training and test sets (30% held out for testing)
val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3), seed = 1234L)

// Train a NaiveBayes model.
val model = new NaiveBayes()
  .fit(trainingData)

// Select example rows to display.
val predictions = model.transform(testData)
predictions.show()

// Select (prediction, true label) and compute test error
val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
val accuracy = evaluator.evaluate(predictions)
println(s"Test set accuracy = $accuracy")

end-examples:

