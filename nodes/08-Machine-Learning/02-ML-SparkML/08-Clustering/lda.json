{
  "id": "4",
  "name": "LDA",
  "description": "LDA is given a collection of documents as input data, via the featuresCol parameter. Each document is specified as a Vector of length vocabSize, where each entry is the count for the corresponding term (word) in the document",

  "input": "It takes in a DataFrame as input and performs LDA",
  "output": "LDA Model is passed to the next Node for Prediction or Storing",

  "type": "ml-estimator",
  "engine": "all",
  "nodeClass": "fire.nodes.ml.NodeLDA",
  "fields" : [

    {"name": "featuresCol", "value":"", "widget": "variable", "title": "Features Column",
          "description": "Features column of type vectorUDT for model fitting.", "datatypes":["vectorudt"]},

    {"name": "k", "value":10, "widget": "textfield","title": "K", "description": "The number of topics to create.", "datatypes":["integer"]},

    {"name": "maxIter", "value":20, "widget": "textfield", "title": "Max Iterations", "description": "The maximum number of iterations.", "datatypes":["integer"]},

    {"name": "optimizer", "value":"online", "widget": "array","title": "Optimizer","optionsArray": ["online","em"],"description": "Optimizer or inference algorithm used to estimate the LDA model."},

    {"name": "topicDistributionCol", "value":"", "widget": "textfield", "title": "TopicDistributionColumn", "description": "Output column with estimates of the topic mixture distribution for each document"},
    {"name": "docConcentration", "value":0.01, "widget": "textfield","title": "Doc Concentration",   "description": "Dirichlet parameter for the prior over document-topic distributions. Smaller values encourage documents to be dominated by a few topics, while larger values lead to more evenly mixed topics."},

    {"name": "checkpointInterval", "value":10, "widget": "textfield", "title": "checkpointInterval", "description": "The checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations."},

    {"name": "subsamplingRate", "value":0.05, "widget": "textfield","title": "subsamplingRate",  "description": "Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]." },

    {"name": "seed", "value":"", "widget": "textfield", "title": "Seed", "description": "Random Seed.", "datatypes":["long"]},
    
    {"name": "maxTermsPerTopic", "value":"", "widget": "textfield", "title": "MaxTermsPerTopic", "description": "Number of Terms in Topics", "datatypes":["int"]},

    {"name": "keepLastCheckpoint", "value":"true", "widget": "array", "title": "Keep Last Checkpoint", "optionsArray": ["true","false"], "description": "indicates whether to keep the last checkpoint", "datatypes":["boolean"]},

    {"name": "optimizeDocConcentration", "value":"false", "widget": "array", "title": "Optimize Doc Concentration", "optionsArray": ["true","false"], "description": "Indicates whether the docConcentration will be optimized during training", "datatypes":["boolean"]}
  ]
}


start-details:

LDA is implemented as an Estimator that supports both EMLDAOptimizer and OnlineLDAOptimizer, and generates a LDAModel as the base model. Expert users may cast a LDAModel generated by EMLDAOptimizer to a DistributedLDAModel if needed.

More details are available at Apache Spark ML docs page:

http://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda

end-details:

start-examples:

Below example is available at : https://spark.apache.org/docs/latest/ml-clustering.html#latent-dirichlet-allocation-lda

import org.apache.spark.ml.clustering.LDA

// Loads data.
val dataset = spark.read.format("libsvm")
  .load("data/mllib/sample_lda_libsvm_data.txt")

// Trains a LDA model.
val lda = new LDA().setK(10).setMaxIter(10)
val model = lda.fit(dataset)

val ll = model.logLikelihood(dataset)
val lp = model.logPerplexity(dataset)
println(s"The lower bound on the log likelihood of the entire corpus: $ll")
println(s"The upper bound on perplexity: $lp")

// Describe topics.
val topics = model.describeTopics(3)
println("The topics described by their top-weighted terms:")
topics.show(false)

// Shows the result.
val transformed = model.transform(dataset)
transformed.show(false)

end-examples:
