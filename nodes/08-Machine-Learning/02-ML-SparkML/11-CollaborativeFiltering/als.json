{
  "id": "4",
  "name": "ALS",
  "description": "Alternating Least Squares (ALS) matrix factorization.",

  "input": "It takes in a DataFrame as input and performs ALS",
  "output": "It generates the ALSModel and passes it to the next Predict and ModelSave Nodes. It also passes the incoming DataFrame to the next Nodes",

  "type": "ml-estimator",
  "nodeClass": "fire.nodes.ml.NodeALS",
  "fields" : [
    {"name":"userCol", "value":"", "widget": "variable", "title": "User Column", "description": "The column name for user ids."},
    {"name":"itemCol", "value":"", "widget": "variable", "title": "Item Column", "description": "The column name for item ids."},
    {"name":"ratingCol", "value":"", "widget": "variable", "title": "Rating Column", "description": "The column name for ratings."},
    {"name": "predictionCol", "value":"", "widget": "textfield", "title": "Prediction Column", "description": "The prediction column created during model scoring"},

    {"name": "maxIter", "value":"10", "widget": "textfield","title": "Max iterations", "description": "The maximum number of iterations.", "datatypes":["integer"]},
    {"name": "regParam", "value":"0.1", "widget": "textfield", "title": "Regularization Param", "description": "The regularization parameter.(>=0)","datatypes":["double"] },

    {"name": "alpha", "value":"1.0", "widget": "textfield", "title": "Alpha", "description": "The alpha parameter in the implicit preference formulation.(>=0)","datatypes":["double"] },

    {"name": "checkpointInterval", "value":"10", "widget": "textfield","title": "Checkpoint Interval", "description": "The checkpoint interval.", "datatypes":["integer"]},

    {"name": "nonnegative", "value":"false", "widget": "array", "title": "Non negative","optionsArray": ["true","false"], "description": "Whether to apply nonnegativity constraints.", "datatypes":["boolean"]},

    {"name": "numItemBlocks", "value":"10", "widget": "textfield","title": "Num Item Blocks", "description": "The number of item blocks.", "datatypes":["integer"]},

    {"name": "numUserBlocks", "value":"10", "widget": "textfield","title": "Num User Blocks", "description": "The number of user blocks.", "datatypes":["integer"]},

    {"name": "rank", "value":"10", "widget": "textfield","title": "Rank", "description": "The rank of the matrix factorization.", "datatypes":["integer"]},

    {"name": "seed", "value":"", "widget": "textfield", "title": "Seed", "description": "Random Seed.", "datatypes":["long"]},

    {"name": "implicitPrefs", "value":"false", "widget": "array", "title": "Implicit Prefs","optionsArray": ["true","false"], "description": "whether to use implicit preference", "datatypes":["boolean"]}

  ]
}


start-details:

Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors. The implementation in spark.ml has the following parameters:

* numBlocks is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).
* rank is the number of latent factors in the model (defaults to 10).
* maxIter is the maximum number of iterations to run (defaults to 10).
* regParam specifies the regularization parameter in ALS (defaults to 1.0).
* implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).
* alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0).
* nonnegative specifies whether or not to use nonnegative constraints for least squares (defaults to false).

More details are available at Apache Spark ML docs page:

http://spark.apache.org/docs/latest/ml-collaborative-filtering.html

end-details:

start-examples:
h2:Below example is available at : https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html#examples

import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating

// Load and parse the data
val data = sc.textFile("data/mllib/als/test.data")
val ratings = data.map(_.split(',') match { case Array(user, item, rate) =>
  Rating(user.toInt, item.toInt, rate.toDouble)
})

// Build the recommendation model using ALS
val rank = 10
val numIterations = 10
val model = ALS.train(ratings, rank, numIterations, 0.01)

// Evaluate the model on rating data
val usersProducts = ratings.map { case Rating(user, product, rate) =>
  (user, product)
}
val predictions =
  model.predict(usersProducts).map { case Rating(user, product, rate) =>
    ((user, product), rate)
  }
val ratesAndPreds = ratings.map { case Rating(user, product, rate) =>
  ((user, product), rate)
}.join(predictions)
val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =>
  val err = (r1 - r2)
  err * err
}.mean()
println(s"Mean Squared Error = $MSE")

// Save and load model
model.save(sc, "target/tmp/myCollaborativeFilter")
val sameModel = MatrixFactorizationModel.load(sc, "target/tmp/myCollaborativeFilter")

end-examples:


