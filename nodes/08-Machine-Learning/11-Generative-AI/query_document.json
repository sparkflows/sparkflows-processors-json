{
  "id": "6",
  "name": "Query Document",
  "description": "Query from large Documents",
  "output": "",
  "type": "pyspark",
  "engine": "pyspark",
  "nodeClass": "fire.nodes.gai.NodeQueryDocument",
  "fields" : [
    {"name":"inputFilePath", "value":"", "widget": "textfield", "title": "Input File Path", "description": "Enter Input File path"},
    {"name":"userQuery", "value":"", "widget": "textfield", "title": "Query", "description": "User Query"},
    {"name":"vectorStorePath", "value":"", "widget": "textfield", "title": "Vector Store Path", "description": "Enter Output File Path"},
    {"name":"outputPath", "value":"", "widget": "textfield", "title": "Output File Path", "description": "Enter Output File Path"},
    {"name": "chunk_size", "value":"2000", "widget": "textfield", "title": "Chunk Size", "description": "Enter chunk size", "datatypes":["integer"]},
    {"name": "chunk_overlap", "value":"100", "widget": "textfield", "title": "Chunk Overlap", "description": "Enter chunk overlap size", "datatypes":["integer"]},
    {"name": "chain_type", "value":"stuff", "widget": "array", "title": "Chain Type", "optionsArray": ["stuff","refine","map_reduce"], "description": "Type of document combining chain to use"},

    {"name": "advanced", "value":"", "widget": "tab", "title": "LLM Connection"},

    {"name": "llmConnection", "value":"", "widget": "object_array", "title": "Connection", "description": "The LLM connection to connect" ,"required":"true"},
    {"name": "temperature", "value":0, "widget": "textfield", "title": "Temperature", "description": "What sampling temperature to use.", "datatypes":["float"],"required":"true"},
    {"name": "model", "value":"gpt-3.5-turbo", "widget": "textfield", "title": "Model", "description": "Model name to use.","required":"true"},
    {"name": "top_p", "value":1, "widget": "textfield", "title": "Top_P", "description": "Total probability mass of tokens to consider at each step.", "datatypes":["float"],"required":"true"},
    {"name": "frequency_penalty", "value":0, "widget": "textfield", "title": "Frequency Penalty", "description": "Penalizes repeated tokens according to frequency.", "datatypes":["float"],"required":"true"},
    {"name": "presence_penalty", "value":0, "widget": "textfield", "title": "Presence Penalty", "description": "Penalizes repeated tokens.", "datatypes":["float"],"required":"true"},
    {"name": "max_retries", "value":6, "widget": "textfield", "title": "Max Retries", "description": "Maximum number of retries to make when generating.", "datatypes":["integer"]},
    {"name": "best_of", "value":1, "widget": "textfield", "title": "Best of", "description": "Generates best_of completions server-side and returns the \"best\"", "datatypes":["integer"]},
    {"name": "n", "value":1, "widget": "textfield", "title": "N", "description": "How many completions to generate for each prompt.", "datatypes":["integer"]},
    {"name": "max_tokens", "value":256, "widget": "textfield", "title": "Max Tokens", "description": "The maximum number of tokens to generate in the completion", "datatypes":["integer"]},
    {"name": "OpenAI_organization", "value":"", "widget": "textfield", "title": "OpenAI Organization", "description": "OpenAI Organization", "datatypes":["string"]},
    {"name": "tiktoken_model_name", "value":"", "widget": "textfield", "title": "Tiktoken Model Name", "description": "The model name to pass to tiktoken", "datatypes":["string"]}
  ]
}
